{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3592d8-8ffd-448a-860f-a152c5cce72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "import os, sys\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle as pickle\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import muon as mu\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import sparse\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7959897-5f46-4390-957f-093dadc62dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inherit hSBM code from https://github.com/martingerlach/hSBM_Topicmodel\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class bionsbm():\n",
    "\t\"\"\"\n",
    "\tClass to run bionsbm\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, obj, label=None, max_depth=6, modality=\"Mod1\"):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.keywords = []\n",
    "\t\tself.nbranches = 1\n",
    "\t\tself.modalities = []\n",
    "\t\tself.max_depth = max_depth\n",
    "\t\tself.obj = obj\n",
    "\n",
    "\t\tif isinstance(obj, mu.MuData):\n",
    "\t\t\tself.modalities=list(obj.mod.keys())   \n",
    "\t\t\tdfs=[obj[key].to_df().T for key in self.modalities]\n",
    "\t\t\tself.make_graph_multiple_df(dfs[0], dfs[1:])\n",
    "\n",
    "\t\telif isinstance(obj, ad.AnnData):\n",
    "\t\t\tself.modalities=[modality]\n",
    "\t\t\tself.make_graph_multiple_df(obj.to_df().T, [])\n",
    "\n",
    "\t\tif label:\n",
    "\t\t\tg_raw=self.g.copy()\n",
    "\t\t\tprint(\"Label found\")\n",
    "\t\t\tmetadata=obj[self.modalities[0]].obs\n",
    "\t\t\tmymap = dict([(y,str(x)) for x,y in enumerate(sorted(set(obj[self.modalities[0]].obs[label])))])\n",
    "\t\t\tinv_map = {v: k for k, v in mymap.items()}\n",
    "\n",
    "\t\t\tdocs_type=[int(mymap[metadata.loc[doc][label]]) for doc in self.documents]\n",
    "\t\t\ttypes={}\n",
    "\t\t\ttypes[\"Docs\"]=docs_type\n",
    "\t\t\tfor i, key in enumerate(self.modalities):\n",
    "\t\t\t\ttypes[key]=[int(i+np.max(docs_type)+1) for a in range(0, obj[key].shape[0])]\n",
    "\t\t\tnode_type = g_raw.new_vertex_property('int', functools.reduce(lambda a, b : a+b, list(types.values())))\n",
    "\t\t\tself.g = g_raw.copy()\n",
    "\t\telse:\n",
    "\t\t\tnode_type=None\n",
    "\t\tself.node_type=node_type \n",
    "\n",
    "\t\t\n",
    "\tdef save_graph(self, filename=\"graph.xml.gz\")->None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave the graph\n",
    "\n",
    "\t\t:param filename: name of the graph stored\n",
    "\t\t\"\"\"\n",
    "\t\tself.g.save(filename)\n",
    "\t\n",
    "\t\n",
    "\tdef load_graph(self, filename=\"graph.xml.gz\")->None:\n",
    "\t\t\"\"\"\n",
    "\t\tLoad a saved graph from disk and rebuild documents, words, and keywords.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tfilename : str, optional\n",
    "\t\t\tPath to the saved graph file (default: \"graph.xml.gz\").\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tself.g = gt.load_graph(filename)\n",
    "\t\tself.documents = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 0]\n",
    "\t\tself.words = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 1]\n",
    "\t\tmetadata_indexes = np.unique(self.g.vp[\"kind\"].a)\n",
    "\t\tmetadata_indexes = metadata_indexes[metadata_indexes > 1] #no doc or words\n",
    "\t\tself.nbranches = len(metadata_indexes)\n",
    "\t\tfor i_keyword in metadata_indexes:\n",
    "\t\t\tself.keywords.append([self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == i_keyword])\n",
    "\n",
    "\n",
    "\tdef make_graph_multiple_df(self, df: pd.DataFrame, df_keyword_list: list)->None:\n",
    "\t\t\"\"\"\n",
    "\t\tCreate a graph from two dataframes one with words, others with keywords or other layers of information\n",
    "\n",
    "\t\t:param df: DataFrame with words on index and texts on columns\n",
    "\t\t:param df_keyword_list: list of DataFrames with keywords on index and texts on columns\n",
    "\t\t\"\"\"\n",
    "\t\tdf_all = df.copy(deep =True)\n",
    "\t\tfor ikey,df_keyword in enumerate(df_keyword_list):\n",
    "\t\t\tdf_keyword = df_keyword.reindex(columns=df.columns)\n",
    "\t\t\tdf_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "\t\t\tdf_keyword[\"kind\"] = ikey+2\n",
    "\t\t\tdf_all = pd.concat((df_all,df_keyword), axis=0)\n",
    "\n",
    "\t\tdef get_kind(word):\n",
    "\t\t\treturn 1 if word in df.index else df_all.at[word,\"kind\"]\n",
    "\n",
    "\t\tself.nbranches = len(df_keyword_list)\n",
    "\t   \n",
    "\t\tself.make_graph(df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "\n",
    "\n",
    "\tdef make_graph(self, df: pd.DataFrame, get_kind):\n",
    "\t\tself.g = gt.Graph(directed=False)\n",
    "\n",
    "\t\tn_docs, n_words = df.shape[1], df.shape[0]\n",
    "\n",
    "\t\t# Add all vertices first\n",
    "\t\tself.g.add_vertex(n_docs + n_words)\n",
    "\n",
    "\t\t# Create vertex properties\n",
    "\t\tname = self.g.new_vp(\"string\")\n",
    "\t\tkind = self.g.new_vp(\"int\")\n",
    "\t\tself.g.vp[\"name\"] = name\n",
    "\t\tself.g.vp[\"kind\"] = kind\n",
    "\n",
    "\t\t# Assign doc vertices (loop for names, array for kind)\n",
    "\t\tfor i, doc in enumerate(df.columns):\n",
    "\t\t\tname[self.g.vertex(i)] = doc\n",
    "\t\tkind.get_array()[:n_docs] = 0\n",
    "\n",
    "\t\t# Assign word vertices (loop for names, array for kind)\n",
    "\t\tfor j, word in enumerate(df.index):\n",
    "\t\t\tname[self.g.vertex(n_docs + j)] = word\n",
    "\t\tkind.get_array()[n_docs:] = np.array([get_kind(w) for w in df.index], dtype=int)\n",
    "\n",
    "\t\t# Edge weights\n",
    "\t\tweight = self.g.new_ep(\"int\")\n",
    "\t\tself.g.ep[\"count\"] = weight\n",
    "\n",
    "\t\t# Build sparse edges\n",
    "\t\trows, cols = df.values.nonzero()\n",
    "\t\tvals = df.values[rows, cols].astype(int)\n",
    "\t\tedges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "\t\tif len(edges)==0: raise ValueError(\"Empty graph\")\n",
    "\n",
    "\t\tself.g.add_edge_list(edges, eprops=[weight])\n",
    "\n",
    "\t\t# Remove edges with 0 weight\n",
    "\t\tfilter_edges = self.g.new_edge_property(\"bool\")\n",
    "\t\tfor e in self.g.edges():\n",
    "\t\t\tfilter_edges[e] = weight[e] > 0\n",
    "\t\tself.g.set_edge_filter(filter_edges)\n",
    "\t\tself.g.purge_edges()\n",
    "\t\tself.g.clear_filters()\n",
    "\n",
    "\t\tself.documents = df.columns\n",
    "\t\tself.words = df.index[self.g.vp['kind'].a[n_docs:] == 1]\n",
    "\t\tfor ik in range(2, 2 + self.nbranches):\n",
    "\t\t\tself.keywords.append(df.index[self.g.vp['kind'].a[n_docs:] == ik])\n",
    "\n",
    "\n",
    "\tdef fit(self, n_init=1, verbose=True, deg_corr=True, overlap=False, parallel=False, B_min=0, B_max=None, clabel=None, name = \"results/mymodel\", *args, **kwargs) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tFit using minimize_nested_blockmodel_dl\n",
    "\t\t\n",
    "\t\t:param n_init: number of initialisation. The best will be kept\n",
    "\t\t:param verbose: Print output\n",
    "\t\t:param deg_corr: use deg corrected model\n",
    "\t\t:param overlap: use overlapping model\n",
    "\t\t:param parallel: perform parallel moves\n",
    "\t\t:param  \\*args: positional arguments to pass to gt.minimize_nested_blockmodel_dl\n",
    "\t\t:param  \\*\\*kwargs: keywords arguments to pass to gt.minimize_nested_blockmodel_dl\n",
    "\t\t\"\"\"\n",
    "\t\tif clabel == None:\n",
    "\t\t\tclabel = self.g.vp['kind']\n",
    "\t\t\tstate_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Clabel is {clabel}, assigning partitions to vertices\", flush=True)\n",
    "\t\t\tstate_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "\t\n",
    "\t\tstate_args[\"eweight\"] = self.g.ep.count\n",
    "\t\tmin_entropy = np.inf\n",
    "\t\tbest_state = None\n",
    "\t\tstate_args[\"deg_corr\"] = deg_corr\n",
    "\t\tstate_args[\"overlap\"] = overlap\n",
    "\n",
    "\t\tif B_max is None:\n",
    "\t\t\tB_max = self.g.num_vertices()\n",
    "\t\t\t\n",
    "\t\tmultilevel_mcmc_args={\"B_min\": B_min, \"B_max\": B_max, \"verbose\": verbose,\"parallel\" : parallel}\n",
    "\n",
    "\t\tprint(\"multilevel_mcmc_args is \\n\", multilevel_mcmc_args, flush=True)\n",
    "\t\tprint(\"state_args is \\n\", state_args, flush=True)\n",
    "\n",
    "\t\tfor _ in range(n_init):\n",
    "\t\t\tprint(\"Fit number:\", _, flush=True)\n",
    "\t\t\tstate = gt.minimize_nested_blockmodel_dl(self.g, state_args=state_args, multilevel_mcmc_args=multilevel_mcmc_args, *args, **kwargs)\n",
    "\t\t\t\n",
    "\t\t\tentropy = state.entropy()\n",
    "\t\t\tif entropy < min_entropy:\n",
    "\t\t\t\tmin_entropy = entropy\n",
    "\t\t\t\tself.state = state\n",
    "\t\t\t\t\n",
    "\t\tself.mdl = min_entropy\n",
    "\n",
    "\t\tL = len(self.state.levels)\n",
    "\t\tself.L = L\n",
    "\t\tself.groups = {}\n",
    "\t\tself.save_data(name=name)\n",
    "\n",
    "\n",
    "\t# Helper functions\n",
    "\tdef dump_model(self, filename=\"bionsbm.pkl\"):\n",
    "\t\t\"\"\"\n",
    "\t\tDump model using pickle\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\twith open(filename, 'wb') as f:\n",
    "\t\t\tpickle.dump(self, f)\n",
    "\n",
    "\tdef load_model(self, filename=\"bionsbm.pkl\"):\n",
    "\t\twith open(filename, \"rb\") as f:\n",
    "\t\t\tmodel = pickle.load(f)\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "\tdef get_mdl(self):\n",
    "\t\t\"\"\"\n",
    "\t\tGet minimum description length\n",
    "\n",
    "\t\tProxy to self.state.entropy()\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.mdl\n",
    "\t\t\t\n",
    "\tdef _get_shape(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: list of tuples (number of documents, number of words, (number of keywords,...))\n",
    "\t\t\"\"\"\n",
    "\t\tD = int(np.sum(self.g.vp['kind'].a == 0)) #documents\n",
    "\t\tW = int(np.sum(self.g.vp['kind'].a == 1)) #words\n",
    "\t\tK = [int(np.sum(self.g.vp['kind'].a == (k+2))) for k in range(self.nbranches)] #keywords\n",
    "\t\treturn D, W, K\n",
    "\n",
    "\tdef get_groups(self, l=0) -> None:\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tNumba-accelerated get_groups that is robust for bipartite graphs (nbranches == 0)\n",
    "\t\tand for arbitrary number of partitions.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t@njit\n",
    "\t\tdef process_edges_numba_stack(sources, targets, z1, z2, kinds, weights,\n",
    "\t\t\t\t\t\t\t\t\t  D, W, K_arr, nbranches,\n",
    "\t\t\t\t\t\t\t\t\t  n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tNumba-compiled loop that increments the stacked accumulator arrays.\n",
    "\t\t\tThis function is defensive: if a 'kind' references a branch index out of range,\n",
    "\t\t\tor an index into keywords is out of range, it's ignored (so bipartite graphs keep working).\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tm = len(sources)\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tv1 = sources[i]\n",
    "\t\t\t\tv2 = targets[i]\n",
    "\t\t\t\tw = weights[i]\n",
    "\t\t\t\tt1 = z1[i]\n",
    "\t\t\t\tt2 = z2[i]\n",
    "\t\t\t\tkind = kinds[i]\n",
    "\t\t\n",
    "\t\t\t\t# update doc-group counts (always)\n",
    "\t\t\t\tn_db[v1, t1] += w\n",
    "\t\t\n",
    "\t\t\t\tif kind == 1:\n",
    "\t\t\t\t\t# word node\n",
    "\t\t\t\t\tidx_w = v2 - D\n",
    "\t\t\t\t\tif idx_w >= 0 and idx_w < n_wb.shape[0]:\n",
    "\t\t\t\t\t\tn_wb[idx_w, t2] += w\n",
    "\t\t\t\t\t# update doc->word-group\n",
    "\t\t\t\t\tn_dbw[v1, t2] += w\n",
    "\t\t\n",
    "\t\t\t\telif kind >= 2:\n",
    "\t\t\t\t\tik = kind - 2\n",
    "\t\t\t\t\t# guard: only process if ik is a valid branch index\n",
    "\t\t\t\t\tif ik >= 0 and ik < nbranches:\n",
    "\t\t\t\t\t\t# compute offset = D + W + sum(K_arr[:ik])\n",
    "\t\t\t\t\t\toffset = D + W\n",
    "\t\t\t\t\t\tfor j in range(ik):\n",
    "\t\t\t\t\t\t\toffset += K_arr[j]\n",
    "\t\t\t\t\t\tidx_k = v2 - offset\n",
    "\t\t\t\t\t\t# guard keyword index bounds\n",
    "\t\t\t\t\t\tif idx_k >= 0 and idx_k < K_arr[ik]:\n",
    "\t\t\t\t\t\t\tn_w_key_b3[ik, idx_k, t2] += w\n",
    "\t\t\t\t\t\t\tn_dbw_key3[ik, v1, t2] += w\n",
    "\t\t\t\t\t\t# else: out-of-range keyword index -> ignore to remain robust\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# unexpected kind (<1): ignore for safety (original assumed only kind==1 or >=2)\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\tif l in self.groups.keys():\n",
    "\t\t\treturn self.groups[l]\n",
    "\n",
    "\t\tstate_l = self.state.project_level(l).copy(overlap=True)\n",
    "\t\tstate_l_edges = state_l.get_edge_blocks()\n",
    "\t\tB = state_l.get_B()\n",
    "\t\tD, W, K = self._get_shape()\n",
    "\t\tnbranches = self.nbranches\n",
    "\n",
    "\t\t# Preallocate primary arrays (word/doc)\n",
    "\t\tn_wb = np.zeros((W, B), dtype=np.float64)\t# words x word-groups\n",
    "\t\tn_db = np.zeros((D, B), dtype=np.float64)\t# docs  x doc-groups\n",
    "\t\tn_dbw = np.zeros((D, B), dtype=np.float64)   # docs  x word-groups\n",
    "\n",
    "\t\t# Preallocate stacked branch arrays (shape: nbranches x max_K x B) and (nbranches x D x B)\n",
    "\t\tif nbranches > 0:\n",
    "\t\t\tmax_K = int(np.max(K))\n",
    "\t\t\t# If some K are zero, max_K will still be >=0; stack is safe\n",
    "\t\t\tn_w_key_b3 = np.zeros((nbranches, max_K, B), dtype=np.float64)\n",
    "\t\t\tn_dbw_key3 = np.zeros((nbranches, D, B), dtype=np.float64)\n",
    "\t\telse:\n",
    "\t\t\t# empty stacked arrays if no branches\n",
    "\t\t\tn_w_key_b3 = np.zeros((0, 0, B), dtype=np.float64)\n",
    "\t\t\tn_dbw_key3 = np.zeros((0, D, B), dtype=np.float64)\n",
    "\n",
    "\t\t# Convert graph edges to arrays\n",
    "\t\tedges = list(self.g.edges())\n",
    "\t\tm = len(edges)\n",
    "\t\tsources = np.empty(m, dtype=np.int64)\n",
    "\t\ttargets = np.empty(m, dtype=np.int64)\n",
    "\t\tz1_arr = np.empty(m, dtype=np.int64)\n",
    "\t\tz2_arr = np.empty(m, dtype=np.int64)\n",
    "\t\tweights = np.empty(m, dtype=np.float64)\n",
    "\t\tkinds = np.empty(m, dtype=np.int64)\n",
    "\n",
    "\t\tfor i, e in enumerate(edges):\n",
    "\t\t\tsources[i] = int(e.source())\n",
    "\t\t\ttargets[i] = int(e.target())\n",
    "\t\t\tz1_arr[i] = int(state_l_edges[e][0])\n",
    "\t\t\tz2_arr[i] = int(state_l_edges[e][1])\n",
    "\t\t\tweights[i] = float(self.g.ep[\"count\"][e])\n",
    "\t\t\tkinds[i] = int(self.g.vp['kind'][int(e.target())])\n",
    "\n",
    "\t\tK_arr = np.array(K, dtype=np.int64)  # can be empty if nbranches==0\n",
    "\n",
    "\t\t# --- Numba edge processing (single compiled function for all cases) ---\n",
    "\t\tprocess_edges_numba_stack(sources, targets, z1_arr, z2_arr, kinds, weights, D, W, K_arr, nbranches, n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3)\n",
    "\n",
    "\t\t# --- Trim empty columns for doc/word arrays (same logic as original) ---\n",
    "\t\tind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "\t\tn_db = n_db[:, ind_d]\n",
    "\t\tBd = len(ind_d)\n",
    "\n",
    "\t\tind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "\t\tn_wb = n_wb[:, ind_w]\n",
    "\t\tBw = len(ind_w)\n",
    "\n",
    "\t\tind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "\t\tn_dbw = n_dbw[:, ind_w2]\n",
    "\n",
    "\t\t# --- Convert stacked branch arrays into per-branch lists (safe slicing) ---\n",
    "\t\tn_w_key_b_list = []\n",
    "\t\tn_dbw_key_list = []\n",
    "\t\tBk = []\n",
    "\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tKk = int(K_arr[ik])\n",
    "\t\t\tif Kk > 0:\n",
    "\t\t\t\t# compute which columns (groups) are non-zero\n",
    "\t\t\t\tcol_sums = np.sum(n_w_key_b3[ik, :Kk, :], axis=0)\n",
    "\t\t\t\tind_wk = np.where(col_sums > 0)[0]\n",
    "\t\t\t\t# slice and copy into a per-branch array (Kk x Bk)\n",
    "\t\t\t\tif ind_wk.size > 0:\n",
    "\t\t\t\t\tn_w_key_b_list.append(n_w_key_b3[ik, :Kk, :][:, ind_wk].copy())\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# keep shape (Kk, 0) if there are no columns\n",
    "\t\t\t\t\tn_w_key_b_list.append(np.zeros((Kk, 0), dtype=np.float64))\n",
    "\t\t\t\tBk.append(len(ind_wk))\n",
    "\t\t\telse:\n",
    "\t\t\t\t# branch with 0 keywords\n",
    "\t\t\t\tn_w_key_b_list.append(np.zeros((0, 0), dtype=np.float64))\n",
    "\t\t\t\tBk.append(0)\n",
    "\n",
    "\t\t\t# doc x keyword-groups for this branch\n",
    "\t\t\tcol_sums_dbw = np.sum(n_dbw_key3[ik], axis=0)\n",
    "\t\t\tind_w2k = np.where(col_sums_dbw > 0)[0]\n",
    "\t\t\tif ind_w2k.size > 0:\n",
    "\t\t\t\tn_dbw_key_list.append(n_dbw_key3[ik][:, ind_w2k].copy())\n",
    "\t\t\telse:\n",
    "\t\t\t\tn_dbw_key_list.append(np.zeros((D, 0), dtype=np.float64))\n",
    "\n",
    "\t\t# --- Compute probabilities exactly like the original (division -> NaN if denominator==0) ---\n",
    "\t\t# P(t_w | w)\n",
    "\t\tdenom = np.sum(n_wb, axis=1, keepdims=True)  # (W,1)\n",
    "\t\tp_tw_w = (n_wb / denom).T\n",
    "\n",
    "\t\t# P(t_k | keyword) per branch\n",
    "\t\tp_tk_w_key = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_w_key_b_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=1, keepdims=True)\n",
    "\t\t\tp_tk_w_key.append((arr / denom).T)\n",
    "\n",
    "\t\t# P(w | t_w)\n",
    "\t\tdenom = np.sum(n_wb, axis=0, keepdims=True)  # (1,Bw)\n",
    "\t\tp_w_tw = n_wb / denom\n",
    "\n",
    "\t\t# P(keyword | t_w_key) per branch\n",
    "\t\tp_w_key_tk = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_w_key_b_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=0, keepdims=True)\n",
    "\t\t\tp_w_key_tk.append(arr / denom)\n",
    "\n",
    "\t\t# P(t_w | d)\n",
    "\t\tdenom = np.sum(n_dbw, axis=1, keepdims=True)\n",
    "\t\tp_tw_d = (n_dbw / denom).T\n",
    "\n",
    "\t\t# P(t_k | d) per branch\n",
    "\t\tp_tk_d = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_dbw_key_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=1, keepdims=True)\n",
    "\t\t\tp_tk_d.append((arr / denom).T)\n",
    "\n",
    "\t\t# P(t_d | d)\n",
    "\t\tdenom = np.sum(n_db, axis=1, keepdims=True)\n",
    "\t\tp_td_d = (n_db / denom).T\n",
    "\n",
    "\t\tresult = {\n",
    "\t\t\t'Bd': Bd,\n",
    "\t\t\t'Bw': Bw,\n",
    "\t\t\t'Bk': Bk,\n",
    "\t\t\t'p_tw_w': p_tw_w,\n",
    "\t\t\t'p_tk_w_key': p_tk_w_key,\n",
    "\t\t\t'p_td_d': p_td_d,\n",
    "\t\t\t'p_w_tw': p_w_tw,\n",
    "\t\t\t'p_w_key_tk': p_w_key_tk,\n",
    "\t\t\t'p_tw_d': p_tw_d,\n",
    "\t\t\t'p_tk_d': p_tk_d}\n",
    "\n",
    "\t\tself.groups[l] = result\n",
    "\t\treturn result\n",
    "\n",
    "\n",
    "\tdef draw(self, *args, **kwargs) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tDraw the network\n",
    "\n",
    "\t\t:param \\*args: positional arguments to pass to self.state.draw\n",
    "\t\t:param \\*\\*kwargs: keyword argument to pass to self.state.draw\n",
    "\t\t\"\"\"\n",
    "\t\tcolmap = self.g.vertex_properties[\"color\"] = self.g.new_vertex_property(\n",
    "\t\t\t\"vector<double>\")\n",
    "\t\t#https://medialab.github.io/iwanthue/\n",
    "\t\tcolors = [  [174,80,209],\n",
    "\t\t\t\t\t[108,192,70],\n",
    "\t\t\t\t\t[207, 170, 60],\n",
    "\t\t\t\t\t[131,120,197],\n",
    "\t\t\t\t\t[126,138,65],\n",
    "\t\t\t\t\t[201,90,138],\n",
    "\t\t\t\t\t[87,172,125],\n",
    "\t\t\t\t\t[213,73,57],\n",
    "\t\t\t\t\t[85,175,209],\n",
    "\t\t\t\t\t[193,120,81]]\n",
    "\t\tfor v in self.g.vertices():\n",
    "\t\t\tk = self.g.vertex_properties['kind'][v]\n",
    "\t\t\tif k < 10:\n",
    "\t\t\t\tcolor = np.array(colors[k])/255.\n",
    "\t\t\telse:\n",
    "\t\t\t\tcolor = np.array([187, 129, 164])/255.\n",
    "\t\t\tcolmap[v] = color\n",
    "\t\tself.state.draw(\n",
    "\t\t\tsubsample_edges = 5000, \n",
    "\t\t\tedge_pen_width = self.g.ep[\"count\"],\n",
    "\t\t\tvertex_color=colmap,\n",
    "\t\t\tvertex_fill_color=colmap, *args, **kwargs)\n",
    "\n",
    "\n",
    "\tdef save_single_level(self, l: int, name: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave per-level probability matrices (topics, clusters, documents) for the given level.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tl : int\n",
    "\t\t\tThe level index to save. Must be within the range of available model levels.\n",
    "\t\tname : str\n",
    "\t\t\tBase path (folder + prefix) where files will be written.\n",
    "\t\t\tExample: \"results/mymodel\" → files like:\n",
    "\t\t\t\t- results/mymodel_level_0_mainfeature_topics.tsv.gz\n",
    "\t\t\t\t- results/mymodel_level_0_clusters.tsv.gz\n",
    "\t\t\t\t- results/mymodel_level_0_mainfeature_topics_documents.tsv.gz\n",
    "\t\t\t\t- results/mymodel_level_0_metafeature_topics.tsv.gz\n",
    "\t\t\t\t- results/mymodel_level_0_metafeature_topics_documents.tsv.gz\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- Files are written as tab-separated values (`.tsv.gz`) with gzip compression.\n",
    "\t\t- Handles both the main feature (`self.modalities[0]`) and any meta-features (`self.modalities[1:]`).\n",
    "\t\t- Raises RuntimeError if any file cannot be written.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# --- Validate inputs ---\n",
    "\t\tif not isinstance(l, int) or l < 0 or l >= len(self.state.levels):\n",
    "\t\t\traise ValueError(f\"Invalid level index {l}. Must be between 0 and {len(self.state.levels) - 1}.\")\n",
    "\t\tif not isinstance(name, str) or not name.strip():\n",
    "\t\t\traise ValueError(\"`name` must be a non-empty string path prefix.\")\n",
    "\n",
    "\t\tmain_feature = self.modalities[0]\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tdata = self.get_groups(l)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise RuntimeError(f\"Failed to get group data for level {l}: {e}\") from e\n",
    "\n",
    "\t\t# Helper to safely save a DataFrame\n",
    "\t\tdef _safe_save(df, filepath):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tPath(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "\t\t\t\tdf.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\traise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "\t\t# --- P(document | cluster) ---\n",
    "\t\tclusters = pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)\n",
    "\t\t_safe_save(clusters, f\"{name}_level_{l}_clusters.tsv.gz\")\n",
    "\n",
    "\n",
    "\t\t# --- P(main_feature | main_topic) ---\n",
    "\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "\t\t_safe_save(p_w_tw, f\"{name}_level_{l}_{main_feature}_topics.tsv.gz\")\n",
    "\n",
    "\t\t# --- P(main_topic | documents) ---\n",
    "\t\tp_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "\t\t_safe_save(p_tw_d, f\"{name}_level_{l}_{main_feature}_topics_documents.tsv.gz\")\n",
    "\n",
    "\t\t# --- P(meta_feature | meta_topic_feature), if any ---\n",
    "\t\tif len(self.modalities) > 1:\n",
    "\t\t\tfor k, meta_features in enumerate(self.modalities[1:]):\n",
    "\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "\t\t\t\t\tcolumns=[f\"{meta_features}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t_safe_save(p_w_tw, f\"{name}_level_{l}_{meta_features}_topics.tsv.gz\")\n",
    "\n",
    "\n",
    "\t\t\t# --- P(meta_topic | document) ---\n",
    "\t\t\tfor k, meta_features in enumerate(self.modalities[1:]):\n",
    "\t\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "\t\t\t\t\tcolumns=[f\"{meta_features}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t_safe_save(p_tw_d, f\"{name}_level_{l}_{meta_features}_topics_documents.tsv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "\tdef save_data(self, name: str = \"results/mymodel\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave the global graph, model, state, and level-specific data for the current nSBM self.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tname : str, optional\n",
    "\t\t\tBase path (folder + prefix) where all outputs will be saved.\n",
    "\t\t\tExample: \"results/mymodel\" will produce:\n",
    "\t\t\t\t- results/mymodel_graph.xml.gz\n",
    "\t\t\t\t- results/mymodel_self.pkl\t\n",
    "\t\t\t\t- results/mymodel_entropy.txt\n",
    "\t\t\t\t- results/mymodel_state.pkl\n",
    "\t\t\t\t- results/mymodel_level_X_*.tsv.gz  (per level, up to 6 levels)\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- The parent folder is created automatically if it does not exist.\n",
    "\t\t- Level saving is parallelized with threads for efficiency in I/O.\n",
    "\t\t- By default, at most 6 levels are saved, or fewer if the model has <6 levels.\n",
    "\t\t- Exceptions in parallel tasks are caught and reported without stopping other tasks.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# --- Validate name ---\n",
    "\t\tif not isinstance(name, str) or not name.strip():\n",
    "\t\t\traise ValueError(\"`name` must be a non-empty string representing the save path.\")\n",
    "\n",
    "\t\t# --- Ensure folder exists ---\n",
    "\t\tfolder = os.path.dirname(name)\n",
    "\t\tif folder:\n",
    "\t\t\tPath(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\t\t# --- Save global files ---\n",
    "\t\ttry:\n",
    "\t\t\tself.save_graph(filename=f\"{name}_graph.xml.gz\")\n",
    "\t\t\tself.dump_model(filename=f\"{name}_self.pkl\")\n",
    "\n",
    "\t\t\twith open(f\"{name}_entropy.txt\", \"w\") as f:\n",
    "\t\t\t\tf.write(str(self.state.entropy()))\n",
    "\n",
    "\t\t\twith open(f\"{name}_state.pkl\", \"wb\") as f:\n",
    "\t\t\t\tpickle.dump(self.state, f)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise RuntimeError(f\"Failed to save global files for model '{name}': {e}\") from e\n",
    "\n",
    "\n",
    "\t\t# --- Save levels in parallel (threaded to avoid data duplication) ---\n",
    "\t\tL = min(len(self.state.levels), self.max_depth)\n",
    "\t\tif L == 0:\n",
    "\t\t\tprint(\"Nothing to save\")\n",
    "\t\t\treturn  # nothing to save\n",
    "\n",
    "\t\terrors = []\n",
    "\t\twith ThreadPoolExecutor() as executor:\n",
    "\t\t\tfutures = {executor.submit(self.save_single_level, l, name): l for l in range(L)}\n",
    "\t\t\tfor future in as_completed(futures):\n",
    "\t\t\t\tl = futures[future]\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfuture.result()\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\terrors.append((l, str(e)))\n",
    "\n",
    "\t\tif errors:\n",
    "\t\t\tmsg = \"; \".join([f\"Level {l}: {err}\" for l, err in errors])\n",
    "\t\t\traise RuntimeError(f\"Errors occurred while saving levels: {msg}\")\n",
    "\n",
    "\tdef annotate_obj(self) -> None:\n",
    "\t\tL = min(len(self.state.levels), self.max_depth)\n",
    "\t\tfor l in range(0,L):\n",
    "\t\t\tmain_feature = self.modalities[0]\n",
    "\t\t\tdata = self.get_groups(l)\n",
    "\t\t\tmdata.obs[f\"Level_{l}_cluster\"]=np.argmax(pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)[mdata.obs.index], axis=0).astype(str)\n",
    "\t\t\n",
    "\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "\t\t\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[mdata[main_feature].var.index]\n",
    "\t\t\tmdata[main_feature].var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\t\t\t\n",
    "\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "\t\t\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[mdata.obs.index]\n",
    "\t\t\tp_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "\t\t\tmdata.obs[f\"Level_{l}_{main_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\t\t\n",
    "\t\t\tif len(self.modalities) > 1:\n",
    "\t\t\t\tfor k, meta_feature in enumerate(self.modalities[1:]):\n",
    "\t\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "\t\t\t\t\t\tcolumns=[f\"{meta_feature}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t\tmdata[meta_feature].var[f\"Level_{l}_{meta_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\t\t\t\n",
    "\t\t\t\t# --- P(meta_topic | document) ---\n",
    "\t\t\t\tfor k, meta_feature in enumerate(self.modalities[1:]):\n",
    "\t\t\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "\t\t\t\t\t\tcolumns=[f\"{meta_feature}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t\tp_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "\t\t\t\t\tmdata.obs[f\"Level_{l}_{meta_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\n",
    "\tdef get_V(self):\n",
    "\t\t'''\n",
    "\t\treturn number of word-nodes == types\n",
    "\t\t'''\n",
    "\t\treturn int(np.sum(self.g.vp['kind'].a == 1))  # no. of types\n",
    "\n",
    "\tdef get_D(self):\n",
    "\t\t'''\n",
    "\t\treturn number of doc-nodes == number of documents\n",
    "\t\t'''\n",
    "\t\treturn int(np.sum(self.g.vp['kind'].a == 0))  # no. of types\n",
    "\n",
    "\tdef get_N(self):\n",
    "\t\t'''\n",
    "\t\treturn number of edges == tokens\n",
    "\t\t'''\n",
    "\t\treturn int(self.g.num_edges())  # no. of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e575da5f-1e6d-4477-bc07-43f154b129ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mdata=mu.read_h5mu(\"../bionsbm/Test_data.h5mu\")\n",
    "mdata=mu.read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cb375e-8c04-4a13-94ba-0756e27da10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multilevel_mcmc_args is \n",
      " {'B_min': 0, 'B_max': 3200, 'verbose': False, 'parallel': False}\n",
      "state_args is \n",
      " {'clabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x76a1986b16d0, at 0x76a0f395cde0>, 'pclabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x76a1986b16d0, at 0x76a0f395cde0>, 'eweight': <EdgePropertyMap object with value type 'int32_t', for Graph 0x76a1986b16d0, at 0x76a0f395ee40>, 'deg_corr': True, 'overlap': False}\n",
      "Fit number: 0\n"
     ]
    }
   ],
   "source": [
    "model = bionsbm(mdata)\n",
    "model.fit(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d9c0ae-1637-4c6c-a4b8-2eb3a3bb74a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4499d917-35d8-418e-b1d2-0399776bffbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532daa38-d631-411e-aafa-2caec8b808a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64573bf5-4e18-48cb-b901-a94364ed229a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3cd6c5a7-abb0-4c25-8d10-968f77646141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def annotate_obj(model):\n",
    "    L = min(len(model.state.levels), model.max_depth)\n",
    "    for l in range(0,L):\n",
    "        main_feature = model.modalities[0]\n",
    "        data = model.get_groups(l)\n",
    "        mdata.obs[f\"Level_{l}_cluster\"]=np.argmax(pd.DataFrame(data=data[\"p_td_d\"], columns=model.documents)[mdata.obs.index], axis=0).astype(str)\n",
    "    \n",
    "        p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=model.words,\n",
    "    \t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[mdata[main_feature].var.index]\n",
    "        mdata[main_feature].var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "        \n",
    "        p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=model.documents,\n",
    "    \t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[mdata.obs.index]\n",
    "        p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "        mdata.obs[f\"Level_{l}_{main_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "    \n",
    "        if len(model.modalities) > 1:\n",
    "            for k, meta_feature in enumerate(model.modalities[1:]):\n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=model.keywords[k],\n",
    "                    columns=[f\"{meta_feature}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                mdata[meta_feature].var[f\"Level_{l}_{meta_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "        \n",
    "            # --- P(meta_topic | document) ---\n",
    "            for k, meta_feature in enumerate(model.modalities[1:]):\n",
    "                p_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=model.documents,\n",
    "                    columns=[f\"{meta_feature}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "                mdata.obs[f\"Level_{l}_{meta_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ef878-72c9-4a76-83c2-201b9d5bd4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff410f1-6692-4088-9c18-50bf889a05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16363eda-692e-4871-b2fc-d4c7d621a8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220f25e-344d-4a56-bcb4-8dd1fa9f4311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c615bc6-f24c-41dc-ae74-6b9946646701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740b4ae-1fb4-473f-ab7e-ad047fd76a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c33bb5-f6f0-48a7-911c-076596c5a5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489baee-5516-4a7a-b861-16e875b158f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
