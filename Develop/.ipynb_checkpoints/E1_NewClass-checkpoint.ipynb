{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626411d5-d124-43d3-9719-2669dafb4eee",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8037602-06c0-4b29-bd5b-1fec243ebc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f876e863-f7bf-4d37-bcad-6adcad0e2f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bionsbm\n",
    "\n",
    "Copyright(C) 2021 fvalle1 & gmalagol10\n",
    "\n",
    "This program is free software: you can redistribute it and / or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY\n",
    "without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see < http: // www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "import os, sys\n",
    "import logging\n",
    "\n",
    "from graph_tool.all import load_graph, Graph, minimize_nested_blockmodel_dl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle as pickle\n",
    "\n",
    "from muon import MuData\n",
    "from anndata import AnnData\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from scipy import sparse\n",
    "from numba import njit\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:  # prevent adding multiple handlers\n",
    "    ch = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "######################################\n",
    "class bionsbm():\n",
    "    \"\"\"\n",
    "    Class to run bionsbm\n",
    "    \"\"\"\n",
    "    def __init__(self, obj, label: Optional[str] = None, max_depth: int = 6, modality: str = \"Mod1\", saving_path: str = \"results/myself\", path_to_graph=None):\n",
    "        \"\"\"\n",
    "        Initialize a bionsbm self.\n",
    "\n",
    "        This constructor sets up the graph representation of the input data\n",
    "        (`AnnData` or `MuData`) and optionally assigns node types based on a label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obj : muon.MuData or anndata.AnnData\n",
    "            Input data object. If `MuData`, all modalities are extracted; if `AnnData`,\n",
    "            only the provided `modality` is used.\n",
    "        label : str, optional\n",
    "            Column in `.obs` used to assign document labels and node types.\n",
    "            If provided, the graph is annotated accordingly.\n",
    "        max_depth : int, default=6\n",
    "            Maximum number of levels to save or annotate in the hierarchical self.\n",
    "        modality : str, default=\"Mod1\"\n",
    "            Name of the modality to use when the input is `AnnData`.\n",
    "        saving_path : str, default=\"results/myself\"\n",
    "            Base path for saving self outputs (graph, state, results).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - For `MuData`, multiple modalities are combined into a multi-branch graph.\n",
    "        - If `label` is provided, a mapping is created to encode document/node types.\n",
    "        - `self.g` (graph) and related attributes (`documents`, `words`, `keywords`)\n",
    "          are initialized by calling `self.make_graph(...)`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.keywords: List = []\n",
    "        self.nbranches: int = 1\n",
    "        self.modalities: List[str] = []\n",
    "        self.max_depth: int = max_depth\n",
    "        self.obj: Any = obj\n",
    "        self.saving_path: str = saving_path\n",
    "        self.path_to_graph = path_to_graph\n",
    "\n",
    "        if isinstance(obj, MuData):\n",
    "            self.modalities=list(obj.mod.keys())   \n",
    "            dfs=[obj[key].to_df().T for key in self.modalities]\n",
    "            self.make_graph(dfs[0], dfs[1:])\n",
    "\n",
    "        elif isinstance(obj, AnnData):\n",
    "            self.modalities=[modality]\n",
    "            self.make_graph(obj.to_df().T, [])\n",
    "\n",
    "        if label:\n",
    "            g_raw=self.g.copy()\n",
    "            logger.info(\"Label found\")\n",
    "            metadata=obj[self.modalities[0]].obs\n",
    "            mymap = dict([(y,str(x)) for x,y in enumerate(sorted(set(obj[self.modalities[0]].obs[label])))])\n",
    "            inv_map = {v: k for k, v in mymap.items()}\n",
    "\n",
    "            docs_type=[int(mymap[metadata.loc[doc][label]]) for doc in self.documents]\n",
    "            types={}\n",
    "            types[\"Docs\"]=docs_type\n",
    "            for i, key in enumerate(self.modalities):\n",
    "                types[key]=[int(i+np.max(docs_type)+1) for a in range(0, obj[key].shape[0])]\n",
    "            node_type = g_raw.new_vertex_property('int', functools.reduce(lambda a, b : a+b, list(types.values())))\n",
    "            self.g = g_raw.copy()\n",
    "        else:\n",
    "            node_type=None\n",
    "        self.node_type=node_type \n",
    "\n",
    "        \n",
    "    def make_graph(self, df_one: pd.DataFrame, df_keyword_list: List[pd.DataFrame]) -> None:\n",
    "        df_all = df_one.copy(deep =True)\n",
    "        for ikey,df_keyword in enumerate(df_keyword_list):\n",
    "            df_keyword = df_keyword.reindex(columns=df_one.columns)\n",
    "            df_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "            df_keyword[\"kind\"] = ikey+2\n",
    "            df_all = pd.concat((df_all, df_keyword), axis=0)\n",
    "        if len(df_keyword_list) > 0:\n",
    "            del df_keyword_list\n",
    "        del df_one\n",
    "\n",
    "        kinds=pd.DataFrame(df_all[\"kind\"].fillna(1))\n",
    "\n",
    "        self.nbranches = len(df_keyword_list)\n",
    "        del df_keyword_list\n",
    "            \n",
    "        df_all.drop(\"kind\", axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        \n",
    "        self.g = Graph(directed=False)\n",
    "\n",
    "        n_docs, n_words = df_all.shape[1], df_all.shape[0]\n",
    "\n",
    "        # Add all vertices first\n",
    "        self.g.add_vertex(n_docs + n_words)\n",
    "\n",
    "        # Create vertex properties\n",
    "        name = self.g.new_vp(\"string\")\n",
    "        kind = self.g.new_vp(\"int\")\n",
    "        self.g.vp[\"name\"] = name\n",
    "        self.g.vp[\"kind\"] = kind\n",
    "\n",
    "        # Assign doc vertices (loop for names, array for kind)\n",
    "        for i, doc in enumerate(df_all.columns):\n",
    "            name[self.g.vertex(i)] = doc\n",
    "        kind.get_array()[:n_docs] = 0\n",
    "\n",
    "        # Assign word vertices (loop for names, array for kind)\n",
    "        for j, word in enumerate(df_all.index):\n",
    "            name[self.g.vertex(n_docs + j)] = word\n",
    "        kind.get_array()[n_docs:] = np.array([int(kinds.at[w,\"kind\"]) for w in df_all.index], dtype=int)\n",
    "\n",
    "        # Edge weights\n",
    "        weight = self.g.new_ep(\"int\")\n",
    "        self.g.ep[\"count\"] = weight\n",
    "\n",
    "        # Build sparse edges\n",
    "        rows, cols = df_all.values.nonzero()\n",
    "        vals = df_all.values[rows, cols].astype(int)\n",
    "        edges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "        if len(edges)==0: raise ValueError(\"Empty graph\")\n",
    "\n",
    "        self.g.add_edge_list(edges, eprops=[weight])\n",
    "\n",
    "        # Remove edges with 0 weight\n",
    "        filter_edges = self.g.new_edge_property(\"bool\")\n",
    "        filter_edges.a = weight.a > 0\n",
    "        self.g.set_edge_filter(filter_edges)\n",
    "        self.g.purge_edges()\n",
    "        self.g.clear_filters()\n",
    "\n",
    "        self.documents = df_all.columns\n",
    "        self.words = df_all.index[self.g.vp['kind'].a[n_docs:] == 1]\n",
    "        for ik in range(2, 2 + self.nbranches):\n",
    "            self.keywords.append(df_all.index[self.g.vp['kind'].a[n_docs:] == ik])\n",
    "\n",
    "\n",
    "    def fit(self, n_init=1, verbose=True, deg_corr=True, overlap=False, parallel=False, B_min=0, B_max=None, clabel=None, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Fit a nested stochastic block self to the graph using `minimize_nested_blockmodel_dl`.\n",
    "    \n",
    "        This method performs multiple initializations and keeps the best self \n",
    "        based on the minimum description length (entropy). It supports degree-corrected \n",
    "        and overlapping block selfs, and can perform parallel moves for efficiency.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_init : int, default=1\n",
    "            Number of random initializations. The self with the lowest entropy is retained.\n",
    "        verbose : bool, default=True\n",
    "            If True, print progress messages.\n",
    "        deg_corr : bool, default=True\n",
    "            If True, use a degree-corrected block self.\n",
    "        overlap : bool, default=False\n",
    "            If True, use an overlapping block self.\n",
    "        parallel : bool, default=False\n",
    "            If True, perform parallel moves during optimization.\n",
    "        B_min : int, default=0\n",
    "            Minimum number of blocks to consider.\n",
    "        B_max : int, optional\n",
    "            Maximum number of blocks to consider. Defaults to the number of vertices.\n",
    "        clabel : str or property map, optional\n",
    "            Vertex property to use as initial block assignment. If None, the 'kind' \n",
    "            vertex property is used.\n",
    "        *args : positional arguments\n",
    "            Additional positional arguments passed to `minimize_nested_blockmodel_dl`.\n",
    "        **kwargs : keyword arguments\n",
    "            Additional keyword arguments passed to `minimize_nested_blockmodel_dl`. \n",
    "        \"\"\"\n",
    "        if clabel == None:\n",
    "            clabel = self.g.vp['kind']\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "        else:\n",
    "            logger.info(\"Clabel is %s, assigning partitions to vertices\", clabel)\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "    \n",
    "        state_args[\"eweight\"] = self.g.ep.count\n",
    "        min_entropy = np.inf\n",
    "        best_state = None\n",
    "        state_args[\"deg_corr\"] = deg_corr\n",
    "        state_args[\"overlap\"] = overlap\n",
    "\n",
    "        if B_max is None:\n",
    "            B_max = self.g.num_vertices()\n",
    "            \n",
    "        multilevel_mcmc_args={\"B_min\": B_min, \"B_max\": B_max, \"verbose\": verbose,\"parallel\" : parallel}\n",
    "\n",
    "        for i in range(n_init):\n",
    "            logger.info(\"Fit number: %d\", i)\n",
    "            state = minimize_nested_blockmodel_dl(self.g, state_args=state_args, multilevel_mcmc_args=multilevel_mcmc_args, *args, **kwargs)\n",
    "            \n",
    "            entropy = state.entropy()\n",
    "            if entropy < min_entropy:\n",
    "                min_entropy = entropy\n",
    "                self.state = state\n",
    "                \n",
    "        self.mdl = min_entropy\n",
    "\n",
    "        self.L = len(self.state.levels)\n",
    "\n",
    "        self.groups = {}\n",
    "        logger.info(\"Saving data in %s\", self.saving_path)\n",
    "        self.save_data()\n",
    "\n",
    "        logger.info(\"Annotate object\")\n",
    "        self.annotate_obj()\n",
    "\n",
    "    def get_groups(self, l=0):\n",
    "        \"\"\"\n",
    "        Compute group-level summary matrices\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l : int, default=0\n",
    "            Hierarchical level to project the fitted nested blockmodel to.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary with the following keys (matching the original `get_groups`):\n",
    "            - 'Bd' : int\n",
    "                Number of active document groups (after pruning empty columns).\n",
    "            - 'Bw' : int\n",
    "                Number of active word groups (after pruning).\n",
    "            - 'Bk' : list[int]\n",
    "                Number of active keyword groups per keyword branch (after pruning).\n",
    "            - 'p_tw_w' : np.ndarray, shape (Bw, W)\n",
    "                Group membership of each word node: P(t_w | w). Rows sum to 1; rows\n",
    "                corresponding to words with zero mass are all-NaN.\n",
    "            - \"p_tk_w_key\" : list[np.ndarray]\n",
    "                For each keyword branch `ik`, matrix of shape (Bk[ik], K[ik]) with\n",
    "                P(t_k | keyword). Rows with zero mass are all-NaN.\n",
    "            - 'p_td_d' : np.ndarray, shape (Bd, D)\n",
    "                Group membership of each document node: P(t_d | d). Rows with zero\n",
    "                mass are all-NaN.\n",
    "            - 'p_w_tw' : np.ndarray, shape (W, Bw)\n",
    "                Topic distribution for words: P(w | t_w). Columns with zero mass\n",
    "                are all-NaN.\n",
    "            - 'p_w_key_tk' : list[np.ndarray]\n",
    "                For each keyword branch `ik`, matrix of shape (K[ik], Bk[ik]) with\n",
    "                P(keyword | t_k). Columns with zero mass are all-NaN.\n",
    "            - 'p_tw_d' : np.ndarray, shape (Bw, D)\n",
    "                Mixture of word-groups in documents: P(t_w | d). Rows with zero\n",
    "                mass are all-NaN.\n",
    "            - 'p_tk_d' : list[np.ndarray]\n",
    "                For each keyword branch `ik`, matrix of shape (Bk[ik], D) with\n",
    "                P(t_k | d). Rows with zero mass are all-NaN.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if l in self.groups.keys():\n",
    "            return self.groups[l]\n",
    "        # --- project to level; non-overlap for speed & simple b[v] array ---\n",
    "        state_l = self.state.project_level(l).copy(overlap=False)\n",
    "        b_arr = state_l.get_blocks().a.astype(np.int64)\n",
    "        B = int(state_l.get_B())\n",
    "    \n",
    "        # --- basic shapes ---\n",
    "        D, W, K = self.get_shape()\n",
    "        K = list(K)\n",
    "        K_cumsum = np.cumsum([0] + K)                  # [0, K0, K0+K1, ...]\n",
    "        KW_offsets = D + W + K_cumsum[:-1]             # global start index for each keyword branch\n",
    "    \n",
    "        # --- pull edges in bulk: src, tgt, weight ---\n",
    "        e_mat = self.g.get_edges(eprops=[self.g.ep[\"count\"]])\n",
    "        src = e_mat[:, 0].astype(np.int64)\n",
    "        tgt = e_mat[:, 1].astype(np.int64)\n",
    "        w   = e_mat[:, 2].astype(np.int64)\n",
    "    \n",
    "        z_src = b_arr[src]\n",
    "        z_tgt = b_arr[tgt]\n",
    "    \n",
    "        kind = self.g.vp['kind'].a\n",
    "        kind_tgt = kind[tgt]\n",
    "    \n",
    "        # --- alloc accumulators ---\n",
    "        n_wb      = np.zeros((W, B), dtype=np.int64)\n",
    "        n_db      = np.zeros((D, B), dtype=np.int64)\n",
    "        n_dbw     = np.zeros((D, B), dtype=np.int64)\n",
    "        n_w_key_b = [np.zeros((K[ik], B), dtype=np.int64) for ik in range(self.nbranches)]\n",
    "        n_dbw_key = [np.zeros((D, B), dtype=np.int64)     for _  in range(self.nbranches)]\n",
    "    \n",
    "        # --- accumulate (vectorized) ---\n",
    "        # docs are sources by construction\n",
    "        np.add.at(n_db, (src, z_src), w)\n",
    "    \n",
    "        # words as targets (kind == 1)\n",
    "        mask_w = (kind_tgt == 1)\n",
    "        if mask_w.any():\n",
    "            w_idx = tgt[mask_w] - D\n",
    "            np.add.at(n_wb,  (w_idx,           z_tgt[mask_w]), w[mask_w])\n",
    "            np.add.at(n_dbw, (src[mask_w],     z_tgt[mask_w]), w[mask_w])\n",
    "    \n",
    "        # keywords as targets (kind >= 2)\n",
    "        if self.nbranches > 0:\n",
    "            mask_kw = (kind_tgt >= 2)\n",
    "            if mask_kw.any():\n",
    "                kw_kinds = kind_tgt[mask_kw]  # values in {2,3,...}\n",
    "                sel_kw = np.where(mask_kw)[0]\n",
    "                for ik in range(self.nbranches):\n",
    "                    m = (kw_kinds == (ik + 2))\n",
    "                    if not m.any():\n",
    "                        continue\n",
    "                    sel = sel_kw[m]\n",
    "                    kw_local = tgt[sel] - KW_offsets[ik]\n",
    "                    np.add.at(n_w_key_b[ik], (kw_local,   z_tgt[sel]), w[sel])\n",
    "                    np.add.at(n_dbw_key[ik], (src[sel],   z_tgt[sel]), w[sel])\n",
    "    \n",
    "        # --- prune empty columns exactly like original ---\n",
    "        ind_d = np.where(np.sum(n_db,  axis=0) > 0)[0];  Bd = len(ind_d);  n_db  = n_db[:,  ind_d]\n",
    "        ind_w = np.where(np.sum(n_wb,  axis=0) > 0)[0];  Bw = len(ind_w);  n_wb  = n_wb[:,  ind_w]\n",
    "        ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0];                       n_dbw = n_dbw[:, ind_w2]\n",
    "    \n",
    "        ind_w_key, ind_w2_keyword, Bk = [], [], []\n",
    "        for ik in range(self.nbranches):\n",
    "            idx1 = np.where(np.sum(n_w_key_b[ik],  axis=0) > 0)[0]\n",
    "            idx2 = np.where(np.sum(n_dbw_key[ik],  axis=0) > 0)[0]\n",
    "            ind_w_key.append(idx1); ind_w2_keyword.append(idx2); Bk.append(len(idx1))\n",
    "            n_w_key_b[ik] = n_w_key_b[ik][:, idx1]\n",
    "            n_dbw_key[ik] = n_dbw_key[ik][:, idx2]\n",
    "    \n",
    "        # --- NaN-preserving normalizers (match original semantics) ---\n",
    "        def _row_norm_nan(M: np.ndarray) -> np.ndarray:\n",
    "            M = M.astype(float, copy=False)\n",
    "            s = M.sum(axis=1, keepdims=True)\n",
    "            out = np.full_like(M, np.nan, dtype=float)\n",
    "            valid = (s[:, 0] != 0)\n",
    "            if np.any(valid):\n",
    "                out[valid] = M[valid] / s[valid]\n",
    "            return out.T  # original returns transposed\n",
    "    \n",
    "        def _col_norm_nan(M: np.ndarray) -> np.ndarray:\n",
    "            M = M.astype(float, copy=False)\n",
    "            s = M.sum(axis=0, keepdims=True)\n",
    "            out = np.full_like(M, np.nan, dtype=float)\n",
    "            valid = (s[0] != 0)\n",
    "            if np.any(valid):\n",
    "                out[:, valid] = M[:, valid] / s[:, valid]\n",
    "            return out\n",
    "    \n",
    "        # --- probabilities (identical layout to original) ---\n",
    "        p_tw_w      = _row_norm_nan(n_wb)\n",
    "        p_tk_w_key  = [_row_norm_nan(n_w_key_b[ik]) for ik in range(self.nbranches)]\n",
    "        p_w_tw      = _col_norm_nan(n_wb)\n",
    "        p_w_key_tk  = [_col_norm_nan(n_w_key_b[ik]) for ik in range(self.nbranches)]\n",
    "        p_tw_d      = _row_norm_nan(n_dbw)\n",
    "        p_tk_d      = [_row_norm_nan(n_dbw_key[ik]) for ik in range(self.nbranches)]\n",
    "        p_td_d      = _row_norm_nan(n_db)\n",
    "    \n",
    "        result = dict(\n",
    "            Bd=Bd, Bw=Bw, Bk=Bk,\n",
    "            p_tw_w=p_tw_w, p_tk_w_key=p_tk_w_key, p_td_d=p_td_d,\n",
    "            p_w_tw=p_w_tw, p_w_key_tk=p_w_key_tk, p_tw_d=p_tw_d, p_tk_d=p_tk_d\n",
    "        )\n",
    "        self.groups[l] = result\n",
    "        return result\n",
    "\n",
    "    # Helper functions\n",
    "    def save_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "        \"\"\"\n",
    "        Save the graph\n",
    "\n",
    "        :param filename: name of the graph stored\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving graph to %s\", filename)\n",
    "        self.g.save(filename)\n",
    "    \n",
    "    \n",
    "    def load_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "        \"\"\"\n",
    "        Load a saved graph from disk and rebuild documents, words, and keywords.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Path to the saved graph file (default: \"graph.xml.gz\").\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading graph from %s\", filename)\n",
    "\n",
    "        self.g = load_graph(filename)\n",
    "        self.documents = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 0]\n",
    "        self.words = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 1]\n",
    "        metadata_indexes = np.unique(self.g.vp[\"kind\"].a)\n",
    "        metadata_indexes = metadata_indexes[metadata_indexes > 1] #no doc or words\n",
    "        self.nbranches = len(metadata_indexes)\n",
    "        for i_keyword in metadata_indexes:\n",
    "            self.keywords.append([self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == i_keyword])\n",
    "\n",
    "\n",
    "    def save_single_level(self, l: int) -> None:\n",
    "        \"\"\"\n",
    "        Save per-level probability matrices (topics, clusters, documents) for the given level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l : int\n",
    "            The level index to save. Must be within the range of available self levels.\n",
    "        saving in self.saving_path_path : str\n",
    "            Base path (folder + prefix) where files will be written.\n",
    "            Example: \"results/myself\" → files like:\n",
    "                - results/myself_level_0_mainfeature_topics.tsv.gz\n",
    "                - results/myself_level_0_clusters.tsv.gz\n",
    "                - results/myself_level_0_mainfeature_topics_documents.tsv.gz\n",
    "                - results/myself_level_0_metafeature_topics.tsv.gz\n",
    "                - results/myself_level_0_metafeature_topics_documents.tsv.gz\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Files are written as tab-separated values (`.tsv.gz`) with gzip compression.\n",
    "        - Raises RuntimeError if any file cannot be written.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Validate inputs ---\n",
    "        if not isinstance(l, int) or l < 0 or l >= len(self.state.levels) or l >= len(self.state.levels):\n",
    "            raise ValueError(f\"Invalid level index {l}. Must be between 0 and {len(self.state.levels) - 1}.\")\n",
    "        if not isinstance(self.saving_path, str) or not self.saving_path.strip():\n",
    "            raise ValueError(\"`self.saving_path` must be a non-empty string path prefix.\")\n",
    "\n",
    "        main_feature = self.modalities[0]\n",
    "\n",
    "        try:\n",
    "            data = self.get_groups(l)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to get group data for level {l}: {e}\") from e\n",
    "\n",
    "        # Helper to safely save a DataFrame\n",
    "        def _safe_save(df, filepath):\n",
    "            try:\n",
    "                Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "                df.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "        # --- P(document | cluster) ---\n",
    "        clusters = pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)\n",
    "        _safe_save(clusters, f\"{self.saving_path}_level_{l}_clusters.tsv.gz\")\n",
    "\n",
    "\n",
    "        # --- P(main_feature | main_topic) ---\n",
    "        p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_w_tw, f\"{self.saving_path}_level_{l}_{main_feature}_topics.tsv.gz\")\n",
    "\n",
    "        # --- P(main_topic | documents) ---\n",
    "        p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_tw_d, f\"{self.saving_path}_level_{l}_{main_feature}_topics_documents.tsv.gz\")\n",
    "\n",
    "        # --- P(meta_feature | meta_topic_feature), if any ---\n",
    "        if len(self.modalities) > 1:\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "                    columns=[f\"{meta_features}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(p_w_tw, f\"{self.saving_path}_level_{l}_{meta_features}_topics.tsv.gz\")\n",
    "\n",
    "\n",
    "            # --- P(meta_topic | document) ---\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                p_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "                    columns=[f\"{meta_features}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(p_tw_d, f\"{self.saving_path}_level_{l}_{meta_features}_topics_documents.tsv.gz\")\n",
    "\n",
    "\n",
    "    def save_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the global graph, self, state, and level-specific data for the current nSBM self.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        saving in self.saving_pathg_path : str, optional\n",
    "            Base path (folder + prefix) where all outputs will be saved.\n",
    "            Example: \"results/myself\" will produce:\n",
    "                - results/myself_graph.xml.gz\n",
    "                - results/myself_model.pkl    \n",
    "                - results/myself_entropy.txt\n",
    "                - results/myself_state.pkl\n",
    "                - results/myself_level_X_*.tsv.gz  (per level, up to 6 levels)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The parent folder is created automatically if it does not exist.\n",
    "        - Level saving is parallelized with threads for efficiency in I/O.\n",
    "        - By default, at most self.max_depth levels are saved, or fewer if the self has <self.max_depth levels.\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving self data to %s\", self.saving_path)\n",
    "\n",
    "        L = min(len(self.state.levels), self.max_depth)\n",
    "        self.L = L\n",
    "        if L == 0:\n",
    "            logger.warning(\"Nothing to save (no levels found)\")\n",
    "            return\n",
    "        \n",
    "        folder = os.path.dirname(self.saving_path)\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            self.save_graph(filename=f\"{self.saving_path}_graph.xml.gz\")\n",
    "            self.dump_model(filename=f\"{self.saving_path}_model.pkl\")\n",
    "\n",
    "            with open(f\"{self.saving_path}_entropy.txt\", \"w\") as f:\n",
    "                f.write(str(self.state.entropy()))\n",
    "\n",
    "            with open(f\"{self.saving_path}_state.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.state, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to save global files: %s\", e)\n",
    "            raise RuntimeError(f\"Failed to save global files for self '{self.saving_path}': {e}\") from e\n",
    "\n",
    "\n",
    "        errors = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(self.save_single_level, l): l for l in range(L)}\n",
    "            for future in as_completed(futures):\n",
    "                l = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    errors.append((l, str(e)))\n",
    "\n",
    "        if errors:\n",
    "            msg = \"; \".join([f\"Level {l}: {err}\" for l, err in errors])\n",
    "            logger.error(\"Errors occurred while saving levels: %s\", msg)\n",
    "            raise RuntimeError(f\"Errors occurred while saving levels: {msg}\")\n",
    "\n",
    "\n",
    "    def annotate_obj(self) -> None:\n",
    "        L = min(len(self.state.levels), self.max_depth)\n",
    "        for l in range(0,L):\n",
    "            main_feature = self.modalities[0]\n",
    "            data = self.get_groups(l)\n",
    "            self.obj.obs[f\"Level_{l}_cluster\"]=np.argmax(pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)[self.obj.obs.index], axis=0).astype(str)\n",
    "            \n",
    "    \n",
    "            if isinstance(self.obj, MuData):\n",
    "                order_var=self.obj[main_feature].var.index\n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "                                columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "                self.obj[main_feature].var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "            elif isinstance(self.obj, AnnData):\n",
    "                order_var=self.obj.var.index             \n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "                                columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "                self.obj.var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "            \n",
    "            p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "                    columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[self.obj.obs.index]\n",
    "            p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "            self.obj.obs[f\"Level_{l}_{main_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "        \n",
    "            if len(self.modalities) > 1:\n",
    "                for k, meta_feature in enumerate(self.modalities[1:]):\n",
    "                    p_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "                        columns=[f\"{meta_feature}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                    self.obj[meta_feature].var[f\"Level_{l}_{meta_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "            \n",
    "                # --- P(meta_topic | document) ---\n",
    "                for k, meta_feature in enumerate(self.modalities[1:]):\n",
    "                    p_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "                        columns=[f\"{meta_feature}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                    p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "                    self.obj.obs[f\"Level_{l}_{meta_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\n",
    "    def dump_model(self, filename=\"bionsbm.pkl\"):\n",
    "        \"\"\"\n",
    "        Dump self using pickle\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"Dumping self to %s\", filename)\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load_model(self, filename=\"bionsbm.pkl\"):\n",
    "        logger.info(\"Loading self from %s\", filename)\n",
    "\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self = pickle.load(f)\n",
    "        return self\n",
    "\n",
    "    def get_V(self):\n",
    "        '''\n",
    "        return number of word-nodes == types\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 1))  # no. of types\n",
    "\n",
    "    def get_D(self):\n",
    "        '''\n",
    "        return number of doc-nodes == number of documents\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 0))  # no. of types\n",
    "\n",
    "    def get_N(self):\n",
    "        '''\n",
    "        return number of edges == tokens\n",
    "        '''\n",
    "        return int(self.g.num_edges())  # no. of types\n",
    "\n",
    "\n",
    "    def get_mdl(self):\n",
    "        \"\"\"\n",
    "        Get minimum description length\n",
    "\n",
    "        Proxy to self.state.entropy()\n",
    "        \"\"\"\n",
    "        return self.mdl\n",
    "            \n",
    "    def get_shape(self):\n",
    "        \"\"\"\n",
    "        :return: list of tuples (number of documents, number of words, (number of keywords,...))\n",
    "        \"\"\"\n",
    "        D = int(np.sum(self.g.vp['kind'].a == 0)) #documents\n",
    "        W = int(np.sum(self.g.vp['kind'].a == 1)) #words\n",
    "        K = [int(np.sum(self.g.vp['kind'].a == (k+2))) for k in range(self.nbranches)] #keywords\n",
    "        return D, W, K\n",
    "\n",
    "##### Drawing\n",
    "    def draw(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Draw the network\n",
    "\n",
    "        :param \\*args: positional arguments to pass to self.state.draw\n",
    "        :param \\*\\*kwargs: keyword argument to pass to self.state.draw\n",
    "        \"\"\"\n",
    "        colmap = self.g.vertex_properties[\"color\"] = self.g.new_vertex_property(\n",
    "            \"vector<double>\")\n",
    "        #https://medialab.github.io/iwanthue/\n",
    "        colors = [  [174,80,209],\n",
    "                    [108,192,70],\n",
    "                    [207, 170, 60],\n",
    "                    [131,120,197],\n",
    "                    [126,138,65],\n",
    "                    [201,90,138],\n",
    "                    [87,172,125],\n",
    "                    [213,73,57],\n",
    "                    [85,175,209],\n",
    "                    [193,120,81]]\n",
    "        for v in self.g.vertices():\n",
    "            k = self.g.vertex_properties['kind'][v]\n",
    "            if k < 10:\n",
    "                color = np.array(colors[k])/255.\n",
    "            else:\n",
    "                color = np.array([187, 129, 164])/255.\n",
    "            colmap[v] = color\n",
    "        self.state.draw(\n",
    "            subsample_edges = 5000, \n",
    "            edge_pen_width = self.g.ep[\"count\"],\n",
    "            vertex_color=colmap,\n",
    "            vertex_fill_color=colmap, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f70ae82-7a1a-4b66-9f26-802ef454a1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from muon import read_h5mu\n",
    "mdata = read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d3b910d-b02d-4c97-b04e-183f320704ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/topmod/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/topmod/lib/python3.11/site-packages/pandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/topmod/lib/python3.11/site-packages/pandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kind'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbionsbm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlncRNA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlncRNA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/lncRNA/mybionsbm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mg\n",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m, in \u001b[0;36mbionsbm.__init__\u001b[0;34m(self, obj, label, max_depth, modality, saving_path, path_to_graph)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, AnnData):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodalities\u001b[38;5;241m=\u001b[39m[modality]\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label:\n\u001b[1;32m    103\u001b[0m     g_raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[10], line 132\u001b[0m, in \u001b[0;36mbionsbm.make_graph\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m df_keyword_list\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df_one\n\u001b[0;32m--> 132\u001b[0m kinds\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdf_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkind\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbranches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_keyword_list)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df_keyword_list\n",
      "File \u001b[0;32m~/miniconda3/envs/topmod/lib/python3.11/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/topmod/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kind'"
     ]
    }
   ],
   "source": [
    "model = bionsbm(obj=mdata[\"lncRNA\"], modality=\"lncRNA\", saving_path=\"results/lncRNA/mybionsbm\")\n",
    "model.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c889d318-1e00-4e43-bea8-0adf05071c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 14:33:50,583 - INFO - Fit number: 0\n",
      "2025-09-19 14:34:09,565 - INFO - Saving data in results/mybionsbm\n",
      "2025-09-19 14:34:09,566 - INFO - Saving self data to results/mybionsbm\n",
      "2025-09-19 14:34:09,566 - INFO - Saving graph to results/mybionsbm_graph.xml.gz\n",
      "2025-09-19 14:34:09,662 - INFO - Dumping self to results/mybionsbm_model.pkl\n",
      "2025-09-19 14:34:10,208 - INFO - Annotate object\n"
     ]
    }
   ],
   "source": [
    "model.fit(n_init=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ea12d-b921-4993-a977-98db216a65d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fec10-a1ab-46ed-ac21-73c63f334e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd37b9d-8bd3-4c10-860e-f72e25ca66af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d6955-4f0d-4a16-a1d8-0d90a62fbf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
