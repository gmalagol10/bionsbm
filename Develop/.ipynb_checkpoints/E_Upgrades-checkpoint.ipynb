{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3393d4d3-60ae-4d85-a962-f6441a69eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "import os, sys\n",
    "import logging\n",
    "\n",
    "from graph_tool.all import load_graph, Graph, minimize_nested_blockmodel_dl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle as pickle\n",
    "\n",
    "from muon import MuData\n",
    "from anndata import AnnData\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from scipy import sparse\n",
    "from numba import njit\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:  # prevent adding multiple handlers\n",
    "    ch = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "######################################\n",
    "import time\n",
    "from muon import read_h5mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ff57d-54a6-43d0-a799-901acd0d72ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdata=read_h5mu(\"Datasets/PBMC/CM/PBMC_Peak_mRNA_lncRNA_Mowgli_Def.h5mu\")\n",
    "mdata_red=MuData({\"Peak\" : mdata[\"Peak\"][:1000, :1000].copy(), \"mRNA\" : mdata[\"mRNA\"][:1000, :700].copy(), \"lncRNA\" : mdata[\"lncRNA\"][:1000, :500].copy()})\n",
    "for mod in mdata_red.mod:\n",
    "    print(mod)\n",
    "    print(type(mdata_red[mod].X))\n",
    "    mdata_red[mod].X=scipy.sparse.csr_matrix(mdata_red[mod].X)\n",
    "    print(type(mdata_red[mod].X))\n",
    "\n",
    "del mdata_red[\"Peak\"].uns, mdata_red[\"mRNA\"].uns, mdata_red[\"lncRNA\"].uns, mdata_red[\"mRNA\"].obs, mdata_red[\"lncRNA\"].obs\n",
    "mdata_red.var=pd.DataFrame(index=mdata_red.var.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8cc2b-ce90-4ef7-8144-2158a02d1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata_red.write(\"../Test_data.h5mu\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89375a85-0fa5-47e1-bc21-e31440271d6a",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45574dfe-1617-4d4d-82bc-2945f2545bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_mudata(\n",
    "    mdata: MuData,\n",
    "    cell_frac: float = 0.5,\n",
    "    feat_frac: float = None,\n",
    "    strat_mod: str = \"Peak\",\n",
    "    strat_col: str = \"CellType\",\n",
    "    random_seed: int = 42\n",
    ") -> MuData:\n",
    "    \"\"\"\n",
    "    Subsample a MuData object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdata : MuData\n",
    "        The MuData object to subsample.\n",
    "    cell_frac : float\n",
    "        Fraction of cells to keep.\n",
    "    feat_frac : float\n",
    "        Fraction of features to keep in each modality.\n",
    "    strat_mod : str\n",
    "        Name of modality to use for stratified cell sampling.\n",
    "    strat_col : str\n",
    "        Column in .obs to use for stratification.\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MuData\n",
    "        Subsampled MuData object.\n",
    "    \"\"\"\n",
    "    if feat_frac is None:\n",
    "        feat_frac = cell_frac\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # --- Subsample cells stratified ---\n",
    "    celltypes = mdata[strat_mod].obs[strat_col]\n",
    "    n_cells = int(np.ceil(cell_frac * mdata.n_obs))\n",
    "    \n",
    "    # compute group sizes proportionally\n",
    "    counts = (celltypes.value_counts(normalize=True) * n_cells).round().astype(int)\n",
    "    \n",
    "    sampled_idx = []\n",
    "    for ct, n in counts.items():\n",
    "        sampled_idx.extend(celltypes[celltypes == ct].sample(n, random_state=random_seed).index)\n",
    "    \n",
    "    mdata_cells = mdata[sampled_idx, :]\n",
    "    \n",
    "    # --- Subsample features per modality ---\n",
    "    mod_subsampled = {}\n",
    "    for mod in mdata_cells.mod.keys():\n",
    "        adata = mdata_cells[mod]\n",
    "        n_feats = int(np.ceil(feat_frac * adata.n_vars))\n",
    "        n_feats = min(n_feats, adata.n_vars)\n",
    "        feat_idx = np.random.choice(adata.n_vars, size=n_feats, replace=False)\n",
    "        mod_subsampled[mod] = adata[:, feat_idx].copy()\n",
    "    \n",
    "    # Return new MuData object\n",
    "    return MuData(mod_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cebde2-885d-469c-83b5-2c40e99fce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bionsbm():\n",
    "    \"\"\"\n",
    "    Class to run bionsbm\n",
    "    \"\"\"\n",
    "    def __init__(self, obj, label: Optional[str] = None, max_depth: int = 6, modality: str = \"Mod1\", saving_path: str = \"results/mymodel\"):\n",
    "        \"\"\"\n",
    "        Initialize a bionsbm model.\n",
    "\n",
    "        This constructor sets up the graph representation of the input data\n",
    "        (`AnnData` or `MuData`) and optionally assigns node types based on a label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obj : muon.MuData or anndata.AnnData\n",
    "            Input data object. If `MuData`, all modalities are extracted; if `AnnData`,\n",
    "            only the provided `modality` is used.\n",
    "        label : str, optional\n",
    "            Column in `.obs` used to assign document labels and node types.\n",
    "            If provided, the graph is annotated accordingly.\n",
    "        max_depth : int, default=6\n",
    "            Maximum number of levels to save or annotate in the hierarchical model.\n",
    "        modality : str, default=\"Mod1\"\n",
    "            Name of the modality to use when the input is `AnnData`.\n",
    "        saving_path : str, default=\"results/mymodel\"\n",
    "            Base path for saving model outputs (graph, state, results).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - For `MuData`, multiple modalities are combined into a multi-branch graph.\n",
    "        - If `label` is provided, a mapping is created to encode document/node types.\n",
    "        - `self.g` (graph) and related attributes (`documents`, `words`, `keywords`)\n",
    "          are initialized by calling `self.make_graph(...)`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.keywords: List = []\n",
    "        self.nbranches: int = 1\n",
    "        self.modalities: List[str] = []\n",
    "        self.max_depth: int = max_depth\n",
    "        self.obj: Any = obj\n",
    "        self.saving_path: str = saving_path\n",
    "\n",
    "        if isinstance(obj, MuData):\n",
    "            self.modalities=list(obj.mod.keys())   \n",
    "          #  dfs=[obj[key].to_df().T for key in self.modalities]\n",
    "         #   self.make_graph(dfs[0], dfs[1:])\n",
    "\n",
    "        elif isinstance(obj, AnnData):\n",
    "            self.modalities=[modality]\n",
    "        #    self.make_graph(obj.to_df().T, [])\n",
    "\n",
    "        if label:\n",
    "            g_raw=self.g.copy()\n",
    "            logger.info(\"Label found\")\n",
    "            metadata=obj[self.modalities[0]].obs\n",
    "            mymap = dict([(y,str(x)) for x,y in enumerate(sorted(set(obj[self.modalities[0]].obs[label])))])\n",
    "            inv_map = {v: k for k, v in mymap.items()}\n",
    "\n",
    "            docs_type=[int(mymap[metadata.loc[doc][label]]) for doc in self.documents]\n",
    "            types={}\n",
    "            types[\"Docs\"]=docs_type\n",
    "            for i, key in enumerate(self.modalities):\n",
    "                types[key]=[int(i+np.max(docs_type)+1) for a in range(0, obj[key].shape[0])]\n",
    "            node_type = g_raw.new_vertex_property('int', functools.reduce(lambda a, b : a+b, list(types.values())))\n",
    "            self.g = g_raw.copy()\n",
    "        else:\n",
    "            node_type=None\n",
    "        self.node_type=node_type \n",
    "\n",
    "        \n",
    "    def make_graph(self, df: pd.DataFrame, df_keyword_list: List[pd.DataFrame]) -> None:\n",
    "        \"\"\"\n",
    "        Build a heterogeneous graph from a main feature DataFrame and optional keyword/meta-feature DataFrames.\n",
    "\n",
    "        This function constructs a bipartite (documents–words) or multi-branch\n",
    "        graph (documents–words–keywords/meta-features) using the input matrices.\n",
    "        If a cached graph file exists at ``self.saving_path``, it is loaded directly\n",
    "        instead of rebuilding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            Main feature matrix with words/features as rows (index) and\n",
    "            documents/samples as columns.\n",
    "        df_keyword_list : list of pandas.DataFrame\n",
    "            List of additional matrices (e.g., keywords, annotations, or meta-features).\n",
    "            Each DataFrame must have the same columns as ``df`` (documents),\n",
    "            and its rows will be treated as a separate feature branch.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Each branch is assigned a unique ``kind`` index:\n",
    "          * 0 → documents\n",
    "          * 1 → main features (e.g., words/genes)\n",
    "          * 2, 3, ... → subsequent keyword/meta-feature branches\n",
    "        - If a saved graph already exists at\n",
    "          ``{self.saving_path}_graph.xml.gz``, it will be loaded instead of recreated.\n",
    "        - After graph construction, the graph is saved to disk in Graph-Tool format.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If ``df`` and ``df_keyword_list`` cannot be aligned properly\n",
    "            (e.g., inconsistent columns).\n",
    "        \"\"\"\n",
    "        if os.path.isfile(f\"{self.saving_path}_graph.xml.gz\") == True: \n",
    "            self.load_graph(filename=f\"{self.saving_path}_graph.xml.gz\")\n",
    "        else:  \n",
    "            logger.info(\"Creating graph from multiple DataFrames\")\n",
    "            df_all = df.copy(deep =True)\n",
    "            for ikey,df_keyword in enumerate(df_keyword_list):\n",
    "                df_keyword = df_keyword.reindex(columns=df.columns)\n",
    "                df_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "                df_keyword[\"kind\"] = ikey+2\n",
    "                df_all = pd.concat((df_all,df_keyword), axis=0)\n",
    "   \n",
    "            def get_kind(word):\n",
    "                return 1 if word in df.index else df_all.at[word,\"kind\"]\n",
    "   \n",
    "            self.nbranches = len(df_keyword_list)\n",
    "           \n",
    "            self.make_graph_single(df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "\n",
    "            folder = os.path.dirname(self.saving_path)\n",
    "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "            self.save_graph(filename=f\"{self.saving_path}_graph.xml.gz\")\n",
    "\n",
    "\n",
    "    def make_graph_single(self, df: pd.DataFrame, get_kind) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Construct a graph-tool graph from a single feature matrix.\n",
    "\n",
    "        This method builds a bipartite or multi-branch graph from the given\n",
    "        DataFrame, where columns represent documents/samples and rows represent\n",
    "        features (e.g., words, genes, or keywords). Vertices are created for\n",
    "        both documents and features, and weighted edges connect documents to\n",
    "        their features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            Feature matrix with rows as features (words, genes, or keywords)\n",
    "            and columns as documents/samples. The values must be numeric and\n",
    "            represent counts or weights of feature occurrences.\n",
    "        get_kind : callable\n",
    "            Function that takes a feature name (row index from ``df``) and\n",
    "            returns an integer specifying the vertex kind:\n",
    "            - 0 → document nodes\n",
    "            - 1 → main feature nodes\n",
    "            - 2, 3, ... → keyword/meta-feature branch nodes\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The constructed graph is undirected.\n",
    "        - Vertices are annotated with two properties:\n",
    "          * ``name`` (string): document or feature name.\n",
    "          * ``kind`` (int): node type (document, word, or keyword branch).\n",
    "        - Edges are annotated with ``count`` (int), representing the weight.\n",
    "        - Edges with zero weight are removed after construction.\n",
    "        - The graph is stored in ``self.g``\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the resulting graph has no edges (i.e., ``df`` is empty or contains only zeros).    \n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(\"Building graph with %d docs and %d words\", df.shape[1], df.shape[0])\n",
    "        self.g = Graph(directed=False)\n",
    "\n",
    "        n_docs, n_words = df.shape[1], df.shape[0]\n",
    "\n",
    "        # Add all vertices first\n",
    "        self.g.add_vertex(n_docs + n_words)\n",
    "\n",
    "        # Create vertex properties\n",
    "        name = self.g.new_vp(\"string\")\n",
    "        kind = self.g.new_vp(\"int\")\n",
    "        self.g.vp[\"name\"] = name\n",
    "        self.g.vp[\"kind\"] = kind\n",
    "\n",
    "        # Assign doc vertices (loop for names, array for kind)\n",
    "        for i, doc in enumerate(df.columns):\n",
    "            name[self.g.vertex(i)] = doc\n",
    "        kind.get_array()[:n_docs] = 0\n",
    "\n",
    "        # Assign word vertices (loop for names, array for kind)\n",
    "        for j, word in enumerate(df.index):\n",
    "            name[self.g.vertex(n_docs + j)] = word\n",
    "        kind.get_array()[n_docs:] = np.array([get_kind(w) for w in df.index], dtype=int)\n",
    "\n",
    "        # Edge weights\n",
    "        weight = self.g.new_ep(\"int\")\n",
    "        self.g.ep[\"count\"] = weight\n",
    "\n",
    "        # Build sparse edges\n",
    "        rows, cols = df.values.nonzero()\n",
    "        vals = df.values[rows, cols].astype(int)\n",
    "        edges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "        if len(edges) == 0:\n",
    "            logger.error(\"Empty graph detected\")\n",
    "            raise ValueError(\"Empty graph\")\n",
    "\n",
    "        self.g.add_edge_list(edges, eprops=[weight])\n",
    "\n",
    "        # Remove edges with 0 weight\n",
    "        filter_edges = self.g.new_edge_property(\"bool\")\n",
    "        for e in self.g.edges():\n",
    "            filter_edges[e] = weight[e] > 0\n",
    "        self.g.set_edge_filter(filter_edges)\n",
    "        self.g.purge_edges()\n",
    "        self.g.clear_filters()\n",
    "\n",
    "        self.documents = df.columns\n",
    "        self.words = df.index[self.g.vp['kind'].a[n_docs:] == 1]\n",
    "        for ik in range(2, 2 + self.nbranches):\n",
    "            self.keywords.append(df.index[self.g.vp['kind'].a[n_docs:] == ik])\n",
    "\n",
    "\n",
    "    def fit(self, n_init=1, verbose=True, deg_corr=True, overlap=False, parallel=False, B_min=0, B_max=None, clabel=None, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Fit a nested stochastic block model to the graph using `minimize_nested_blockmodel_dl`.\n",
    "    \n",
    "        This method performs multiple initializations and keeps the best model \n",
    "        based on the minimum description length (entropy). It supports degree-corrected \n",
    "        and overlapping block models, and can perform parallel moves for efficiency.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_init : int, default=1\n",
    "            Number of random initializations. The model with the lowest entropy is retained.\n",
    "        verbose : bool, default=True\n",
    "            If True, print progress messages.\n",
    "        deg_corr : bool, default=True\n",
    "            If True, use a degree-corrected block model.\n",
    "        overlap : bool, default=False\n",
    "            If True, use an overlapping block model.\n",
    "        parallel : bool, default=False\n",
    "            If True, perform parallel moves during optimization.\n",
    "        B_min : int, default=0\n",
    "            Minimum number of blocks to consider.\n",
    "        B_max : int, optional\n",
    "            Maximum number of blocks to consider. Defaults to the number of vertices.\n",
    "        clabel : str or property map, optional\n",
    "            Vertex property to use as initial block assignment. If None, the 'kind' \n",
    "            vertex property is used.\n",
    "        *args : positional arguments\n",
    "            Additional positional arguments passed to `minimize_nested_blockmodel_dl`.\n",
    "        **kwargs : keyword arguments\n",
    "            Additional keyword arguments passed to `minimize_nested_blockmodel_dl`. \n",
    "        \"\"\"\n",
    "        if clabel == None:\n",
    "            clabel = self.g.vp['kind']\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "        else:\n",
    "            logger.info(\"Clabel is %s, assigning partitions to vertices\", clabel)\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "    \n",
    "        state_args[\"eweight\"] = self.g.ep.count\n",
    "        min_entropy = np.inf\n",
    "        best_state = None\n",
    "        state_args[\"deg_corr\"] = deg_corr\n",
    "        state_args[\"overlap\"] = overlap\n",
    "\n",
    "        if B_max is None:\n",
    "            B_max = self.g.num_vertices()\n",
    "            \n",
    "        multilevel_mcmc_args={\"B_min\": B_min, \"B_max\": B_max, \"verbose\": verbose,\"parallel\" : parallel}\n",
    "\n",
    "        logger.debug(\"multilevel_mcmc_args: %s\", multilevel_mcmc_args)\n",
    "        logger.debug(\"state_args: %s\", state_args)\n",
    "\n",
    "        for i in range(n_init):\n",
    "            logger.info(\"Fit number: %d\", i)\n",
    "            state = minimize_nested_blockmodel_dl(self.g, state_args=state_args, multilevel_mcmc_args=multilevel_mcmc_args, *args, **kwargs)\n",
    "            \n",
    "            entropy = state.entropy()\n",
    "            if entropy < min_entropy:\n",
    "                min_entropy = entropy\n",
    "                self.state = state\n",
    "                \n",
    "        self.mdl = min_entropy\n",
    "\n",
    "        L = len(self.state.levels)\n",
    "        self.L = L\n",
    "\n",
    "        self.groups = {}\n",
    " #       logger.info(\"Saving data in %s\", self.saving_path)\n",
    "  #      self.save_data(path_to_save=self.saving_path)\n",
    "\n",
    "   #     logger.info(\"Annotate object\")\n",
    "    #    self.annotate_obj()\n",
    "\n",
    "\n",
    "    # Helper functions\n",
    "    def save_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "        \"\"\"\n",
    "        Save the graph\n",
    "\n",
    "        :param filename: name of the graph stored\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving graph to %s\", filename)\n",
    "        self.g.save(filename)\n",
    "    \n",
    "    \n",
    "    def load_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "        \"\"\"\n",
    "        Load a saved graph from disk and rebuild documents, words, and keywords.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str, optional\n",
    "            Path to the saved graph file (default: \"graph.xml.gz\").\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading graph from %s\", filename)\n",
    "\n",
    "        self.g = load_graph(filename)\n",
    "        self.documents = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 0]\n",
    "        self.words = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 1]\n",
    "        metadata_indexes = np.unique(self.g.vp[\"kind\"].a)\n",
    "        metadata_indexes = metadata_indexes[metadata_indexes > 1] #no doc or words\n",
    "        self.nbranches = len(metadata_indexes)\n",
    "        for i_keyword in metadata_indexes:\n",
    "            self.keywords.append([self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == i_keyword])\n",
    "\n",
    "    \n",
    "    def dump_model(self, filename=\"bionsbm.pkl\"):\n",
    "        \"\"\"\n",
    "        Dump model using pickle\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"Dumping model to %s\", filename)\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def load_model(self, filename=\"bionsbm.pkl\"):\n",
    "        logger.info(\"Loading model from %s\", filename)\n",
    "\n",
    "        with open(filename, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_mdl(self):\n",
    "        \"\"\"\n",
    "        Get minimum description length\n",
    "\n",
    "        Proxy to self.state.entropy()\n",
    "        \"\"\"\n",
    "        return self.mdl\n",
    "            \n",
    "    def get_shape(self):\n",
    "        \"\"\"\n",
    "        :return: list of tuples (number of documents, number of words, (number of keywords,...))\n",
    "        \"\"\"\n",
    "        D = int(np.sum(self.g.vp['kind'].a == 0)) #documents\n",
    "        W = int(np.sum(self.g.vp['kind'].a == 1)) #words\n",
    "        K = [int(np.sum(self.g.vp['kind'].a == (k+2))) for k in range(self.nbranches)] #keywords\n",
    "        return D, W, K\n",
    "\n",
    "    def get_groups(self, l=0) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Numba-accelerated get_groups that is robust for bipartite graphs (nbranches == 0)\n",
    "        and for arbitrary number of partitions.\n",
    "        \"\"\"\n",
    "\n",
    "        @njit\n",
    "        def process_edges_numba_stack(sources, targets, z1, z2, kinds, weights,\n",
    "                                      D, W, K_arr, nbranches,\n",
    "                                      n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3):\n",
    "            \"\"\"\n",
    "            Numba-compiled loop that increments the stacked accumulator arrays.\n",
    "            This function is defensive: if a 'kind' references a branch index out of range,\n",
    "            or an index into keywords is out of range, it's ignored (so bipartite graphs keep working).\n",
    "            \"\"\"\n",
    "            m = len(sources)\n",
    "            for i in range(m):\n",
    "                v1 = sources[i]\n",
    "                v2 = targets[i]\n",
    "                w = weights[i]\n",
    "                t1 = z1[i]\n",
    "                t2 = z2[i]\n",
    "                kind = kinds[i]\n",
    "        \n",
    "                # update doc-group counts (always)\n",
    "                n_db[v1, t1] += w\n",
    "        \n",
    "                if kind == 1:\n",
    "                    # word node\n",
    "                    idx_w = v2 - D\n",
    "                    if idx_w >= 0 and idx_w < n_wb.shape[0]:\n",
    "                        n_wb[idx_w, t2] += w\n",
    "                    # update doc->word-group\n",
    "                    n_dbw[v1, t2] += w\n",
    "        \n",
    "                elif kind >= 2:\n",
    "                    ik = kind - 2\n",
    "                    # guard: only process if ik is a valid branch index\n",
    "                    if ik >= 0 and ik < nbranches:\n",
    "                        # compute offset = D + W + sum(K_arr[:ik])\n",
    "                        offset = D + W\n",
    "                        for j in range(ik):\n",
    "                            offset += K_arr[j]\n",
    "                        idx_k = v2 - offset\n",
    "                        # guard keyword index bounds\n",
    "                        if idx_k >= 0 and idx_k < K_arr[ik]:\n",
    "                            n_w_key_b3[ik, idx_k, t2] += w\n",
    "                            n_dbw_key3[ik, v1, t2] += w\n",
    "                        # else: out-of-range keyword index -> ignore to remain robust\n",
    "                else:\n",
    "                    # unexpected kind (<1): ignore for safety (original assumed only kind==1 or >=2)\n",
    "                    pass\n",
    "            \n",
    "        if l in self.groups.keys():\n",
    "            return self.groups[l]\n",
    "\n",
    "        state_l = self.state.project_level(l).copy(overlap=True)\n",
    "        state_l_edges = state_l.get_edge_blocks()\n",
    "        B = state_l.get_B()\n",
    "        D, W, K = self.get_shape()\n",
    "        nbranches = self.nbranches\n",
    "\n",
    "        # Preallocate primary arrays (word/doc)\n",
    "        n_wb = np.zeros((W, B), dtype=np.float64)    # words x word-groups\n",
    "        n_db = np.zeros((D, B), dtype=np.float64)    # docs  x doc-groups\n",
    "        n_dbw = np.zeros((D, B), dtype=np.float64)   # docs  x word-groups\n",
    "\n",
    "        # Preallocate stacked branch arrays (shape: nbranches x max_K x B) and (nbranches x D x B)\n",
    "        if nbranches > 0:\n",
    "            max_K = int(np.max(K))\n",
    "            # If some K are zero, max_K will still be >=0; stack is safe\n",
    "            n_w_key_b3 = np.zeros((nbranches, max_K, B), dtype=np.float64)\n",
    "            n_dbw_key3 = np.zeros((nbranches, D, B), dtype=np.float64)\n",
    "        else:\n",
    "            # empty stacked arrays if no branches\n",
    "            n_w_key_b3 = np.zeros((0, 0, B), dtype=np.float64)\n",
    "            n_dbw_key3 = np.zeros((0, D, B), dtype=np.float64)\n",
    "\n",
    "        # Convert graph edges to arrays\n",
    "        edges = list(self.g.edges())\n",
    "        m = len(edges)\n",
    "        sources = np.empty(m, dtype=np.int64)\n",
    "        targets = np.empty(m, dtype=np.int64)\n",
    "        z1_arr = np.empty(m, dtype=np.int64)\n",
    "        z2_arr = np.empty(m, dtype=np.int64)\n",
    "        weights = np.empty(m, dtype=np.float64)\n",
    "        kinds = np.empty(m, dtype=np.int64)\n",
    "\n",
    "        for i, e in enumerate(edges):\n",
    "            sources[i] = int(e.source())\n",
    "            targets[i] = int(e.target())\n",
    "            z1_arr[i] = int(state_l_edges[e][0])\n",
    "            z2_arr[i] = int(state_l_edges[e][1])\n",
    "            weights[i] = float(self.g.ep[\"count\"][e])\n",
    "            kinds[i] = int(self.g.vp['kind'][int(e.target())])\n",
    "\n",
    "        K_arr = np.array(K, dtype=np.int64)  # can be empty if nbranches==0\n",
    "\n",
    "        # --- Numba edge processing (single compiled function for all cases) ---\n",
    "        process_edges_numba_stack(sources, targets, z1_arr, z2_arr, kinds, weights, D, W, K_arr, nbranches, n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3)\n",
    "\n",
    "        # --- Trim empty columns for doc/word arrays (same logic as original) ---\n",
    "        ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "        n_db = n_db[:, ind_d]\n",
    "        Bd = len(ind_d)\n",
    "\n",
    "        ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "        n_wb = n_wb[:, ind_w]\n",
    "        Bw = len(ind_w)\n",
    "\n",
    "        ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "        n_dbw = n_dbw[:, ind_w2]\n",
    "\n",
    "        # --- Convert stacked branch arrays into per-branch lists (safe slicing) ---\n",
    "        n_w_key_b_list = []\n",
    "        n_dbw_key_list = []\n",
    "        Bk = []\n",
    "\n",
    "        for ik in range(nbranches):\n",
    "            Kk = int(K_arr[ik])\n",
    "            if Kk > 0:\n",
    "                # compute which columns (groups) are non-zero\n",
    "                col_sums = np.sum(n_w_key_b3[ik, :Kk, :], axis=0)\n",
    "                ind_wk = np.where(col_sums > 0)[0]\n",
    "                # slice and copy into a per-branch array (Kk x Bk)\n",
    "                if ind_wk.size > 0:\n",
    "                    n_w_key_b_list.append(n_w_key_b3[ik, :Kk, :][:, ind_wk].copy())\n",
    "                else:\n",
    "                    # keep shape (Kk, 0) if there are no columns\n",
    "                    n_w_key_b_list.append(np.zeros((Kk, 0), dtype=np.float64))\n",
    "                Bk.append(len(ind_wk))\n",
    "            else:\n",
    "                # branch with 0 keywords\n",
    "                n_w_key_b_list.append(np.zeros((0, 0), dtype=np.float64))\n",
    "                Bk.append(0)\n",
    "\n",
    "            # doc x keyword-groups for this branch\n",
    "            col_sums_dbw = np.sum(n_dbw_key3[ik], axis=0)\n",
    "            ind_w2k = np.where(col_sums_dbw > 0)[0]\n",
    "            if ind_w2k.size > 0:\n",
    "                n_dbw_key_list.append(n_dbw_key3[ik][:, ind_w2k].copy())\n",
    "            else:\n",
    "                n_dbw_key_list.append(np.zeros((D, 0), dtype=np.float64))\n",
    "\n",
    "        # --- Compute probabilities exactly like the original (division -> NaN if denominator==0) ---\n",
    "        # P(t_w | w)\n",
    "        denom = np.sum(n_wb, axis=1, keepdims=True)  # (W,1)\n",
    "        p_tw_w = (n_wb / denom).T\n",
    "\n",
    "        # P(t_k | keyword) per branch\n",
    "        p_tk_w_key = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_w_key_b_list[ik]\n",
    "            denom = np.sum(arr, axis=1, keepdims=True)\n",
    "            p_tk_w_key.append((arr / denom).T)\n",
    "\n",
    "        # P(w | t_w)\n",
    "        denom = np.sum(n_wb, axis=0, keepdims=True)  # (1,Bw)\n",
    "        p_w_tw = n_wb / denom\n",
    "\n",
    "        # P(keyword | t_w_key) per branch\n",
    "        p_w_key_tk = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_w_key_b_list[ik]\n",
    "            denom = np.sum(arr, axis=0, keepdims=True)\n",
    "            p_w_key_tk.append(arr / denom)\n",
    "\n",
    "        # P(t_w | d)\n",
    "        denom = np.sum(n_dbw, axis=1, keepdims=True)\n",
    "        p_tw_d = (n_dbw / denom).T\n",
    "\n",
    "        # P(t_k | d) per branch\n",
    "        p_tk_d = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_dbw_key_list[ik]\n",
    "            denom = np.sum(arr, axis=1, keepdims=True)\n",
    "            p_tk_d.append((arr / denom).T)\n",
    "\n",
    "        # P(t_d | d)\n",
    "        denom = np.sum(n_db, axis=1, keepdims=True)\n",
    "        p_td_d = (n_db / denom).T\n",
    "\n",
    "        result = {\n",
    "            'Bd': Bd,\n",
    "            'Bw': Bw,\n",
    "            'Bk': Bk,\n",
    "            'p_tw_w': p_tw_w,\n",
    "            'p_tk_w_key': p_tk_w_key,\n",
    "            'p_td_d': p_td_d,\n",
    "            'p_w_tw': p_w_tw,\n",
    "            'p_w_key_tk': p_w_key_tk,\n",
    "            'p_tw_d': p_tw_d,\n",
    "            'p_tk_d': p_tk_d}\n",
    "\n",
    "        self.groups[l] = result\n",
    "        return result\n",
    "\n",
    "\n",
    "    def draw(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Draw the network\n",
    "\n",
    "        :param \\*args: positional arguments to pass to self.state.draw\n",
    "        :param \\*\\*kwargs: keyword argument to pass to self.state.draw\n",
    "        \"\"\"\n",
    "        colmap = self.g.vertex_properties[\"color\"] = self.g.new_vertex_property(\n",
    "            \"vector<double>\")\n",
    "        #https://medialab.github.io/iwanthue/\n",
    "        colors = [  [174,80,209],\n",
    "                    [108,192,70],\n",
    "                    [207, 170, 60],\n",
    "                    [131,120,197],\n",
    "                    [126,138,65],\n",
    "                    [201,90,138],\n",
    "                    [87,172,125],\n",
    "                    [213,73,57],\n",
    "                    [85,175,209],\n",
    "                    [193,120,81]]\n",
    "        for v in self.g.vertices():\n",
    "            k = self.g.vertex_properties['kind'][v]\n",
    "            if k < 10:\n",
    "                color = np.array(colors[k])/255.\n",
    "            else:\n",
    "                color = np.array([187, 129, 164])/255.\n",
    "            colmap[v] = color\n",
    "        self.state.draw(\n",
    "            subsample_edges = 5000, \n",
    "            edge_pen_width = self.g.ep[\"count\"],\n",
    "            vertex_color=colmap,\n",
    "            vertex_fill_color=colmap, *args, **kwargs)\n",
    "\n",
    "\n",
    "    def save_single_level(self, l: int, path_to_save: str) -> None:\n",
    "        \"\"\"\n",
    "        Save per-level probability matrices (topics, clusters, documents) for the given level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l : int\n",
    "            The level index to save. Must be within the range of available model levels.\n",
    "        savingpath_to_save_path : str\n",
    "            Base path (folder + prefix) where files will be written.\n",
    "            Example: \"results/mymodel\" → files like:\n",
    "                - results/mymodel_level_0_mainfeature_topics.tsv.gz\n",
    "                - results/mymodel_level_0_clusters.tsv.gz\n",
    "                - results/mymodel_level_0_mainfeature_topics_documents.tsv.gz\n",
    "                - results/mymodel_level_0_metafeature_topics.tsv.gz\n",
    "                - results/mymodel_level_0_metafeature_topics_documents.tsv.gz\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Files are written as tab-separated values (`.tsv.gz`) with gzip compression.\n",
    "        - Raises RuntimeError if any file cannot be written.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Validate inputs ---\n",
    "        if not isinstance(l, int) or l < 0 or l >= len(self.state.levels) or l >= len(self.state.levels):\n",
    "            raise ValueError(f\"Invalid level index {l}. Must be between 0 and {len(self.state.levels) - 1}.\")\n",
    "        if not isinstance(path_to_save, str) or not path_to_save.strip():\n",
    "            raise ValueError(\"`path_to_save` must be a non-empty string path prefix.\")\n",
    "\n",
    "        main_feature = self.modalities[0]\n",
    "\n",
    "        try:\n",
    "            data = self.get_groups(l)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to get group data for level {l}: {e}\") from e\n",
    "\n",
    "        # Helper to safely save a DataFrame\n",
    "        def _safe_save(df, filepath):\n",
    "            try:\n",
    "                Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "                df.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "        # --- P(document | cluster) ---\n",
    "        clusters = pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)\n",
    "        _safe_save(clusters, f\"{path_to_save}_level_{l}_clusters.tsv.gz\")\n",
    "\n",
    "\n",
    "        # --- P(main_feature | main_topic) ---\n",
    "        p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_w_tw, f\"{path_to_save}_level_{l}_{main_feature}_topics.tsv.gz\")\n",
    "\n",
    "        # --- P(main_topic | documents) ---\n",
    "        p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_tw_d, f\"{path_to_save}_level_{l}_{main_feature}_topics_documents.tsv.gz\")\n",
    "\n",
    "        # --- P(meta_feature | meta_topic_feature), if any ---\n",
    "        if len(self.modalities) > 1:\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "                    columns=[f\"{meta_features}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(p_w_tw, f\"{path_to_save}_level_{l}_{meta_features}_topics.tsv.gz\")\n",
    "\n",
    "\n",
    "            # --- P(meta_topic | document) ---\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                p_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "                    columns=[f\"{meta_features}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(p_tw_d, f\"{path_to_save}_level_{l}_{meta_features}_topics_documents.tsv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "    def save_data(self, path_to_save: str = \"results/mymodel\") -> None:\n",
    "        \"\"\"\n",
    "        Save the global graph, model, state, and level-specific data for the current nSBM self.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        savinpath_to_saveg_path : str, optional\n",
    "            Base path (folder + prefix) where all outputs will be saved.\n",
    "            Example: \"results/mymodel\" will produce:\n",
    "                - results/mymodel_graph.xml.gz\n",
    "                - results/mymodel_model.pkl    \n",
    "                - results/mymodel_entropy.txt\n",
    "                - results/mymodel_state.pkl\n",
    "                - results/mymodel_level_X_*.tsv.gz  (per level, up to 6 levels)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The parent folder is created automatically if it does not exist.\n",
    "        - Level saving is parallelized with threads for efficiency in I/O.\n",
    "        - By default, at most self.max_depth levels are saved, or fewer if the model has <self.max_depth levels.\n",
    "        \"\"\"\n",
    "        logger.info(\"Saving model data to %s\", path_to_save)\n",
    "\n",
    "        L = min(len(self.state.levels), self.max_depth)\n",
    "        self.L = L\n",
    "        if L == 0:\n",
    "            logger.warning(\"Nothing to save (no levels found)\")\n",
    "            return\n",
    "        \n",
    "        folder = os.path.dirname(path_to_save)\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            self.save_graph(filename=f\"{path_to_save}_graph.xml.gz\")\n",
    "            self.dump_model(filename=f\"{path_to_save}_model.pkl\")\n",
    "\n",
    "            with open(f\"{path_to_save}_entropy.txt\", \"w\") as f:\n",
    "                f.write(str(self.state.entropy()))\n",
    "\n",
    "            with open(f\"{path_to_save}_state.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.state, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to save global files: %s\", e)\n",
    "            raise RuntimeError(f\"Failed to save global files for model '{path_to_save}': {e}\") from e\n",
    "\n",
    "\n",
    "        errors = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(self.save_single_level, l, path_to_save): l for l in range(L)}\n",
    "            for future in as_completed(futures):\n",
    "                l = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    errors.append((l, str(e)))\n",
    "\n",
    "        if errors:\n",
    "            msg = \"; \".join([f\"Level {l}: {err}\" for l, err in errors])\n",
    "            logger.error(\"Errors occurred while saving levels: %s\", msg)\n",
    "            raise RuntimeError(f\"Errors occurred while saving levels: {msg}\")\n",
    "\n",
    "\n",
    "    def annotate_obj(self) -> None:\n",
    "        L = min(len(self.state.levels), self.max_depth)\n",
    "        for l in range(0,L):\n",
    "            main_feature = self.modalities[0]\n",
    "            data = self.get_groups(l)\n",
    "            self.obj.obs[f\"Level_{l}_cluster\"]=np.argmax(pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)[self.obj.obs.index], axis=0).astype(str)\n",
    "            \n",
    "    \n",
    "            if isinstance(self.obj, MuData):\n",
    "                order_var=self.obj[main_feature].var.index\n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "                                columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "                self.obj[main_feature].var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "            elif isinstance(self.obj, AnnData):\n",
    "                order_var=self.obj.var.index             \n",
    "                p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "                                columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "                self.obj.var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "            \n",
    "            p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "                    columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[self.obj.obs.index]\n",
    "            p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "            self.obj.obs[f\"Level_{l}_{main_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "        \n",
    "            if len(self.modalities) > 1:\n",
    "                for k, meta_feature in enumerate(self.modalities[1:]):\n",
    "                    p_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "                        columns=[f\"{meta_feature}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                    self.obj[meta_feature].var[f\"Level_{l}_{meta_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "            \n",
    "                # --- P(meta_topic | document) ---\n",
    "                for k, meta_feature in enumerate(self.modalities[1:]):\n",
    "                    p_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "                        columns=[f\"{meta_feature}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                    p_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "                    self.obj.obs[f\"Level_{l}_{meta_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\n",
    "    def get_V(self):\n",
    "        '''\n",
    "        return number of word-nodes == types\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 1))  # no. of types\n",
    "\n",
    "    def get_D(self):\n",
    "        '''\n",
    "        return number of doc-nodes == number of documents\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 0))  # no. of types\n",
    "\n",
    "    def get_N(self):\n",
    "        '''\n",
    "        return number of edges == tokens\n",
    "        '''\n",
    "        return int(self.g.num_edges())  # no. of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef42232-baaf-4e19-a138-2fe2ca5210b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fa5e9-addd-4ef2-8b00-845ab53c9633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801465d-be90-4f3f-a355-746a8af84c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c69c64a-b582-4231-889b-d671e9f95283",
   "metadata": {},
   "source": [
    "# New make_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65a2ccf-19bf-4f64-8e67-0207c7a3055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata=read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9b0507-0483-4545-b178-25a495b5f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_multiple_df(model, df: pd.DataFrame, df_keyword_list: list, fast=True)->None:\n",
    "    \"\"\"\n",
    "    Create a graph from two dataframes one with words, others with keywords or other layers of information\n",
    "\n",
    "    :param df: DataFrame with words on index and texts on columns\n",
    "    :param df_keyword_list: list of DataFrames with keywords on index and texts on columns\n",
    "    \"\"\"\n",
    "    df_all = df.copy(deep =True)\n",
    "    for ikey,df_keyword in enumerate(df_keyword_list):\n",
    "        df_keyword = df_keyword.reindex(columns=df.columns)\n",
    "        df_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "        df_keyword[\"kind\"] = ikey+2\n",
    "        df_all = pd.concat((df_all,df_keyword), axis=0)\n",
    "\n",
    "    def get_kind(word):\n",
    "        return 1 if word in df.index else df_all.at[word,\"kind\"]\n",
    "\n",
    "    model.nbranches = len(df_keyword_list)\n",
    "\n",
    "    if fast:\n",
    "        make_graph_fast(model, df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "    else:\n",
    "        make_graph(model, df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "    \n",
    "def make_graph(model, df: pd.DataFrame, get_kind) -> None:\n",
    "    \"\"\"\n",
    "    Create a graph from a pandas DataFrame\n",
    "\n",
    "    :param df: DataFrame with words on index and texts on columns. Actually this is a BoW.\n",
    "    :param get_kind: function that returns 1 or 2 given an element of df.index. [1 for words 2 for keywords]\n",
    "    \"\"\"\n",
    "    model.g = Graph(directed=False)\n",
    "    name = model.g.vp[\"name\"] = model.g.new_vp(\"string\")\n",
    "    kind = model.g.vp[\"kind\"] = model.g.new_vp(\"int\")\n",
    "    weight = model.g.ep[\"count\"] = model.g.new_ep(\"int\")\n",
    "\n",
    "    for doc in df.columns:\n",
    "        d = model.g.add_vertex()\n",
    "        name[d] = doc\n",
    "        kind[d] = 0\n",
    "\n",
    "    for word in df.index:\n",
    "        w = model.g.add_vertex()\n",
    "        name[w] = word\n",
    "        kind[w] = get_kind(word)\n",
    "\n",
    "    D = df.shape[1]\n",
    "\n",
    "    for i_doc, doc in enumerate(df.columns):\n",
    "        text = df[doc]\n",
    "        model.g.add_edge_list([(i_doc, D + x[0][0], x[1]) for x in zip(enumerate(df.index), text)], eprops=[weight])\n",
    "\n",
    "    filter_edges = model.g.new_edge_property(\"bool\")\n",
    "    for e in model.g.edges():\n",
    "        filter_edges[e] = weight[e] > 0\n",
    "\n",
    "    model.g.set_edge_filter(filter_edges)\n",
    "    model.g.purge_edges()\n",
    "    model.g.clear_filters()\n",
    "\n",
    "    model.documents = df.columns\n",
    "    model.words = df.index[model.g.vp['kind'].a[D:] == 1]\n",
    "    for ik in range(2, 2 + model.nbranches):  # 2 is doc and words\n",
    "        model.keywords.append(df.index[model.g.vp['kind'].a[D:] == ik])\n",
    "\n",
    "def make_graph_fast(model, df: pd.DataFrame, get_kind):\n",
    "    model.g = Graph(directed=False)\n",
    "\n",
    "    n_docs, n_words = df.shape[1], df.shape[0]\n",
    "\n",
    "    # Add all vertices first\n",
    "    model.g.add_vertex(n_docs + n_words)\n",
    "\n",
    "    # Create vertex properties\n",
    "    name = model.g.new_vp(\"string\")\n",
    "    kind = model.g.new_vp(\"int\")\n",
    "    model.g.vp[\"name\"] = name\n",
    "    model.g.vp[\"kind\"] = kind\n",
    "\n",
    "    # Assign doc vertices (loop for names, array for kind)\n",
    "    for i, doc in enumerate(df.columns):\n",
    "        name[model.g.vertex(i)] = doc\n",
    "    kind.get_array()[:n_docs] = 0\n",
    "\n",
    "    # Assign word vertices (loop for names, array for kind)\n",
    "    for j, word in enumerate(df.index):\n",
    "        name[model.g.vertex(n_docs + j)] = word\n",
    "    kind.get_array()[n_docs:] = np.array([get_kind(w) for w in df.index], dtype=int)\n",
    "\n",
    "    # Edge weights\n",
    "    weight = model.g.new_ep(\"int\")\n",
    "    model.g.ep[\"count\"] = weight\n",
    "\n",
    "    # Build sparse edges\n",
    "    rows, cols = df.values.nonzero()\n",
    "    vals = df.values[rows, cols].astype(int)\n",
    "    edges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "    if len(edges)==0: raise ValueError(\"Empty graph\")\n",
    "\n",
    "    model.g.add_edge_list(edges, eprops=[weight])\n",
    "\n",
    "    # Remove edges with 0 weight\n",
    "    filter_edges = model.g.new_edge_property(\"bool\")\n",
    "    for e in model.g.edges():\n",
    "        filter_edges[e] = weight[e] > 0\n",
    "    model.g.set_edge_filter(filter_edges)\n",
    "    model.g.purge_edges()\n",
    "    model.g.clear_filters()\n",
    "\n",
    "    model.documents = df.columns\n",
    "    model.words = df.index[model.g.vp['kind'].a[n_docs:] == 1]\n",
    "    for ik in range(2, 2 + model.nbranches):\n",
    "        model.keywords.append(df.index[model.g.vp['kind'].a[n_docs:] == ik])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655c7727-f9eb-4954-9678-8935d526ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Compare models\n",
    "# ----------------------------\n",
    "def compare_models(m1, m2):\n",
    "    g1, g2 = m1.g, m2.g\n",
    "\n",
    "    assert g1.num_vertices() == g2.num_vertices(), \"Different number of vertices\"\n",
    "    assert g1.num_edges() == g2.num_edges(), \"Different number of edges\"\n",
    "\n",
    "    for prop in [\"name\", \"kind\"]:\n",
    "        p1, p2 = g1.vp[prop], g2.vp[prop]\n",
    "        vals1 = [p1[v] for v in g1.vertices()]\n",
    "        vals2 = [p2[v] for v in g2.vertices()]\n",
    "        assert vals1 == vals2, f\"Vertex property {prop} differs\"\n",
    "\n",
    "    w1, w2 = g1.ep[\"count\"], g2.ep[\"count\"]\n",
    "    edges1 = sorted([(int(e.source()), int(e.target()), int(w1[e])) for e in g1.edges()])\n",
    "    edges2 = sorted([(int(e.source()), int(e.target()), int(w2[e])) for e in g2.edges()])\n",
    "    assert edges1 == edges2, \"Edges or weights differ\"\n",
    "\n",
    "    assert list(m1.documents) == list(m2.documents), \"Documents differ\"\n",
    "    assert np.array_equal(m1.words, m2.words), \"Words differ\"\n",
    "    assert len(m1.keywords) == len(m2.keywords), \"Different number of keyword groups\"\n",
    "    for i, (kw1, kw2) in enumerate(zip(m1.keywords, m2.keywords)):\n",
    "        assert np.array_equal(kw1, kw2), f\"Keywords differ at branch {i}\"\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Benchmark with correctness check\n",
    "# ----------------------------\n",
    "def benchmark_and_check(mdata):\n",
    "    # Old\n",
    "    modalities=list(mdata.mod.keys())   \n",
    "    dfs=[mdata[key].to_df().T for key in modalities]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model1 = bionsbm(obj=mdata)\n",
    "    make_graph_multiple_df(model1, dfs[0], dfs[1:], fast=False)\n",
    "    t1 = time.time()\n",
    "    old_time = t1 - t0\n",
    "\n",
    "    # New\n",
    "    t0 = time.time()\n",
    "    model2 = bionsbm(obj=mdata)\n",
    "    make_graph_multiple_df(model2, dfs[0], dfs[1:], fast=True)\n",
    "    t1 = time.time()\n",
    "    new_time = t1 - t0\n",
    "\n",
    "    # Check equality\n",
    "    assert compare_models(model1, model2), \"Models differ!\"\n",
    "\n",
    "    return old_time, new_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a263ae4f-407e-4ec9-9493-326ce0f6a0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 (51, 110)\n",
      "0.1 (99, 220)\n",
      "0.15 (150, 330)\n",
      "0.2 (201, 440)\n",
      "0.25 (250, 550)\n",
      "0.3 (301, 660)\n",
      "0.35 (351, 770)\n",
      "0.4 (400, 880)\n",
      "0.45 (450, 990)\n",
      "0.5 (498, 1100)\n",
      "0.55 (550, 1211)\n",
      "0.6 (600, 1320)\n",
      "0.65 (649, 1430)\n",
      "0.7 (699, 1540)\n",
      "0.75 (750, 1650)\n",
      "0.8 (799, 1760)\n",
      "0.85 (850, 1870)\n",
      "0.9 (901, 1980)\n",
      "0.95 (949, 2090)\n",
      "1.0 (1000, 2200)\n"
     ]
    }
   ],
   "source": [
    "data={}\n",
    "data[\"Original\"]=[]\n",
    "data[\"Fast\"]=[]\n",
    "data[\"NumberofNodes\"]=[]\n",
    "\n",
    "for n in np.array(np.linspace(0.5,10, 20)/10):\n",
    "    mdata_sub = subsample_mudata(mdata, cell_frac=n, strat_mod=\"Peak\", strat_col=\"CellType\", random_seed=123)\n",
    "    print(n, mdata_sub.shape)\n",
    "\n",
    "    old_t, new_t = benchmark_and_check(mdata_sub)\n",
    "    data[\"Original\"].append(old_t)\n",
    "    data[\"Fast\"].append(new_t)\n",
    "    data[\"NumberofNodes\"].append(np.prod((mdata_sub.shape[0], mdata_sub[\"Peak\"].shape[1], mdata_sub[\"mRNA\"].shape[1], mdata_sub[\"lncRNA\"].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b329b3-b878-4508-bf2f-a4b8e182babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('time_make_graph.json', 'w') as f:\n",
    "    json.dump(pd.DataFrame.from_dict(data).astype(float).to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c23aa982-5765-4035-8daa-32c519237dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Fast</th>\n",
       "      <th>NumberofNodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026327</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>2231250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087956</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>34650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187475</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>177187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.330303</td>\n",
       "      <td>0.013007</td>\n",
       "      <td>562800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.513486</td>\n",
       "      <td>0.019269</td>\n",
       "      <td>1367187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.746722</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>2844450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.005501</td>\n",
       "      <td>0.034312</td>\n",
       "      <td>5267193750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.412687</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>8960000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.773360</td>\n",
       "      <td>0.053982</td>\n",
       "      <td>14352187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.207997</td>\n",
       "      <td>0.067957</td>\n",
       "      <td>21787500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.681601</td>\n",
       "      <td>0.076709</td>\n",
       "      <td>32110375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.081877</td>\n",
       "      <td>0.092003</td>\n",
       "      <td>45360000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.682723</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>62381068750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.404604</td>\n",
       "      <td>0.123693</td>\n",
       "      <td>83914950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.034691</td>\n",
       "      <td>0.140796</td>\n",
       "      <td>110742187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.826922</td>\n",
       "      <td>0.155607</td>\n",
       "      <td>143180800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.801081</td>\n",
       "      <td>0.178118</td>\n",
       "      <td>182702187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.807610</td>\n",
       "      <td>0.200851</td>\n",
       "      <td>229890150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.744569</td>\n",
       "      <td>0.224109</td>\n",
       "      <td>284777106250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.547273</td>\n",
       "      <td>0.238377</td>\n",
       "      <td>350000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original      Fast  NumberofNodes\n",
       "0   0.026327  0.003230        2231250\n",
       "1   0.087956  0.004866       34650000\n",
       "2   0.187475  0.008388      177187500\n",
       "3   0.330303  0.013007      562800000\n",
       "4   0.513486  0.019269     1367187500\n",
       "5   0.746722  0.026406     2844450000\n",
       "6   1.005501  0.034312     5267193750\n",
       "7   1.412687  0.044481     8960000000\n",
       "8   1.773360  0.053982    14352187500\n",
       "9   2.207997  0.067957    21787500000\n",
       "10  2.681601  0.076709    32110375000\n",
       "11  3.081877  0.092003    45360000000\n",
       "12  3.682723  0.108163    62381068750\n",
       "13  4.404604  0.123693    83914950000\n",
       "14  5.034691  0.140796   110742187500\n",
       "15  5.826922  0.155607   143180800000\n",
       "16  6.801081  0.178118   182702187500\n",
       "17  7.807610  0.200851   229890150000\n",
       "18  8.744569  0.224109   284777106250\n",
       "19  9.547273  0.238377   350000000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times=pd.DataFrame.from_dict(data)\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576226f6-2592-4724-bf1c-e084ba185d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=times[\"NumberofNodes\"], y=np.array(times[\"Original\"]/times[\"Fast\"]))\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.xlabel(\"Fraction of data\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.title(\"make_graph Performance Benchmark\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Numer of nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb1e72-4419-426e-9495-0659701af25f",
   "metadata": {},
   "source": [
    "# New get_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5763f5ca-71f4-4c6c-8392-600ac3487f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(model, l=0):\n",
    "    \"\"\"\n",
    "    return groups\n",
    "\n",
    "    :param l: hierarchy level\n",
    "    \"\"\"\n",
    "\n",
    "    state_l = model.state.project_level(l).copy(overlap=True)\n",
    "    state_l_edges = state_l.get_edge_blocks()\n",
    "    B = state_l.get_B()\n",
    "    D, W, K = model.get_shape()\n",
    "\n",
    "    n_wb = np.zeros((W, B))  ## number of half-edges incident on word-node w and labeled as word-group tw\n",
    "    n_w_key_b = [np.zeros((K[ik], B)) for ik in range(model.nbranches)]  ## number of half-edges incident on word-node w and labeled as word-group tw\n",
    "    n_db = np.zeros((D, B))  ## number of half-edges incident on document-node d and labeled as document-group td\n",
    "    n_dbw = np.zeros((D, B)) ## number of half-edges incident on document-node d and labeled as word-group tw\n",
    "    n_dbw_key = [np.zeros((D, B)) for _ in range(model.nbranches)] ## number of half-edges incident on document-node d and labeled as keyword-group tw_key\n",
    "\n",
    "    for e in model.g.edges():\n",
    "        z1, z2 = state_l_edges[e]\n",
    "        v1 = e.source()\n",
    "        v2 = e.target()\n",
    "        weight = model.g.ep[\"count\"][e]\n",
    "        n_db[int(v1), z1] += weight\n",
    "        kind = model.g.vp['kind'][v2]\n",
    "        if kind == 1:\n",
    "            n_wb[int(v2) - D, z2] += weight\n",
    "            n_dbw[int(v1), z2] += weight\n",
    "        else:\n",
    "            n_w_key_b[kind-2][int(v2) - D - W - sum(K[:(kind-2)]), z2] += weight\n",
    "            n_dbw_key[kind-2][int(v1), z2] += weight\n",
    "\n",
    "    #p_w = np.sum(n_wb, axis=1) / float(np.sum(n_wb))\n",
    "\n",
    "    ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "    Bd = len(ind_d)\n",
    "    n_db = n_db[:, ind_d]\n",
    "\n",
    "    ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "    Bw = len(ind_w)\n",
    "    n_wb = n_wb[:, ind_w]\n",
    "\n",
    "    ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "    n_dbw = n_dbw[:, ind_w2]\n",
    "\n",
    "    ind_w_key = []\n",
    "    ind_w2_keyword = []\n",
    "    Bk = []\n",
    "\n",
    "    for ik in range(model.nbranches):\n",
    "        ind_w_key.append(np.where(np.sum(n_w_key_b[ik], axis=0) > 0)[0])\n",
    "        Bk.append(len(ind_w_key[ik]))\n",
    "        n_w_key_b[ik] = n_w_key_b[ik][:, ind_w_key[ik]]\n",
    "        \n",
    "        ind_w2_keyword.append(np.where(np.sum(n_dbw_key[ik], axis=0) > 0)[0])\n",
    "        n_dbw_key[ik] = n_dbw_key[ik][:, ind_w2_keyword[ik]]\n",
    "    \n",
    "\n",
    "    # group membership of each word-node P(t_w | w)\n",
    "    p_tw_w = (n_wb / np.sum(n_wb, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "    p_tk_w_key = []\n",
    "    for ik in range(model.nbranches):\n",
    "        # group membership of each keyword-node P(t_k | keyword)\n",
    "        p_tk_w_key.append((n_w_key_b[ik] / np.sum(n_w_key_b[ik], axis=1)[:, np.newaxis]).T)\n",
    "    \n",
    "    ## topic-distribution for words P(w | t_w)\n",
    "    p_w_tw = n_wb / np.sum(n_wb, axis=0)[np.newaxis, :]\n",
    "    \n",
    "    p_w_key_tk = []\n",
    "    for ik in range(model.nbranches):\n",
    "        ## topickey-distribution for keywords P(keyword | t_w_key)\n",
    "        p_w_key_tk.append(n_w_key_b[ik] / np.sum(n_w_key_b[ik], axis=0)[np.newaxis, :])\n",
    "    \n",
    "    ## Mixture of word-groups into documetns P(t_w | d)\n",
    "    p_tw_d = (n_dbw / np.sum(n_dbw, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "    p_tk_d = []\n",
    "    for ik in range(model.nbranches):\n",
    "        ## Mixture of word-groups into documetns P(t_w | d)\n",
    "        p_tk_d.append((n_dbw_key[ik] / np.sum(n_dbw_key[ik], axis=1)[:, np.newaxis]).T)\n",
    "    \n",
    "    # group membership of each doc-node P(t_d | d)\n",
    "    p_td_d = (n_db / np.sum(n_db, axis=1)[:, np.newaxis]).T\n",
    "\n",
    "    result = {}\n",
    "    result['Bd'] = Bd\n",
    "    result['Bw'] = Bw\n",
    "    result['Bk'] = Bk\n",
    "    result['p_tw_w'] = p_tw_w\n",
    "    result[\"p_tk_w_key\"] = p_tk_w_key\n",
    "    result['p_td_d'] = p_td_d\n",
    "    result['p_w_tw'] = p_w_tw\n",
    "    result['p_w_key_tk'] = p_w_key_tk\n",
    "    result['p_tw_d'] = p_tw_d\n",
    "    result['p_tk_d'] = p_tk_d\n",
    "\n",
    "    model.groups[l] = result\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795992be-45fa-4a5b-ac97-968635d85fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups_fast(model, l=0):\n",
    "    \"\"\"\n",
    "    Optimized Numba-safe version of get_groups for bipartite/multipartite graphs.\n",
    "    Uses precomputed branch offsets (no inner per-edge loop).\n",
    "    \"\"\"\n",
    "    if l in model.groups:\n",
    "        return model.groups[l]\n",
    "\n",
    "    # --- Setup ---\n",
    "    state_l = model.state.project_level(l).copy(overlap=True)\n",
    "    state_l_edges = state_l.get_edge_blocks()\n",
    "    B = state_l.get_B()\n",
    "    D, W, K = model.get_shape()\n",
    "    nbranches = model.nbranches\n",
    "    K_arr = np.array(K, dtype=np.int64)\n",
    "\n",
    "    # --- Precompute branch offsets ---\n",
    "    if nbranches > 0 and K_arr.size > 0:\n",
    "        prefix_K = np.empty(nbranches, dtype=np.int64)\n",
    "        prefix_K[0] = 0\n",
    "        for ii in range(1, nbranches):\n",
    "            prefix_K[ii] = prefix_K[ii-1] + K_arr[ii-1]\n",
    "        offsets = (D + W) + prefix_K   # offsets[ik] = D + W + sum(K_arr[:ik])\n",
    "    else:\n",
    "        offsets = np.empty(0, dtype=np.int64)\n",
    "\n",
    "    # --- Convert edges to arrays ---\n",
    "    edges = list(model.g.edges())\n",
    "    m = len(edges)\n",
    "    sources = np.empty(m, dtype=np.int64)\n",
    "    targets = np.empty(m, dtype=np.int64)\n",
    "    z1_arr = np.empty(m, dtype=np.int64)\n",
    "    z2_arr = np.empty(m, dtype=np.int64)\n",
    "    weights = np.empty(m, dtype=np.float64)\n",
    "    kinds = np.empty(m, dtype=np.int64)\n",
    "\n",
    "    for i, e in enumerate(edges):\n",
    "        sources[i] = int(e.source())\n",
    "        targets[i] = int(e.target())\n",
    "        z1_arr[i] = int(state_l_edges[e][0])\n",
    "        z2_arr[i] = int(state_l_edges[e][1])\n",
    "        weights[i] = float(model.g.ep[\"count\"][e])\n",
    "        kinds[i] = int(model.g.vp['kind'][int(e.target())])\n",
    "\n",
    "    # --- Allocate accumulators ---\n",
    "    n_wb = np.zeros((W, B), dtype=np.float64)\n",
    "    n_db = np.zeros((D, B), dtype=np.float64)\n",
    "    n_dbw = np.zeros((D, B), dtype=np.float64)\n",
    "    n_w_key_b3 = np.zeros((nbranches, np.max(K_arr) if nbranches > 0 else 0, B), dtype=np.float64)\n",
    "    n_dbw_key3 = np.zeros((nbranches, D, B), dtype=np.float64)\n",
    "\n",
    "    # --- Process edges ---\n",
    "    process_edges_numba_stack(\n",
    "        sources, targets, z1_arr, z2_arr, kinds, weights,\n",
    "        D, W, K_arr, offsets, nbranches,\n",
    "        n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3\n",
    "    )\n",
    "\n",
    "    # --- Trim matrices (remove empty columns) ---\n",
    "    ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "    n_db = n_db[:, ind_d]\n",
    "    Bd = len(ind_d)\n",
    "\n",
    "    ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "    n_wb = n_wb[:, ind_w]\n",
    "    Bw = len(ind_w)\n",
    "\n",
    "    ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "    n_dbw = n_dbw[:, ind_w2]\n",
    "\n",
    "    n_w_key_b_list = []\n",
    "    n_dbw_key_list = []\n",
    "    Bk = []\n",
    "    for ik in range(nbranches):\n",
    "        Kk = int(K_arr[ik]) if K_arr.size > 0 else 0\n",
    "        if Kk > 0:\n",
    "            col_sums = np.sum(n_w_key_b3[ik, :Kk, :], axis=0)\n",
    "            ind_wk = np.where(col_sums > 0)[0]\n",
    "            if ind_wk.size > 0:\n",
    "                n_w_key_b_list.append(n_w_key_b3[ik, :Kk, :][:, ind_wk].copy())\n",
    "            else:\n",
    "                n_w_key_b_list.append(np.zeros((Kk, 0), dtype=np.float64))\n",
    "            Bk.append(len(ind_wk))\n",
    "        else:\n",
    "            n_w_key_b_list.append(np.zeros((0, 0), dtype=np.float64))\n",
    "            Bk.append(0)\n",
    "\n",
    "        col_sums_dbw = np.sum(n_dbw_key3[ik], axis=0)\n",
    "        ind_w2k = np.where(col_sums_dbw > 0)[0]\n",
    "        if ind_w2k.size > 0:\n",
    "            n_dbw_key_list.append(n_dbw_key3[ik][:, ind_w2k].copy())\n",
    "        else:\n",
    "            n_dbw_key_list.append(np.zeros((D, 0), dtype=np.float64))\n",
    "\n",
    "    # --- Compute distributions (normalize with NaN-preservation) ---\n",
    "    denom = np.sum(n_wb, axis=1, keepdims=True)\n",
    "    p_tw_w = (n_wb / denom).T\n",
    "\n",
    "    p_tk_w_key = []\n",
    "    for ik in range(nbranches):\n",
    "        arr = n_w_key_b_list[ik]\n",
    "        denom = np.sum(arr, axis=1, keepdims=True)\n",
    "        p_tk_w_key.append((arr / denom).T)\n",
    "\n",
    "    denom = np.sum(n_wb, axis=0, keepdims=True)\n",
    "    p_w_tw = n_wb / denom\n",
    "\n",
    "    p_w_key_tk = []\n",
    "    for ik in range(nbranches):\n",
    "        arr = n_w_key_b_list[ik]\n",
    "        denom = np.sum(arr, axis=0, keepdims=True)\n",
    "        p_w_key_tk.append(arr / denom)\n",
    "\n",
    "    denom = np.sum(n_dbw, axis=1, keepdims=True)\n",
    "    p_tw_d = (n_dbw / denom).T\n",
    "\n",
    "    p_tk_d = []\n",
    "    for ik in range(nbranches):\n",
    "        arr = n_dbw_key_list[ik]\n",
    "        denom = np.sum(arr, axis=1, keepdims=True)\n",
    "        p_tk_d.append((arr / denom).T)\n",
    "\n",
    "    denom = np.sum(n_db, axis=1, keepdims=True)\n",
    "    p_td_d = (n_db / denom).T\n",
    "\n",
    "    # --- Collect results ---\n",
    "    result = {\n",
    "        'Bd': Bd, 'Bw': Bw, 'Bk': Bk,\n",
    "        'p_tw_w': p_tw_w, 'p_tk_w_key': p_tk_w_key, 'p_td_d': p_td_d,\n",
    "        'p_w_tw': p_w_tw, 'p_w_key_tk': p_w_key_tk, 'p_tw_d': p_tw_d, 'p_tk_d': p_tk_d\n",
    "    }\n",
    "\n",
    "    model.groups[l] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Numba kernel with O(1) offset lookup ---\n",
    "@njit\n",
    "def process_edges_numba_stack(sources, targets, z1, z2, kinds, weights,\n",
    "                              D, W, K_arr, offsets, nbranches,\n",
    "                              n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3):\n",
    "    m = len(sources)\n",
    "    for i in range(m):\n",
    "        v1 = sources[i]\n",
    "        v2 = targets[i]\n",
    "        w = weights[i]\n",
    "        t1 = z1[i]\n",
    "        t2 = z2[i]\n",
    "        kind = kinds[i]\n",
    "\n",
    "        # update doc-group counts\n",
    "        n_db[v1, t1] += w\n",
    "\n",
    "        if kind == 1:\n",
    "            # word node\n",
    "            idx_w = v2 - D\n",
    "            if 0 <= idx_w < n_wb.shape[0]:\n",
    "                n_wb[idx_w, t2] += w\n",
    "            n_dbw[v1, t2] += w\n",
    "\n",
    "        elif kind >= 2:\n",
    "            ik = kind - 2\n",
    "            if 0 <= ik < nbranches:\n",
    "                idx_k = v2 - offsets[ik]  # O(1) offset lookup\n",
    "                if 0 <= idx_k < K_arr[ik]:\n",
    "                    n_w_key_b3[ik, idx_k, t2] += w\n",
    "                    n_dbw_key3[ik, v1, t2] += w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd4b0c1-509b-41bc-88a2-1ddc63818836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume compare_groups_results is already defined (handles scalars, arrays, lists, NaNs)\n",
    "# from previous code\n",
    "def compare_groups_results(r1, r2, atol=1e-12, name=\"root\"):\n",
    "    \"\"\"\n",
    "    Compare two get_groups-like results.\n",
    "\n",
    "    Raises AssertionError if any mismatch.\n",
    "    Prints detailed info about differences.\n",
    "    \"\"\"\n",
    "    for key in r1:\n",
    "        val1 = r1[key]\n",
    "        val2 = r2[key]\n",
    "        full_name = f\"{name}.{key}\"\n",
    "\n",
    "        # Check type mismatch\n",
    "        assert type(val1) == type(val2), f\"Type mismatch for {full_name}: {type(val1)} vs {type(val2)}\"\n",
    "\n",
    "        if isinstance(val1, list):\n",
    "            assert len(val1) == len(val2), f\"List length mismatch for {full_name}\"\n",
    "            for i, (a, b) in enumerate(zip(val1, val2)):\n",
    "                elem_name = f\"{full_name}[{i}]\"\n",
    "                if isinstance(a, np.ndarray):\n",
    "                    assert a.shape == b.shape, f\"Shape mismatch for {elem_name}: {a.shape} vs {b.shape}\"\n",
    "                    mask = ~np.isclose(a, b, atol=atol, equal_nan=True)\n",
    "                    if np.any(mask):\n",
    "                        idx = np.argwhere(mask)\n",
    "                        n_diff = len(idx)\n",
    "                        print(f\"{elem_name}: {n_diff} differences found\")\n",
    "                        for j, (r, c) in enumerate(idx[:20]):\n",
    "                            print(f\"{elem_name}[{r},{c}]: {a[r,c]} vs {b[r,c]}\")\n",
    "                        if n_diff > 20:\n",
    "                            print(f\"... and {n_diff - 20} more differences\")\n",
    "                        raise AssertionError(f\"{elem_name} arrays differ\")\n",
    "                else:  # scalar in list\n",
    "                    assert a == b, f\"Scalar mismatch in {elem_name}: {a} vs {b}\"\n",
    "\n",
    "        elif isinstance(val1, np.ndarray):\n",
    "            assert val1.shape == val2.shape, f\"Shape mismatch for {full_name}: {val1.shape} vs {val2.shape}\"\n",
    "            mask = ~np.isclose(val1, val2, atol=atol, equal_nan=True)\n",
    "            if np.any(mask):\n",
    "                idx = np.argwhere(mask)\n",
    "                n_diff = len(idx)\n",
    "                print(f\"{full_name}: {n_diff} differences found\")\n",
    "                for j, (r, c) in enumerate(idx[:20]):\n",
    "                    print(f\"{full_name}[{r},{c}]: {val1[r,c]} vs {val2[r,c]}\")\n",
    "                if n_diff > 20:\n",
    "                    print(f\"... and {n_diff - 20} more differences\")\n",
    "                raise AssertionError(f\"{full_name} arrays differ\")\n",
    "\n",
    "        else:  # scalar\n",
    "            assert val1 == val2, f\"Scalar mismatch for {full_name}: {val1} vs {val2}\"\n",
    "\n",
    "    print(\"All keys match exactly (within tolerance and NaN equality).\")\n",
    "    \n",
    "def benchmark_get_groups(model, l=0):\n",
    "    \"\"\"\n",
    "    Benchmark original, optimized, and Numba-safe get_groups functions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting benchmarking...\\n\")\n",
    "    \n",
    "    # --- 1. Original ---\n",
    "    model.groups={}\n",
    "    start = time.time()\n",
    "    res_orig = get_groups(model, l=l)\n",
    "    t_orig = time.time() - start\n",
    "\n",
    "    # --- 2. fast ---\n",
    "    model.groups={}\n",
    "    start = time.time()\n",
    "    res_fast = get_groups_fast(model, l=l)\n",
    "    t_fast = time.time() - start\n",
    "\n",
    "    # --- Verify results ---\n",
    "    print(\"Verifying fast vs original...\")\n",
    "    compare_groups_results(res_orig, res_fast)\n",
    "    print(\"Optimized version matches original!\\n\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"Benchmark summary (seconds):\")\n",
    "    print(f\"Original: {t_orig:.4f}\")\n",
    "    print(f\"Optimized: {t_fast:.4f} (speedup {t_orig/t_fast:.2f}x)\")\n",
    "\n",
    "    return t_orig, t_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c9a9898-bf95-45f1-8b39-d2c954b7219f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata=read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7c94a-bf4c-46b7-8caf-d3da658b3aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 15:04:02,524 - INFO - Loading graph from results/mymodel_graph.xml.gz\n",
      "2025-09-15 15:04:02,588 - INFO - Fit number: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 (51, 110)\n",
      "Starting benchmarking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 15:04:13,979 - INFO - Loading graph from results/mymodel_graph.xml.gz\n",
      "2025-09-15 15:04:14,028 - INFO - Fit number: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying fast vs original...\n",
      "All keys match exactly (within tolerance and NaN equality).\n",
      "Optimized version matches original!\n",
      "\n",
      "Benchmark summary (seconds):\n",
      "Original: 2.4813\n",
      "Optimized: 0.2118 (speedup 11.71x)\n",
      "0.1 (99, 220)\n",
      "Starting benchmarking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 15:04:29,659 - INFO - Loading graph from results/mymodel_graph.xml.gz\n",
      "2025-09-15 15:04:29,706 - INFO - Fit number: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying fast vs original...\n",
      "All keys match exactly (within tolerance and NaN equality).\n",
      "Optimized version matches original!\n",
      "\n",
      "Benchmark summary (seconds):\n",
      "Original: 2.4771\n",
      "Optimized: 0.2088 (speedup 11.86x)\n",
      "0.15 (150, 330)\n"
     ]
    }
   ],
   "source": [
    "data={}\n",
    "data[\"Original\"]=[]\n",
    "data[\"Fast\"]=[]\n",
    "data[\"NumberofNodes\"]=[]\n",
    "\n",
    "for n in np.array(np.linspace(0.5,10, 20)/10):\n",
    "    mdata_sub = subsample_mudata(mdata, cell_frac=n, strat_mod=\"Peak\", strat_col=\"CellType\", random_seed=123)\n",
    "    print(n, mdata_sub.shape)\n",
    "\n",
    "    model=bionsbm(mdata_sub)\n",
    "    modalities=list(mdata_sub.mod.keys())   \n",
    "    dfs=[mdata_sub[key].to_df().T for key in modalities]\n",
    "    model.make_graph(dfs[0], dfs[1:])\n",
    "    model.fit(verbose=False, n_init=1, parallel=False)\n",
    "    \n",
    "    old_t, new_t = benchmark_get_groups(model, l=0)\n",
    "    data[\"Original\"].append(old_t)\n",
    "    data[\"Fast\"].append(new_t)\n",
    "    data[\"NumberofNodes\"].append(np.prod((mdata_sub.shape[0], mdata_sub[\"Peak\"].shape[1], mdata_sub[\"mRNA\"].shape[1], mdata_sub[\"lncRNA\"].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7568d21-f670-45f0-a7d7-ca0ed3145d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('time_get_groups.json', 'w') as f:\n",
    "    json.dump(pd.DataFrame.from_dict(data).astype(float).to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c348d3a-0882-41ac-98b0-2ce400a3c6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times=pd.DataFrame.from_dict(data)\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d10be1-3cbc-42ea-abd5-519aa9f00e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=times[\"NumberofNodes\"], y=np.array(times[\"Original\"]/times[\"Fast\"]))\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.xlabel(\"Fraction of data\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.title(\"get_groups Performance Benchmark\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Numer of nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0998b0a-8397-455e-add5-117246de08f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
