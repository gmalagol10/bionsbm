{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3592d8-8ffd-448a-860f-a152c5cce72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "import os, sys\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle as pickle\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import muon as mu\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import sparse\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7959897-5f46-4390-957f-093dadc62dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inherit hSBM code from https://github.com/martingerlach/hSBM_Topicmodel\n",
    "\"\"\"\n",
    "\n",
    "class bionsbm():\n",
    "    \"\"\"\n",
    "    Class to run bionsbm\n",
    "    \"\"\"\n",
    "    def __init__(self, obj, label=None, max_depth=6):\n",
    "        super().__init__()\n",
    "        self.keywords = []\n",
    "        self.nbranches = 1\n",
    "        self.modalities = []\n",
    "        self.max_depth = max_depth\n",
    "        self.obj = obj\n",
    "\n",
    "        if isinstance(obj, mu.MuData):\n",
    "            self.modalities=list(obj.mod.keys())   \n",
    "            dfs=[obj[key].to_df().T for key in self.modalities]\n",
    "            self.make_graph_multiple_df(dfs[0], dfs[1:])\n",
    "\n",
    "        elif isinstance(obj, ad.AnnData):\n",
    "            self.modalities=[\"Mod1\"]\n",
    "            self.make_graph_multiple_df(obj.to_df().T, [])\n",
    "\n",
    "        if label:\n",
    "            g_raw=self.g.copy()\n",
    "            print(\"Label found\")\n",
    "            metadata=obj[self.modalities[0]].obs\n",
    "            mymap = dict([(y,str(x)) for x,y in enumerate(sorted(set(obj[self.modalities[0]].obs[label])))])\n",
    "            inv_map = {v: k for k, v in mymap.items()}\n",
    "\n",
    "            docs_type=[int(mymap[metadata.loc[doc][label]]) for doc in self.documents]\n",
    "            types={}\n",
    "            types[\"Docs\"]=docs_type\n",
    "            for i, key in enumerate(self.modalities):\n",
    "                types[key]=[int(i+np.max(docs_type)+1) for a in range(0, obj[key].shape[0])]\n",
    "            node_type = g_raw.new_vertex_property('int', functools.reduce(lambda a, b : a+b, list(types.values())))\n",
    "            self.g = g_raw.copy()\n",
    "        else:\n",
    "            node_type=None\n",
    "        self.node_type=node_type \n",
    "\n",
    "    def save_graph(self, filename=\"graph.xml.gz\")->None:\n",
    "        \"\"\"\n",
    "        Save the graph\n",
    "\n",
    "        :param filename: name of the graph stored\n",
    "        \"\"\"\n",
    "        self.g.save(filename)\n",
    "\n",
    "    def dump_model(self, filename=\"bionsbm.pkl\"):\n",
    "        \"\"\"\n",
    "        Dump model using pickle\n",
    "\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "    def make_graph_multiple_df(self, df: pd.DataFrame, df_keyword_list: list)->None:\n",
    "        \"\"\"\n",
    "        Create a graph from two dataframes one with words, others with keywords or other layers of information\n",
    "\n",
    "        :param df: DataFrame with words on index and texts on columns\n",
    "        :param df_keyword_list: list of DataFrames with keywords on index and texts on columns\n",
    "        \"\"\"\n",
    "        df_all = df.copy(deep =True)\n",
    "        for ikey,df_keyword in enumerate(df_keyword_list):\n",
    "            df_keyword = df_keyword.reindex(columns=df.columns)\n",
    "            df_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "            df_keyword[\"kind\"] = ikey+2\n",
    "            df_all = pd.concat((df_all,df_keyword), axis=0)\n",
    "\n",
    "        def get_kind(word):\n",
    "            return 1 if word in df.index else df_all.at[word,\"kind\"]\n",
    "\n",
    "        self.nbranches = len(df_keyword_list)\n",
    "       \n",
    "        self.make_graph(df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "\n",
    "\n",
    "    def make_graph(self, df: pd.DataFrame, get_kind):\n",
    "        self.g = gt.Graph(directed=False)\n",
    "\n",
    "        n_docs, n_words = df.shape[1], df.shape[0]\n",
    "\n",
    "        # Add all vertices first\n",
    "        self.g.add_vertex(n_docs + n_words)\n",
    "\n",
    "        # Create vertex properties\n",
    "        name = self.g.new_vp(\"string\")\n",
    "        kind = self.g.new_vp(\"int\")\n",
    "        self.g.vp[\"name\"] = name\n",
    "        self.g.vp[\"kind\"] = kind\n",
    "\n",
    "        # Assign doc vertices (loop for names, array for kind)\n",
    "        for i, doc in enumerate(df.columns):\n",
    "            name[self.g.vertex(i)] = doc\n",
    "        kind.get_array()[:n_docs] = 0\n",
    "\n",
    "        # Assign word vertices (loop for names, array for kind)\n",
    "        for j, word in enumerate(df.index):\n",
    "            name[self.g.vertex(n_docs + j)] = word\n",
    "        kind.get_array()[n_docs:] = np.array([get_kind(w) for w in df.index], dtype=int)\n",
    "\n",
    "        # Edge weights\n",
    "        weight = self.g.new_ep(\"int\")\n",
    "        self.g.ep[\"count\"] = weight\n",
    "\n",
    "        # Build sparse edges\n",
    "        rows, cols = df.values.nonzero()\n",
    "        vals = df.values[rows, cols].astype(int)\n",
    "        edges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "        if len(edges)==0: raise ValueError(\"Empty graph\")\n",
    "\n",
    "        self.g.add_edge_list(edges, eprops=[weight])\n",
    "\n",
    "        # Remove edges with 0 weight\n",
    "        filter_edges = self.g.new_edge_property(\"bool\")\n",
    "        for e in self.g.edges():\n",
    "            filter_edges[e] = weight[e] > 0\n",
    "        self.g.set_edge_filter(filter_edges)\n",
    "        self.g.purge_edges()\n",
    "        self.g.clear_filters()\n",
    "\n",
    "        self.documents = df.columns\n",
    "        self.words = df.index[self.g.vp['kind'].a[n_docs:] == 1]\n",
    "        for ik in range(2, 2 + self.nbranches):\n",
    "            self.keywords.append(df.index[self.g.vp['kind'].a[n_docs:] == ik])\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, n_init=1, verbose=True, deg_corr=True, overlap=False, parallel=False, B_min=0, B_max=None, clabel=None, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Fit using minimize_nested_blockmodel_dl\n",
    "        \n",
    "        :param n_init: number of initialisation. The best will be kept\n",
    "        :param verbose: Print output\n",
    "        :param deg_corr: use deg corrected model\n",
    "        :param overlap: use overlapping model\n",
    "        :param parallel: perform parallel moves\n",
    "        :param  \\*args: positional arguments to pass to gt.minimize_nested_blockmodel_dl\n",
    "        :param  \\*\\*kwargs: keywords arguments to pass to gt.minimize_nested_blockmodel_dl\n",
    "        \"\"\"\n",
    "        if clabel == None:\n",
    "            clabel = self.g.vp['kind']\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "        else:\n",
    "            print(f\"Clabel is {clabel}, assigning partitions to vertices\", flush=True)\n",
    "            state_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "    \n",
    "        state_args[\"eweight\"] = self.g.ep.count\n",
    "        min_entropy = np.inf\n",
    "        best_state = None\n",
    "        state_args[\"deg_corr\"] = deg_corr\n",
    "        state_args[\"overlap\"] = overlap\n",
    "\n",
    "        if B_max is None:\n",
    "            B_max = self.g.num_vertices()\n",
    "            \n",
    "        multilevel_mcmc_args={\"B_min\": B_min, \"B_max\": B_max, \"verbose\": verbose,\"parallel\" : parallel}\n",
    "\n",
    "        print(\"multilevel_mcmc_args is \\n\", multilevel_mcmc_args, flush=True)\n",
    "        print(\"state_args is \\n\", state_args, flush=True)\n",
    "\n",
    "        for _ in range(n_init):\n",
    "            print(\"Fit number:\", _, flush=True)\n",
    "            state = gt.minimize_nested_blockmodel_dl(self.g, state_args=state_args, multilevel_mcmc_args=multilevel_mcmc_args, *args, **kwargs)\n",
    "            \n",
    "            entropy = state.entropy()\n",
    "            if entropy < min_entropy:\n",
    "                min_entropy = entropy\n",
    "                self.state = state\n",
    "                \n",
    "        self.mdl = min_entropy\n",
    "\n",
    "        L = len(self.state.levels)\n",
    "        self.L = L\n",
    "        self.groups = {}\n",
    "\n",
    "\n",
    "    def get_mdl(self):\n",
    "        \"\"\"\n",
    "        Get minimum description length\n",
    "\n",
    "        Proxy to self.state.entropy()\n",
    "        \"\"\"\n",
    "        return self.mdl\n",
    "            \n",
    "    def _get_shape(self):\n",
    "        \"\"\"\n",
    "        :return: list of tuples (number of documents, number of words, (number of keywords,...))\n",
    "        \"\"\"\n",
    "        D = int(np.sum(self.g.vp['kind'].a == 0)) #documents\n",
    "        W = int(np.sum(self.g.vp['kind'].a == 1)) #words\n",
    "        K = [int(np.sum(self.g.vp['kind'].a == (k+2))) for k in range(self.nbranches)] #keywords\n",
    "        return D, W, K\n",
    "\n",
    "    # Helper functions      \n",
    "\n",
    "    def get_groups(self, l=0):\n",
    "\n",
    "    # --- Numba function for edge processing with list of arrays ---\n",
    "        @njit\n",
    "        def process_edges_numba_list(sources, targets, z1, z2, kinds, weights,\n",
    "                                     D, W, K_arr, nbranches,\n",
    "                                     n_db, n_wb, n_dbw, n_w_key_b_list, n_dbw_key_list):\n",
    "\n",
    "            for i in range(len(sources)):\n",
    "                v1 = sources[i]\n",
    "                v2 = targets[i]\n",
    "                w = weights[i]\n",
    "                t1 = z1[i]\n",
    "                t2 = z2[i]\n",
    "                kind = kinds[i]\n",
    "\n",
    "                n_db[v1, t1] += w\n",
    "\n",
    "                if kind == 1:\n",
    "                    n_wb[v2 - D, t2] += w\n",
    "                    n_dbw[v1, t2] += w\n",
    "                else:\n",
    "                    ik = kind - 2\n",
    "                    offset = D + W\n",
    "                    for j in range(ik):\n",
    "                        offset += K_arr[j]\n",
    "                    n_w_key_b_list[ik][v2 - offset, t2] += w\n",
    "                    n_dbw_key_list[ik][v1, t2] += w\n",
    "\n",
    "\n",
    "        if l in self.groups:\n",
    "            return self.groups[l]\n",
    "\n",
    "        state_l = self.state.project_level(l).copy(overlap=True)\n",
    "        state_l_edges = state_l.get_edge_blocks()\n",
    "        B = state_l.get_B()\n",
    "        D, W, K = self._get_shape()\n",
    "        nbranches = self.nbranches\n",
    "\n",
    "        # Preallocate arrays\n",
    "        n_wb = np.zeros((W, B))\n",
    "        n_db = np.zeros((D, B))\n",
    "        n_dbw = np.zeros((D, B))\n",
    "\n",
    "        # For branches, use list of arrays (one per branch) to avoid broadcasting issues\n",
    "        n_w_key_b = [np.zeros((K[ik], B)) for ik in range(nbranches)]\n",
    "        n_dbw_key = [np.zeros((D, B)) for _ in range(nbranches)]\n",
    "\n",
    "        # Convert graph edges to arrays\n",
    "        edges = list(self.g.edges())\n",
    "        sources = np.array([e.source() for e in edges], dtype=np.int64)\n",
    "        targets = np.array([e.target() for e in edges], dtype=np.int64)\n",
    "        weights = np.array([self.g.ep[\"count\"][e] for e in edges], dtype=np.float64)\n",
    "        z1_arr = np.array([state_l_edges[e][0] for e in edges], dtype=np.int64)\n",
    "        z2_arr = np.array([state_l_edges[e][1] for e in edges], dtype=np.int64)\n",
    "        kinds = np.array([self.g.vp['kind'][v] for v in targets], dtype=np.int64)\n",
    "\n",
    "        # --- Edge processing (Numba-accelerated) ---\n",
    "        process_edges_numba_list(sources, targets, z1_arr, z2_arr, kinds, weights, D, W, K, nbranches, n_db, n_wb, n_dbw, n_w_key_b, n_dbw_key)\n",
    "\n",
    "        # --- Keep only nonzero columns safely ---\n",
    "        ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "        n_db = n_db[:, ind_d]\n",
    "        Bd = len(ind_d)\n",
    "\n",
    "        ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "        n_wb = n_wb[:, ind_w]\n",
    "        Bw = len(ind_w)\n",
    "\n",
    "        ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "        n_dbw = n_dbw[:, ind_w2]\n",
    "\n",
    "        Bk = []\n",
    "        for ik in range(nbranches):\n",
    "            ind_wk = np.where(np.sum(n_w_key_b[ik], axis=0) > 0)[0]\n",
    "            n_w_key_b[ik] = n_w_key_b[ik][:, ind_wk].copy()\n",
    "            Bk.append(len(ind_wk))\n",
    "\n",
    "            ind_w2k = np.where(np.sum(n_dbw_key[ik], axis=0) > 0)[0]\n",
    "            n_dbw_key[ik] = n_dbw_key[ik][:, ind_w2k].copy()\n",
    "\n",
    "        # --- Compute probabilities ---\n",
    "        p_tw_w = (n_wb / np.nansum(n_wb, axis=1)[:, None]).T\n",
    "        p_tk_w_key = [(n_w_key_b[ik] / np.nansum(n_w_key_b[ik], axis=1)[:, None]).T\n",
    "                      for ik in range(nbranches)]\n",
    "        p_w_tw = n_wb / np.nansum(n_wb, axis=0)[None, :]\n",
    "        p_w_key_tk = [n_w_key_b[ik] / np.nansum(n_w_key_b[ik], axis=0)[None, :]\n",
    "                      for ik in range(nbranches)]\n",
    "        p_tw_d = (n_dbw / np.nansum(n_dbw, axis=1)[:, None]).T\n",
    "        p_tk_d = [(n_dbw_key[ik] / np.nansum(n_dbw_key[ik], axis=1)[:, None]).T\n",
    "                  for ik in range(nbranches)]\n",
    "        p_td_d = (n_db / np.nansum(n_db, axis=1)[:, None]).T\n",
    "\n",
    "        result = {    'Bd': Bd, 'Bw': Bw, 'Bk': Bk,\n",
    "                    'p_tw_w': p_tw_w,\n",
    "                    'p_tk_w_key': p_tk_w_key,\n",
    "                    'p_td_d': p_td_d,\n",
    "                    'p_w_tw': p_w_tw,\n",
    "                    'p_w_key_tk': p_w_key_tk,\n",
    "                    'p_tw_d': p_tw_d,\n",
    "                    'p_tk_d': p_tk_d}\n",
    "\n",
    "        self.groups[l] = result\n",
    "        return result\n",
    "\n",
    "    def get_groups_fast_numba_bipartite_safe(self, l=0):\n",
    "        \"\"\"\n",
    "        Numba-accelerated get_groups that is robust for bipartite graphs (nbranches == 0)\n",
    "        and for arbitrary number of partitions.\n",
    "        \"\"\"\n",
    "    \n",
    "        @njit\n",
    "        def process_edges_numba_stack(sources, targets, z1, z2, kinds, weights,\n",
    "                                      D, W, K_arr, nbranches,\n",
    "                                      n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3):\n",
    "            \"\"\"\n",
    "            Numba-compiled loop that increments the stacked accumulator arrays.\n",
    "            This function is defensive: if a 'kind' references a branch index out of range,\n",
    "            or an index into keywords is out of range, it's ignored (so bipartite graphs keep working).\n",
    "            \"\"\"\n",
    "            m = len(sources)\n",
    "            for i in range(m):\n",
    "                v1 = sources[i]\n",
    "                v2 = targets[i]\n",
    "                w = weights[i]\n",
    "                t1 = z1[i]\n",
    "                t2 = z2[i]\n",
    "                kind = kinds[i]\n",
    "        \n",
    "                # update doc-group counts (always)\n",
    "                n_db[v1, t1] += w\n",
    "        \n",
    "                if kind == 1:\n",
    "                    # word node\n",
    "                    idx_w = v2 - D\n",
    "                    if idx_w >= 0 and idx_w < n_wb.shape[0]:\n",
    "                        n_wb[idx_w, t2] += w\n",
    "                    # update doc->word-group\n",
    "                    n_dbw[v1, t2] += w\n",
    "        \n",
    "                elif kind >= 2:\n",
    "                    ik = kind - 2\n",
    "                    # guard: only process if ik is a valid branch index\n",
    "                    if ik >= 0 and ik < nbranches:\n",
    "                        # compute offset = D + W + sum(K_arr[:ik])\n",
    "                        offset = D + W\n",
    "                        for j in range(ik):\n",
    "                            offset += K_arr[j]\n",
    "                        idx_k = v2 - offset\n",
    "                        # guard keyword index bounds\n",
    "                        if idx_k >= 0 and idx_k < K_arr[ik]:\n",
    "                            n_w_key_b3[ik, idx_k, t2] += w\n",
    "                            n_dbw_key3[ik, v1, t2] += w\n",
    "                        # else: out-of-range keyword index -> ignore to remain robust\n",
    "                else:\n",
    "                    # unexpected kind (<1): ignore for safety (original assumed only kind==1 or >=2)\n",
    "                    pass\n",
    "            \n",
    "            # cache\n",
    "         #   if l in self.groups:\n",
    "          #      return self.groups[l]\n",
    "        \n",
    "        state_l = self.state.project_level(l).copy(overlap=True)\n",
    "        state_l_edges = state_l.get_edge_blocks()\n",
    "        B = state_l.get_B()\n",
    "        D, W, K = self._get_shape()\n",
    "        nbranches = self.nbranches\n",
    "    \n",
    "        # Preallocate primary arrays (word/doc)\n",
    "        n_wb = np.zeros((W, B), dtype=np.float64)    # words x word-groups\n",
    "        n_db = np.zeros((D, B), dtype=np.float64)    # docs  x doc-groups\n",
    "        n_dbw = np.zeros((D, B), dtype=np.float64)   # docs  x word-groups\n",
    "    \n",
    "        # Preallocate stacked branch arrays (shape: nbranches x max_K x B) and (nbranches x D x B)\n",
    "        if nbranches > 0:\n",
    "            max_K = int(np.max(K))\n",
    "            # If some K are zero, max_K will still be >=0; stack is safe\n",
    "            n_w_key_b3 = np.zeros((nbranches, max_K, B), dtype=np.float64)\n",
    "            n_dbw_key3 = np.zeros((nbranches, D, B), dtype=np.float64)\n",
    "        else:\n",
    "            # empty stacked arrays if no branches\n",
    "            n_w_key_b3 = np.zeros((0, 0, B), dtype=np.float64)\n",
    "            n_dbw_key3 = np.zeros((0, D, B), dtype=np.float64)\n",
    "    \n",
    "        # Convert graph edges to arrays\n",
    "        edges = list(self.g.edges())\n",
    "        m = len(edges)\n",
    "        sources = np.empty(m, dtype=np.int64)\n",
    "        targets = np.empty(m, dtype=np.int64)\n",
    "        z1_arr = np.empty(m, dtype=np.int64)\n",
    "        z2_arr = np.empty(m, dtype=np.int64)\n",
    "        weights = np.empty(m, dtype=np.float64)\n",
    "        kinds = np.empty(m, dtype=np.int64)\n",
    "    \n",
    "        for i, e in enumerate(edges):\n",
    "            sources[i] = int(e.source())\n",
    "            targets[i] = int(e.target())\n",
    "            z1_arr[i] = int(state_l_edges[e][0])\n",
    "            z2_arr[i] = int(state_l_edges[e][1])\n",
    "            weights[i] = float(self.g.ep[\"count\"][e])\n",
    "            kinds[i] = int(self.g.vp['kind'][int(e.target())])\n",
    "    \n",
    "        K_arr = np.array(K, dtype=np.int64)  # can be empty if nbranches==0\n",
    "    \n",
    "        # --- Numba edge processing (single compiled function for all cases) ---\n",
    "        process_edges_numba_stack(\n",
    "            sources, targets, z1_arr, z2_arr, kinds, weights,\n",
    "            D, W, K_arr, nbranches,\n",
    "            n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3\n",
    "        )\n",
    "    \n",
    "        # --- Trim empty columns for doc/word arrays (same logic as original) ---\n",
    "        ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "        n_db = n_db[:, ind_d]\n",
    "        Bd = len(ind_d)\n",
    "    \n",
    "        ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "        n_wb = n_wb[:, ind_w]\n",
    "        Bw = len(ind_w)\n",
    "    \n",
    "        ind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "        n_dbw = n_dbw[:, ind_w2]\n",
    "    \n",
    "        # --- Convert stacked branch arrays into per-branch lists (safe slicing) ---\n",
    "        n_w_key_b_list = []\n",
    "        n_dbw_key_list = []\n",
    "        Bk = []\n",
    "    \n",
    "        for ik in range(nbranches):\n",
    "            Kk = int(K_arr[ik])\n",
    "            if Kk > 0:\n",
    "                # compute which columns (groups) are non-zero\n",
    "                col_sums = np.sum(n_w_key_b3[ik, :Kk, :], axis=0)\n",
    "                ind_wk = np.where(col_sums > 0)[0]\n",
    "                # slice and copy into a per-branch array (Kk x Bk)\n",
    "                if ind_wk.size > 0:\n",
    "                    n_w_key_b_list.append(n_w_key_b3[ik, :Kk, :][:, ind_wk].copy())\n",
    "                else:\n",
    "                    # keep shape (Kk, 0) if there are no columns\n",
    "                    n_w_key_b_list.append(np.zeros((Kk, 0), dtype=np.float64))\n",
    "                Bk.append(len(ind_wk))\n",
    "            else:\n",
    "                # branch with 0 keywords\n",
    "                n_w_key_b_list.append(np.zeros((0, 0), dtype=np.float64))\n",
    "                Bk.append(0)\n",
    "    \n",
    "            # doc x keyword-groups for this branch\n",
    "            col_sums_dbw = np.sum(n_dbw_key3[ik], axis=0)\n",
    "            ind_w2k = np.where(col_sums_dbw > 0)[0]\n",
    "            if ind_w2k.size > 0:\n",
    "                n_dbw_key_list.append(n_dbw_key3[ik][:, ind_w2k].copy())\n",
    "            else:\n",
    "                n_dbw_key_list.append(np.zeros((D, 0), dtype=np.float64))\n",
    "    \n",
    "        # --- Compute probabilities exactly like the original (division -> NaN if denominator==0) ---\n",
    "        # P(t_w | w)\n",
    "        denom = np.sum(n_wb, axis=1, keepdims=True)  # (W,1)\n",
    "        p_tw_w = (n_wb / denom).T\n",
    "    \n",
    "        # P(t_k | keyword) per branch\n",
    "        p_tk_w_key = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_w_key_b_list[ik]\n",
    "            denom = np.sum(arr, axis=1, keepdims=True)\n",
    "            p_tk_w_key.append((arr / denom).T)\n",
    "    \n",
    "        # P(w | t_w)\n",
    "        denom = np.sum(n_wb, axis=0, keepdims=True)  # (1,Bw)\n",
    "        p_w_tw = n_wb / denom\n",
    "    \n",
    "        # P(keyword | t_w_key) per branch\n",
    "        p_w_key_tk = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_w_key_b_list[ik]\n",
    "            denom = np.sum(arr, axis=0, keepdims=True)\n",
    "            p_w_key_tk.append(arr / denom)\n",
    "    \n",
    "        # P(t_w | d)\n",
    "        denom = np.sum(n_dbw, axis=1, keepdims=True)\n",
    "        p_tw_d = (n_dbw / denom).T\n",
    "    \n",
    "        # P(t_k | d) per branch\n",
    "        p_tk_d = []\n",
    "        for ik in range(nbranches):\n",
    "            arr = n_dbw_key_list[ik]\n",
    "            denom = np.sum(arr, axis=1, keepdims=True)\n",
    "            p_tk_d.append((arr / denom).T)\n",
    "    \n",
    "        # P(t_d | d)\n",
    "        denom = np.sum(n_db, axis=1, keepdims=True)\n",
    "        p_td_d = (n_db / denom).T\n",
    "    \n",
    "        result = {\n",
    "            'Bd': Bd,\n",
    "            'Bw': Bw,\n",
    "            'Bk': Bk,\n",
    "            'p_tw_w': p_tw_w,\n",
    "            'p_tk_w_key': p_tk_w_key,\n",
    "            'p_td_d': p_td_d,\n",
    "            'p_w_tw': p_w_tw,\n",
    "            'p_w_key_tk': p_w_key_tk,\n",
    "            'p_tw_d': p_tw_d,\n",
    "            'p_tk_d': p_tk_d,\n",
    "        }\n",
    "    \n",
    "        self.groups[l] = result\n",
    "        return result\n",
    "\n",
    "\n",
    "  \n",
    "    def save_single_level(self, l: int, name: str) -> None:\n",
    "        \"\"\"\n",
    "        Save per-level probability matrices (topics, clusters, documents) for the given level.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        l : int\n",
    "            The level index to save. Must be within the range of available model levels.\n",
    "        name : str\n",
    "            Base path (folder + prefix) where files will be written.\n",
    "            Example: \"results/mymodel\" → files like:\n",
    "                - results/mymodel_level_0_mainfeature_topics.tsv.gz\n",
    "                - results/mymodel_level_0_clusters.tsv.gz\n",
    "                - results/mymodel_level_0_mainfeature_topics_documents.tsv.gz\n",
    "                - results/mymodel_level_0_metafeature_topics.tsv.gz\n",
    "                - results/mymodel_level_0_metafeature_topics_documents.tsv.gz\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Files are written as tab-separated values (`.tsv.gz`) with gzip compression.\n",
    "        - Handles both the main feature (`self.modalities[0]`) and any meta-features (`self.modalities[1:]`).\n",
    "        - Raises RuntimeError if any file cannot be written.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Validate inputs ---\n",
    "        if not isinstance(l, int) or l < 0 or l >= len(self.state.levels):\n",
    "            raise ValueError(f\"Invalid level index {l}. Must be between 0 and {len(self.state.levels) - 1}.\")\n",
    "        if not isinstance(name, str) or not name.strip():\n",
    "            raise ValueError(\"`name` must be a non-empty string path prefix.\")\n",
    "\n",
    "        main_feature = self.modalities[0]\n",
    "\n",
    "        try:\n",
    "            data = self.get_groups_fast_numba_bipartite_safe(l)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to get group data for level {l}: {e}\") from e\n",
    "\n",
    "        # Helper to safely save a DataFrame\n",
    "        def _safe_save(df, filepath):\n",
    "            try:\n",
    "                Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "                df.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "        # Helper to safely save a DataFrame\n",
    "        def _safe_save(df, filepath):\n",
    "            try:\n",
    "                Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "                df.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "        # --- P(document | cluster) ---\n",
    "        clusters = pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)\n",
    "        _safe_save(clusters, f\"{name}_level_{l}_clusters.tsv.gz\")\n",
    "\n",
    "\n",
    "        # --- P(main_feature | main_topic) ---\n",
    "        p_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_w_tw, f\"{name}_level_{l}_{main_feature}_topics.tsv.gz\")\n",
    "\n",
    "        # --- P(main_topic | documents) ---\n",
    "        p_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "            columns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "        _safe_save(p_tw_d, f\"{name}_level_{l}_{main_feature}_topics_documents.tsv.gz\")\n",
    "\n",
    "        # --- P(meta_feature | meta_topic_feature), if any ---\n",
    "        if len(self.modalities) > 1:\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                feat_topic = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "                    columns=[f\"{meta_features}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(feat_topic, f\"{name}_level_{l}_{meta_features}_topics.tsv.gz\")\n",
    "\n",
    "\n",
    "            # --- P(meta_topic | document) ---\n",
    "            for k, meta_features in enumerate(self.modalities[1:]):\n",
    "                p_tk_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "                    columns=[f\"{meta_features}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "                _safe_save(p_tk_d, f\"{name}_level_{l}_{meta_features}_topics_documents.tsv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "    def save_data(self, name: str = \"results/mymodel\") -> None:\n",
    "        \"\"\"\n",
    "        Save the global graph, model, state, and level-specific data for the current nSBM self.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str, optional\n",
    "            Base path (folder + prefix) where all outputs will be saved.\n",
    "            Example: \"results/mymodel\" will produce:\n",
    "                - results/mymodel_graph.xml.gz\n",
    "                - results/mymodel_model.pkl    \n",
    "                - results/mymodel_entropy.txt\n",
    "                - results/mymodel_state.pkl\n",
    "                - results/mymodel_level_X_*.tsv.gz  (per level, up to 6 levels)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The parent folder is created automatically if it does not exist.\n",
    "        - Level saving is parallelized with threads for efficiency in I/O.\n",
    "        - By default, at most 6 levels are saved, or fewer if the model has <6 levels.\n",
    "        - Exceptions in parallel tasks are caught and reported without stopping other tasks.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Validate name ---\n",
    "        if not isinstance(name, str) or not name.strip():\n",
    "            raise ValueError(\"`name` must be a non-empty string representing the save path.\")\n",
    "\n",
    "        # --- Ensure folder exists ---\n",
    "        folder = os.path.dirname(name)\n",
    "        if folder:\n",
    "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --- Save global files ---\n",
    "        try:\n",
    "            self.save_graph(filename=f\"{name}_graph.xml.gz\")\n",
    "            self.dump_model(filename=f\"{name}_model.pkl\")\n",
    "\n",
    "            with open(f\"{name}_entropy.txt\", \"w\") as f:\n",
    "                f.write(str(self.state.entropy()))\n",
    "\n",
    "            with open(f\"{name}_state.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.state, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save global files for model '{name}': {e}\") from e\n",
    "\n",
    "\n",
    "        # --- Save levels in parallel (threaded to avoid data duplication) ---\n",
    "        L = min(len(self.state.levels), self.max_depth)\n",
    "        if L == 0:\n",
    "            print(\"Nothing to save\")\n",
    "            return  # nothing to save\n",
    "\n",
    "        errors = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {executor.submit(self.save_single_level, l, name): l for l in range(L)}\n",
    "            for future in as_completed(futures):\n",
    "                l = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    errors.append((l, str(e)))\n",
    "\n",
    "        if errors:\n",
    "            msg = \"; \".join([f\"Level {l}: {err}\" for l, err in errors])\n",
    "            raise RuntimeError(f\"Errors occurred while saving levels: {msg}\")\n",
    "\n",
    "\n",
    "    def get_V(self):\n",
    "        '''\n",
    "        return number of word-nodes == types\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 1))  # no. of types\n",
    "\n",
    "    def get_D(self):\n",
    "        '''\n",
    "        return number of doc-nodes == number of documents\n",
    "        '''\n",
    "        return int(np.sum(self.g.vp['kind'].a == 0))  # no. of types\n",
    "\n",
    "    def get_N(self):\n",
    "        '''\n",
    "        return number of edges == tokens\n",
    "        '''\n",
    "        return int(self.g.num_edges())  # no. of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e575da5f-1e6d-4477-bc07-43f154b129ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mdata=mu.read_h5mu(\"../bionsbm/Test_data.h5mu\")\n",
    "mdata=mu.read_h5mu(\"../bionsbm/Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98cb375e-8c04-4a13-94ba-0756e27da10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multilevel_mcmc_args is \n",
      " {'B_min': 0, 'B_max': 3200, 'verbose': False, 'parallel': False}\n",
      "state_args is \n",
      " {'clabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x751310303e90, at 0x75127c7e9690>, 'pclabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x751310303e90, at 0x75127c7e9690>, 'eweight': <EdgePropertyMap object with value type 'int32_t', for Graph 0x751310303e90, at 0x7512a81f51d0>, 'deg_corr': True, 'overlap': False}\n",
      "Fit number: 0\n"
     ]
    }
   ],
   "source": [
    "model_single = bionsbm(mdata)\n",
    "model_single.fit(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d9c0ae-1637-4c6c-a4b8-2eb3a3bb74a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_single.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e55261e-1235-4c65-bc79-952fb6b99687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lncRNA_topic_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>##LINC00566</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC02191</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC00858</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##EHHADH-AS1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC02447</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC02366</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##NFIA-AS2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC01311</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##CDIPTOSP</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##LINC01520</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              lncRNA_topic_0\n",
       "##LINC00566              0.0\n",
       "##LINC02191              0.0\n",
       "##LINC00858              0.0\n",
       "##EHHADH-AS1             0.0\n",
       "##LINC02447              0.0\n",
       "...                      ...\n",
       "##LINC02366              0.0\n",
       "##NFIA-AS2               0.0\n",
       "##LINC01311              0.0\n",
       "##CDIPTOSP               0.0\n",
       "##LINC01520              0.0\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results/mymodel_level_1_lncRNA_topics.tsv.gz\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d702d8-e110-4ce3-ac2d-dc17d3c5653a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1e84a35-345d-4799-9412-64476cd9ea37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bd': 5,\n",
       " 'Bw': 13,\n",
       " 'Bk': [],\n",
       " 'p_tw_w': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'p_tk_w_key': [],\n",
       " 'p_td_d': array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'p_w_tw': array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.0263789 , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.01289802, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.01753325, ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " 'p_w_key_tk': [],\n",
       " 'p_tw_d': array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.1       ],\n",
       "        [0.04166667, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.54166667, 0.        , 0.        , ..., 0.73076923, 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.53333333, 0.22222222, ..., 0.        , 0.        ,\n",
       "         0.3       ],\n",
       "        [0.        , 0.06666667, 0.33333333, ..., 0.        , 0.16666667,\n",
       "         0.3       ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.03846154, 0.5       ,\n",
       "         0.        ]]),\n",
       " 'p_tk_d': []}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_single = model_single.get_groups_fast_numba_bipartite_safe(l=0)\n",
    "res_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e7e858a-0d50-4c47-b53b-bdbe50a0e64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multilevel_mcmc_args is \n",
      " {'B_min': 0, 'B_max': 3200, 'verbose': False, 'parallel': False}\n",
      "state_args is \n",
      " {'clabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x7a05207e1050, at 0x7a0531fb2010>, 'pclabel': <VertexPropertyMap object with value type 'int32_t', for Graph 0x7a05207e1050, at 0x7a0531fb2010>, 'eweight': <EdgePropertyMap object with value type 'int32_t', for Graph 0x7a05207e1050, at 0x7a0536ecaad0>, 'deg_corr': True, 'overlap': False}\n",
      "Fit number: 0\n"
     ]
    }
   ],
   "source": [
    "model_multi = bionsbm(mdata)\n",
    "model_multi.fit(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a5a0f2f-51bd-4737-bbe6-ae87bbc20e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bd': 6,\n",
       " 'Bw': 14,\n",
       " 'Bk': [6, 1],\n",
       " 'p_tw_w': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'p_tk_w_key': [array([[ 1.,  0., nan, ...,  1.,  1.,  0.],\n",
       "         [ 0.,  0., nan, ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., nan, ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., nan, ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., nan, ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1., nan, ...,  0.,  0.,  1.]]),\n",
       "  array([[nan, nan, nan, nan, nan,  1.,  1., nan,  1., nan, nan, nan, nan,\n",
       "          nan,  1.,  1., nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan,  1.,  1., nan, nan,  1., nan, nan, nan, nan,\n",
       "          nan, nan,  1., nan, nan, nan, nan, nan, nan,  1.,  1., nan, nan,\n",
       "           1., nan, nan, nan, nan, nan,  1., nan, nan, nan, nan,  1.,  1.,\n",
       "          nan, nan,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan,  1., nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan,\n",
       "           1., nan,  1.,  1., nan, nan, nan, nan, nan, nan, nan,  1., nan,\n",
       "          nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,\n",
       "          nan,  1., nan, nan, nan, nan, nan, nan, nan,  1., nan,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan,  1., nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan,  1., nan, nan, nan,  1.,  1., nan, nan,\n",
       "           1., nan, nan, nan, nan, nan, nan, nan,  1., nan,  1.,  1., nan,\n",
       "           1., nan, nan, nan,  1.,  1., nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan, nan,  1., nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan,  1., nan,  1., nan,  1., nan,  1., nan,  1., nan, nan, nan,\n",
       "          nan,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan,  1.,\n",
       "           1., nan, nan, nan, nan,  1.,  1., nan, nan, nan,  1., nan, nan,\n",
       "           1., nan,  1., nan, nan,  1., nan, nan,  1., nan,  1., nan, nan,\n",
       "          nan,  1.,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan,  1., nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan,  1.,\n",
       "          nan,  1., nan, nan, nan, nan,  1., nan, nan, nan, nan, nan, nan,\n",
       "           1., nan, nan, nan, nan, nan, nan,  1.,  1., nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan,  1.,  1., nan, nan,  1., nan, nan, nan,\n",
       "           1.,  1.,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan,  1., nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan,  1.,  1., nan,  1., nan,  1., nan, nan,  1., nan,  1., nan,\n",
       "          nan,  1.,  1.,  1., nan, nan, nan, nan, nan, nan,  1.,  1., nan,\n",
       "           1., nan,  1., nan, nan,  1., nan, nan,  1., nan, nan, nan,  1.,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan, nan,\n",
       "           1., nan, nan,  1., nan,  1., nan,  1., nan,  1., nan, nan, nan,\n",
       "           1., nan, nan,  1., nan, nan,  1., nan,  1.,  1., nan, nan,  1.,\n",
       "          nan, nan, nan, nan,  1., nan, nan,  1.,  1., nan,  1., nan, nan,\n",
       "          nan, nan,  1., nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan,  1.,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan,  1.,\n",
       "           1., nan,  1., nan, nan, nan,  1., nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan, nan, nan, nan, nan]])],\n",
       " 'p_td_d': array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'p_w_tw': array([[0.        , 0.00123423, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.00877674, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.01193088, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " 'p_w_key_tk': [array([[0.00149701, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.08177827],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         ...,\n",
       "         [0.00037425, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.00224551, 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.00713502]]),\n",
       "  array([[0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.02492212],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01557632],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.01869159],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01557632],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.01869159],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.02180685],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01557632],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.03426791],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.02180685],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01869159],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.02180685],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.02180685],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01557632],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00311526],\n",
       "         [0.03426791],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.01557632],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.04049844],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.03426791],\n",
       "         [0.01246106],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.00311526],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00934579],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.00623053],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]])],\n",
       " 'p_tw_d': array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.625     , 0.        , 0.        , ..., 0.88461538, 0.        ,\n",
       "         0.1       ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.16666667,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.11111111, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.06666667, 0.22222222, ..., 0.03846154, 0.        ,\n",
       "         0.1       ],\n",
       "        [0.        , 0.53333333, 0.33333333, ..., 0.        , 0.        ,\n",
       "         0.4       ]]),\n",
       " 'p_tk_d': [array([[0.6       , 0.66666667, 0.        , ..., 0.        , 0.75      ,\n",
       "          0.4       ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.25      ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.4       ],\n",
       "         [0.        , 0.33333333, 1.        , ..., 0.14285714, 0.        ,\n",
       "          0.2       ],\n",
       "         [0.4       , 0.        , 0.        , ..., 0.85714286, 0.        ,\n",
       "          0.        ]]),\n",
       "  array([[nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan,\n",
       "          nan, nan, nan,  1.,  1., nan, nan,  1., nan,  1.,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan,  1., nan,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan,  1., nan,\n",
       "          nan, nan,  1., nan, nan, nan,  1., nan,  1., nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1., nan,  1., nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan,  1., nan, nan, nan, nan,  1., nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan,  1.,  1., nan,  1.,  1.,  1., nan, nan,  1., nan, nan,\n",
       "           1., nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan,  1., nan,\n",
       "           1.,  1.,  1., nan, nan, nan,  1., nan, nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan,  1., nan,  1., nan, nan, nan,  1.,  1.,\n",
       "          nan, nan,  1., nan,  1., nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           1., nan,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan,  1., nan,  1., nan, nan,\n",
       "          nan, nan,  1.,  1., nan, nan, nan, nan,  1., nan, nan, nan,  1.,\n",
       "          nan,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan,  1., nan,\n",
       "          nan,  1., nan, nan, nan, nan,  1., nan,  1., nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan,  1.,  1.,  1., nan, nan, nan, nan, nan,  1.,\n",
       "           1., nan, nan, nan,  1.,  1.,  1., nan, nan,  1., nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1., nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1., nan,  1., nan, nan, nan,\n",
       "           1., nan, nan, nan, nan,  1., nan,  1., nan,  1.,  1.,  1., nan,\n",
       "          nan,  1., nan, nan, nan, nan, nan,  1., nan, nan, nan,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan, nan,  1.,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "           1., nan, nan, nan,  1., nan, nan,  1., nan, nan, nan,  1., nan,\n",
       "          nan, nan,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan,  1., nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan,\n",
       "          nan,  1., nan, nan, nan, nan,  1.,  1., nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan,  1., nan,  1.,  1., nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,  1.,\n",
       "          nan,  1., nan,  1., nan,  1., nan, nan, nan, nan, nan, nan, nan,\n",
       "           1.,  1., nan, nan, nan, nan,  1.,  1., nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan,  1., nan,  1., nan, nan,  1.,\n",
       "          nan,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan,  1., nan,  1.,  1., nan, nan, nan,  1.,\n",
       "          nan, nan, nan,  1., nan, nan, nan, nan, nan,  1.,  1.,  1., nan,\n",
       "          nan, nan, nan,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           1.,  1., nan, nan, nan, nan,  1.,  1., nan, nan,  1.,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           1., nan, nan, nan, nan, nan,  1., nan,  1., nan, nan, nan,  1.,\n",
       "           1., nan, nan, nan, nan,  1., nan,  1., nan, nan, nan, nan,  1.,\n",
       "          nan, nan, nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan,\n",
       "          nan, nan,  1.,  1., nan, nan, nan, nan,  1.,  1., nan,  1., nan,\n",
       "          nan, nan, nan, nan,  1., nan, nan,  1.,  1., nan, nan,  1., nan,\n",
       "          nan, nan, nan, nan,  1., nan, nan, nan, nan,  1.,  1., nan, nan,\n",
       "           1., nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan,  1., nan, nan, nan,  1., nan, nan,  1.,\n",
       "          nan, nan, nan, nan, nan,  1., nan, nan, nan,  1., nan,  1.,  1.,\n",
       "          nan,  1., nan, nan,  1., nan,  1., nan, nan, nan, nan, nan, nan,\n",
       "           1., nan, nan,  1., nan, nan, nan, nan, nan,  1., nan, nan, nan,\n",
       "          nan, nan,  1., nan, nan,  1., nan,  1., nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan,  1.,  1., nan,\n",
       "          nan, nan,  1., nan, nan,  1., nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan,  1., nan,  1., nan, nan, nan, nan,  1.,  1.,  1.,\n",
       "          nan,  1.,  1.,  1.,  1.,  1., nan, nan,  1., nan, nan,  1., nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1.,  1., nan, nan, nan, nan,\n",
       "          nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan,  1., nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan,  1.,\n",
       "           1., nan, nan, nan, nan, nan, nan, nan,  1., nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1.,  1., nan, nan, nan,  1.,\n",
       "          nan,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "           1., nan, nan, nan,  1., nan, nan, nan, nan,  1.,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1., nan, nan,  1., nan, nan, nan,\n",
       "          nan,  1., nan,  1., nan, nan, nan,  1., nan, nan,  1., nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan,  1.,  1., nan,  1., nan, nan,  1.,\n",
       "           1.,  1., nan, nan, nan, nan,  1., nan, nan, nan, nan,  1.]])]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_multi = model_multi.get_groups_fast_numba_bipartite_safe(l=0)\n",
    "res_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbb5d4-96c6-4d73-ad53-4ebec324424e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646dcbb-dc7d-4627-b84e-e1db6db70d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e545c2-5e84-4ef0-809f-158d31f594be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "511d321c-771e-4d25-b668-409bd04e59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys match exactly (within tolerance and NaN equality).\n"
     ]
    }
   ],
   "source": [
    "def compare_groups_results(r1, r2, atol=1e-12, name=\"root\"):\n",
    "    \"\"\"\n",
    "    Compare two get_groups-like results.\n",
    "\n",
    "    Raises AssertionError if any mismatch.\n",
    "    Prints detailed info about differences.\n",
    "    \"\"\"\n",
    "    for key in r1:\n",
    "        val1 = r1[key]\n",
    "        val2 = r2[key]\n",
    "        full_name = f\"{name}.{key}\"\n",
    "\n",
    "        # Check type mismatch\n",
    "        assert type(val1) == type(val2), f\"Type mismatch for {full_name}: {type(val1)} vs {type(val2)}\"\n",
    "\n",
    "        if isinstance(val1, list):\n",
    "            assert len(val1) == len(val2), f\"List length mismatch for {full_name}\"\n",
    "            for i, (a, b) in enumerate(zip(val1, val2)):\n",
    "                elem_name = f\"{full_name}[{i}]\"\n",
    "                if isinstance(a, np.ndarray):\n",
    "                    assert a.shape == b.shape, f\"Shape mismatch for {elem_name}: {a.shape} vs {b.shape}\"\n",
    "                    mask = ~np.isclose(a, b, atol=atol, equal_nan=True)\n",
    "                    if np.any(mask):\n",
    "                        idx = np.argwhere(mask)\n",
    "                        n_diff = len(idx)\n",
    "                        print(f\"{elem_name}: {n_diff} differences found\")\n",
    "                        for j, (r, c) in enumerate(idx[:20]):\n",
    "                            print(f\"{elem_name}[{r},{c}]: {a[r,c]} vs {b[r,c]}\")\n",
    "                        if n_diff > 20:\n",
    "                            print(f\"... and {n_diff - 20} more differences\")\n",
    "                        raise AssertionError(f\"{elem_name} arrays differ\")\n",
    "                else:  # scalar in list\n",
    "                    assert a == b, f\"Scalar mismatch in {elem_name}: {a} vs {b}\"\n",
    "\n",
    "        elif isinstance(val1, np.ndarray):\n",
    "            assert val1.shape == val2.shape, f\"Shape mismatch for {full_name}: {val1.shape} vs {val2.shape}\"\n",
    "            mask = ~np.isclose(val1, val2, atol=atol, equal_nan=True)\n",
    "            if np.any(mask):\n",
    "                idx = np.argwhere(mask)\n",
    "                n_diff = len(idx)\n",
    "                print(f\"{full_name}: {n_diff} differences found\")\n",
    "                for j, (r, c) in enumerate(idx[:20]):\n",
    "                    print(f\"{full_name}[{r},{c}]: {val1[r,c]} vs {val2[r,c]}\")\n",
    "                if n_diff > 20:\n",
    "                    print(f\"... and {n_diff - 20} more differences\")\n",
    "                raise AssertionError(f\"{full_name} arrays differ\")\n",
    "\n",
    "        else:  # scalar\n",
    "            assert val1 == val2, f\"Scalar mismatch for {full_name}: {val1} vs {val2}\"\n",
    "\n",
    "    print(\"All keys match exactly (within tolerance and NaN equality).\")\n",
    "    \n",
    "compare_groups_results(res_orig, res_numba_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c6f75-dc26-43d3-860c-cd4d7494fa46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d1240-613e-4a98-be29-55f8ba00d1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c13595-ec66-4cb2-bed2-bdd57debac63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef76d5-1ca0-42c2-84a6-5a2da1472902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a56ea904-adfd-486e-ad2c-abdaf3a0b596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Errors occurred while saving levels: Level 1: Failed to get group data for level 1: cannot compute fingerprint of empty list; Level 4: Failed to get group data for level 4: cannot compute fingerprint of empty list; Level 5: Failed to get group data for level 5: cannot compute fingerprint of empty list; Level 3: Failed to get group data for level 3: cannot compute fingerprint of empty list; Level 2: Failed to get group data for level 2: cannot compute fingerprint of empty list; Level 0: Failed to get group data for level 0: cannot compute fingerprint of empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 545\u001b[0m, in \u001b[0;36mbionsbm.save_data\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[1;32m    544\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLevel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l, err \u001b[38;5;129;01min\u001b[39;00m errors])\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors occurred while saving levels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Errors occurred while saving levels: Level 1: Failed to get group data for level 1: cannot compute fingerprint of empty list; Level 4: Failed to get group data for level 4: cannot compute fingerprint of empty list; Level 5: Failed to get group data for level 5: cannot compute fingerprint of empty list; Level 3: Failed to get group data for level 3: cannot compute fingerprint of empty list; Level 2: Failed to get group data for level 2: cannot compute fingerprint of empty list; Level 0: Failed to get group data for level 0: cannot compute fingerprint of empty list"
     ]
    }
   ],
   "source": [
    "model.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8cade24-5d2f-440e-b21b-53166fd2f4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mod1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_feature = model.modalities[0]\n",
    "main_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25eb1e6-b88e-434c-bd8f-6c5a78927272",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot compute fingerprint of empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 288\u001b[0m, in \u001b[0;36mbionsbm.get_groups\u001b[0;34m(self, l)\u001b[0m\n\u001b[1;32m    285\u001b[0m kinds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\u001b[38;5;241m.\u001b[39mvp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m'\u001b[39m][v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m targets], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# --- Edge processing (Numba-accelerated) ---\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m \u001b[43mprocess_edges_numba_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz1_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz2_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkinds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbranches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_wb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dbw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_w_key_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_dbw_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# --- Keep only nonzero columns safely ---\u001b[39;00m\n\u001b[1;32m    291\u001b[0m ind_d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39msum(n_db, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute fingerprint of empty list"
     ]
    }
   ],
   "source": [
    "data = model.get_groups(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875ee18-2be7-487e-b140-ce3802f4deda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d4312-c4f9-495a-b6f5-dc3008f69814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b42e2-1c65-4505-88dc-505215a06d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d819bf-ba25-41da-bead-f1057597f270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18718656-f887-4a05-9118-116361eb9c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9164c975-768c-4cc1-9d8c-655e454382c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph object, undirected, with 2000 vertices and 19777 edges, 2 internal vertex properties, 1 internal edge property, at 0x729ca92dde50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bionsbm=bionsbm(mdata[\"Peak\"])\n",
    "model_bionsbm.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f25b494d-ffa3-422a-a585-ce076de14b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph object, undirected, with 2000 vertices and 19777 edges, 2 internal vertex properties, 1 internal edge property, at 0x729ca972ba50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sbmtm import sbmtm\n",
    "model_sbmtm=sbmtm()\n",
    "model_sbmtm.make_graph_from_BoW_df(mdata[\"Peak\"].to_df().T)\n",
    "model_sbmtm.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26f11290-192e-4172-b74b-368de0b01a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(m1, m2):\n",
    "    g1, g2 = m1.g, m2.g\n",
    "\n",
    "    assert g1.num_vertices() == g2.num_vertices(), \"Different number of vertices\"\n",
    "    assert g1.num_edges() == g2.num_edges(), \"Different number of edges\"\n",
    "\n",
    "    for prop in [\"name\", \"kind\"]:\n",
    "        p1, p2 = g1.vp[prop], g2.vp[prop]\n",
    "        vals1 = [p1[v] for v in g1.vertices()]\n",
    "        vals2 = [p2[v] for v in g2.vertices()]\n",
    "        assert vals1 == vals2, f\"Vertex property {prop} differs\"\n",
    "\n",
    "    w1, w2 = g1.ep[\"count\"], g2.ep[\"count\"]\n",
    "    edges1 = sorted([(int(e.source()), int(e.target()), int(w1[e])) for e in g1.edges()])\n",
    "    edges2 = sorted([(int(e.source()), int(e.target()), int(w2[e])) for e in g2.edges()])\n",
    "    assert edges1 == edges2, \"Edges or weights differ\"\n",
    "\n",
    "    assert list(m1.documents) == list(m2.documents), \"Documents differ\"\n",
    "    assert np.array_equal(m1.words, m2.words), \"Words differ\"\n",
    "#    assert len(m1.keywords) == len(m2.keywords), \"Different number of keyword groups\"\n",
    " #   for i, (kw1, kw2) in enumerate(zip(m1.keywords, m2.keywords)):\n",
    "  #      assert np.array_equal(kw1, kw2), f\"Keywords differ at branch {i}\"\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32323d6e-98da-41be-b90e-5b937771b95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models(model_bionsbm, model_sbmtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b580ce4-7558-408a-b591-e669e87e5852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
