{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626411d5-d124-43d3-9719-2669dafb4eee",
   "metadata": {},
   "source": [
    "# Develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8037602-06c0-4b29-bd5b-1fec243ebc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bionsbm\n",
    "\n",
    "Copyright(C) 2021 fvalle1 & gmalagol10\n",
    "\n",
    "This program is free software: you can redistribute it and / or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY\n",
    "without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see < http: // www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import functools\n",
    "import os, sys\n",
    "import logging\n",
    "\n",
    "from graph_tool.all import load_graph, Graph, minimize_nested_blockmodel_dl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cloudpickle as pickle\n",
    "\n",
    "from muon import MuData\n",
    "from anndata import AnnData\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from scipy import sparse\n",
    "from numba import njit\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logger.handlers:  # prevent adding multiple handlers\n",
    "\tch = logging.StreamHandler()\n",
    "\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\tch.setFormatter(formatter)\n",
    "\tlogger.addHandler(ch)\n",
    "######################################\n",
    "import time\n",
    "from muon import read_h5mu\n",
    "\n",
    "class bionsbm():\n",
    "\t\"\"\"\n",
    "\tClass to run bionsbm\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, obj, label: Optional[str] = None, max_depth: int = 6, modality: str = \"Mod1\", saving_path: str = \"results/myself\"):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize a bionsbm self.\n",
    "\n",
    "\t\tThis constructor sets up the graph representation of the input data\n",
    "\t\t(`AnnData` or `MuData`) and optionally assigns node types based on a label.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tobj : muon.MuData or anndata.AnnData\n",
    "\t\t\tInput data object. If `MuData`, all modalities are extracted; if `AnnData`,\n",
    "\t\t\tonly the provided `modality` is used.\n",
    "\t\tlabel : str, optional\n",
    "\t\t\tColumn in `.obs` used to assign document labels and node types.\n",
    "\t\t\tIf provided, the graph is annotated accordingly.\n",
    "\t\tmax_depth : int, default=6\n",
    "\t\t\tMaximum number of levels to save or annotate in the hierarchical self.\n",
    "\t\tmodality : str, default=\"Mod1\"\n",
    "\t\t\tName of the modality to use when the input is `AnnData`.\n",
    "\t\tsaving_path : str, default=\"results/myself\"\n",
    "\t\t\tBase path for saving self outputs (graph, state, results).\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- For `MuData`, multiple modalities are combined into a multi-branch graph.\n",
    "\t\t- If `label` is provided, a mapping is created to encode document/node types.\n",
    "\t\t- `self.g` (graph) and related attributes (`documents`, `words`, `keywords`)\n",
    "\t\t  are initialized by calling `self.make_graph(...)`.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.keywords: List = []\n",
    "\t\tself.nbranches: int = 1\n",
    "\t\tself.modalities: List[str] = []\n",
    "\t\tself.max_depth: int = max_depth\n",
    "\t\tself.obj: Any = obj\n",
    "\t\tself.saving_path: str = saving_path\n",
    "\n",
    "\t\tif isinstance(obj, MuData):\n",
    "\t\t\tself.modalities=list(obj.mod.keys())   \n",
    "\t\t\tdfs=[obj[key].to_df().T for key in self.modalities]\n",
    "\t\t\tself.make_graph(dfs[0], dfs[1:])\n",
    "\n",
    "\t\telif isinstance(obj, AnnData):\n",
    "\t\t\tself.modalities=[modality]\n",
    "\t\t\tself.make_graph(obj.to_df().T, [])\n",
    "\n",
    "\t\tif label:\n",
    "\t\t\tg_raw=self.g.copy()\n",
    "\t\t\tlogger.info(\"Label found\")\n",
    "\t\t\tmetadata=obj[self.modalities[0]].obs\n",
    "\t\t\tmymap = dict([(y,str(x)) for x,y in enumerate(sorted(set(obj[self.modalities[0]].obs[label])))])\n",
    "\t\t\tinv_map = {v: k for k, v in mymap.items()}\n",
    "\n",
    "\t\t\tdocs_type=[int(mymap[metadata.loc[doc][label]]) for doc in self.documents]\n",
    "\t\t\ttypes={}\n",
    "\t\t\ttypes[\"Docs\"]=docs_type\n",
    "\t\t\tfor i, key in enumerate(self.modalities):\n",
    "\t\t\t\ttypes[key]=[int(i+np.max(docs_type)+1) for a in range(0, obj[key].shape[0])]\n",
    "\t\t\tnode_type = g_raw.new_vertex_property('int', functools.reduce(lambda a, b : a+b, list(types.values())))\n",
    "\t\t\tself.g = g_raw.copy()\n",
    "\t\telse:\n",
    "\t\t\tnode_type=None\n",
    "\t\tself.node_type=node_type \n",
    "\n",
    "\t\t\n",
    "\tdef make_graph(self, df: pd.DataFrame, df_keyword_list: List[pd.DataFrame]) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tBuild a heterogeneous graph from a main feature DataFrame and optional keyword/meta-feature DataFrames.\n",
    "\n",
    "\t\tThis function constructs a bipartite (documents–words) or multi-branch\n",
    "\t\tgraph (documents–words–keywords/meta-features) using the input matrices.\n",
    "\t\tIf a cached graph file exists at ``self.saving_path``, it is loaded directly\n",
    "\t\tinstead of rebuilding.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tdf : pandas.DataFrame\n",
    "\t\t\tMain feature matrix with words/features as rows (index) and\n",
    "\t\t\tdocuments/samples as columns.\n",
    "\t\tdf_keyword_list : list of pandas.DataFrame\n",
    "\t\t\tList of additional matrices (e.g., keywords, annotations, or meta-features).\n",
    "\t\t\tEach DataFrame must have the same columns as ``df`` (documents),\n",
    "\t\t\tand its rows will be treated as a separate feature branch.\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- Each branch is assigned a unique ``kind`` index:\n",
    "\t\t  * 0 → documents\n",
    "\t\t  * 1 → main features (e.g., words/genes)\n",
    "\t\t  * 2, 3, ... → subsequent keyword/meta-feature branches\n",
    "\t\t- If a saved graph already exists at\n",
    "\t\t  ``{self.saving_path}_graph.xml.gz``, it will be loaded instead of recreated.\n",
    "\t\t- After graph construction, the graph is saved to disk in Graph-Tool format.\n",
    "\n",
    "\t\tRaises\n",
    "\t\t------\n",
    "\t\tValueError\n",
    "\t\t\tIf ``df`` and ``df_keyword_list`` cannot be aligned properly\n",
    "\t\t\t(e.g., inconsistent columns).\n",
    "\t\t\"\"\"\n",
    "\t\tif os.path.isfile(f\"{self.saving_path}_graph.xml.gz\") == True: \n",
    "\t\t\tself.load_graph(filename=f\"{self.saving_path}_graph.xml.gz\")\n",
    "\t\telse:  \n",
    "\t\t\tlogger.info(\"Creating graph from multiple DataFrames\")\n",
    "\t\t\tdf_all = df.copy(deep =True)\n",
    "\t\t\tfor ikey,df_keyword in enumerate(df_keyword_list):\n",
    "\t\t\t\tdf_keyword = df_keyword.reindex(columns=df.columns)\n",
    "\t\t\t\tdf_keyword.index = [\"\".join([\"#\" for _ in range(ikey+1)])+str(keyword) for keyword in df_keyword.index]\n",
    "\t\t\t\tdf_keyword[\"kind\"] = ikey+2\n",
    "\t\t\t\tdf_all = pd.concat((df_all,df_keyword), axis=0)\n",
    "   \n",
    "\t\t\tdef get_kind(word):\n",
    "\t\t\t\treturn 1 if word in df.index else df_all.at[word,\"kind\"]\n",
    "   \n",
    "\t\t\tself.nbranches = len(df_keyword_list)\n",
    "\t\t   \n",
    "\t\t\tself.make_graph_single(df_all.drop(\"kind\", axis=1, errors='ignore'), get_kind)\n",
    "\n",
    "\t\t\tfolder = os.path.dirname(self.saving_path)\n",
    "\t\t\tPath(folder).mkdir(parents=True, exist_ok=True)\n",
    "\t\t\tself.save_graph(filename=f\"{self.saving_path}_graph.xml.gz\")\n",
    "\n",
    "\n",
    "\tdef make_graph_single(self, df: pd.DataFrame, get_kind) -> None:\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tConstruct a graph-tool graph from a single feature matrix.\n",
    "\n",
    "\t\tThis method builds a bipartite or multi-branch graph from the given\n",
    "\t\tDataFrame, where columns represent documents/samples and rows represent\n",
    "\t\tfeatures (e.g., words, genes, or keywords). Vertices are created for\n",
    "\t\tboth documents and features, and weighted edges connect documents to\n",
    "\t\ttheir features.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tdf : pandas.DataFrame\n",
    "\t\t\tFeature matrix with rows as features (words, genes, or keywords)\n",
    "\t\t\tand columns as documents/samples. The values must be numeric and\n",
    "\t\t\trepresent counts or weights of feature occurrences.\n",
    "\t\tget_kind : callable\n",
    "\t\t\tFunction that takes a feature name (row index from ``df``) and\n",
    "\t\t\treturns an integer specifying the vertex kind:\n",
    "\t\t\t- 0 → document nodes\n",
    "\t\t\t- 1 → main feature nodes\n",
    "\t\t\t- 2, 3, ... → keyword/meta-feature branch nodes\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- The constructed graph is undirected.\n",
    "\t\t- Vertices are annotated with two properties:\n",
    "\t\t  * ``name`` (string): document or feature name.\n",
    "\t\t  * ``kind`` (int): node type (document, word, or keyword branch).\n",
    "\t\t- Edges are annotated with ``count`` (int), representing the weight.\n",
    "\t\t- Edges with zero weight are removed after construction.\n",
    "\t\t- The graph is stored in ``self.g``\n",
    "\n",
    "\t\tRaises\n",
    "\t\t------\n",
    "\t\tValueError\n",
    "\t\t\tIf the resulting graph has no edges (i.e., ``df`` is empty or contains only zeros).\t\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tlogger.info(\"Building graph with %d docs and %d words\", df.shape[1], df.shape[0])\n",
    "\t\tself.g = Graph(directed=False)\n",
    "\n",
    "\t\tn_docs, n_words = df.shape[1], df.shape[0]\n",
    "\t\n",
    "\t\t# Add all vertices first\n",
    "\t\tself.g.add_vertex(n_docs + n_words)\n",
    "\t\n",
    "\t\t# Create vertex properties\n",
    "\t\tname = self.g.new_vp(\"string\")\n",
    "\t\tkind = self.g.new_vp(\"int\")\n",
    "\t\tself.g.vp[\"name\"] = name\n",
    "\t\tself.g.vp[\"kind\"] = kind\n",
    "\t\n",
    "\t\t# Assign doc vertices (loop for names, array for kind)\n",
    "\t\tfor i, doc in enumerate(df.columns):\n",
    "\t\t\tname[self.g.vertex(i)] = doc\n",
    "\t\tkind.get_array()[:n_docs] = 0\n",
    "\t\n",
    "\t\t# Assign word vertices (loop for names, array for kind)\n",
    "\t\tfor j, word in enumerate(df.index):\n",
    "\t\t\tname[self.g.vertex(n_docs + j)] = word\n",
    "\t\tkind.get_array()[n_docs:] = np.array([get_kind(w) for w in df.index], dtype=int)\n",
    "\t\n",
    "\t\t# Edge weights\n",
    "\t\tweight = self.g.new_ep(\"int\")\n",
    "\t\tself.g.ep[\"count\"] = weight\n",
    "\t\n",
    "\t\t# Build sparse edges\n",
    "\t\trows, cols = df.values.nonzero()\n",
    "\t\tvals = df.values[rows, cols].astype(int)\n",
    "\t\tedges = [(c, n_docs + r, v) for r, c, v in zip(rows, cols, vals)]\n",
    "\t\tif len(edges)==0: raise ValueError(\"Empty graph\")\n",
    "\t\n",
    "\t\tself.g.add_edge_list(edges, eprops=[weight])\n",
    "\t\n",
    "\t\t# Remove edges with 0 weight\n",
    "\t\tfilter_edges = self.g.new_edge_property(\"bool\")\n",
    "\t\tfilter_edges.a = weight.a > 0\n",
    "\t\tself.g.set_edge_filter(filter_edges)\n",
    "\t\tself.g.purge_edges()\n",
    "\t\tself.g.clear_filters()\n",
    "\t\n",
    "\t\tself.documents = df.columns\n",
    "\t\tself.words = df.index[self.g.vp['kind'].a[n_docs:] == 1]\n",
    "\t\tfor ik in range(2, 2 + self.nbranches):\n",
    "\t\t\tself.keywords.append(df.index[self.g.vp['kind'].a[n_docs:] == ik])\n",
    "\n",
    "\n",
    "\tdef fit(self, n_init=1, verbose=True, deg_corr=True, overlap=False, parallel=False, B_min=0, B_max=None, clabel=None, *args, **kwargs) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tFit a nested stochastic block self to the graph using `minimize_nested_blockmodel_dl`.\n",
    "\t\n",
    "\t\tThis method performs multiple initializations and keeps the best self \n",
    "\t\tbased on the minimum description length (entropy). It supports degree-corrected \n",
    "\t\tand overlapping block selfs, and can perform parallel moves for efficiency.\n",
    "\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tn_init : int, default=1\n",
    "\t\t\tNumber of random initializations. The self with the lowest entropy is retained.\n",
    "\t\tverbose : bool, default=True\n",
    "\t\t\tIf True, print progress messages.\n",
    "\t\tdeg_corr : bool, default=True\n",
    "\t\t\tIf True, use a degree-corrected block self.\n",
    "\t\toverlap : bool, default=False\n",
    "\t\t\tIf True, use an overlapping block self.\n",
    "\t\tparallel : bool, default=False\n",
    "\t\t\tIf True, perform parallel moves during optimization.\n",
    "\t\tB_min : int, default=0\n",
    "\t\t\tMinimum number of blocks to consider.\n",
    "\t\tB_max : int, optional\n",
    "\t\t\tMaximum number of blocks to consider. Defaults to the number of vertices.\n",
    "\t\tclabel : str or property map, optional\n",
    "\t\t\tVertex property to use as initial block assignment. If None, the 'kind' \n",
    "\t\t\tvertex property is used.\n",
    "\t\t*args : positional arguments\n",
    "\t\t\tAdditional positional arguments passed to `minimize_nested_blockmodel_dl`.\n",
    "\t\t**kwargs : keyword arguments\n",
    "\t\t\tAdditional keyword arguments passed to `minimize_nested_blockmodel_dl`. \n",
    "\t\t\"\"\"\n",
    "\t\tif clabel == None:\n",
    "\t\t\tclabel = self.g.vp['kind']\n",
    "\t\t\tstate_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "\t\telse:\n",
    "\t\t\tlogger.info(\"Clabel is %s, assigning partitions to vertices\", clabel)\n",
    "\t\t\tstate_args = {'clabel': clabel, 'pclabel': clabel}\n",
    "\t\n",
    "\t\tstate_args[\"eweight\"] = self.g.ep.count\n",
    "\t\tmin_entropy = np.inf\n",
    "\t\tbest_state = None\n",
    "\t\tstate_args[\"deg_corr\"] = deg_corr\n",
    "\t\tstate_args[\"overlap\"] = overlap\n",
    "\n",
    "\t\tif B_max is None:\n",
    "\t\t\tB_max = self.g.num_vertices()\n",
    "\t\t\t\n",
    "\t\tmultilevel_mcmc_args={\"B_min\": B_min, \"B_max\": B_max, \"verbose\": verbose,\"parallel\" : parallel}\n",
    "\n",
    "\t\tlogger.debug(\"multilevel_mcmc_args: %s\", multilevel_mcmc_args)\n",
    "\t\tlogger.debug(\"state_args: %s\", state_args)\n",
    "\n",
    "\t\tfor i in range(n_init):\n",
    "\t\t\tlogger.info(\"Fit number: %d\", i)\n",
    "\t\t\tstate = minimize_nested_blockmodel_dl(self.g, state_args=state_args, multilevel_mcmc_args=multilevel_mcmc_args, *args, **kwargs)\n",
    "\t\t\t\n",
    "\t\t\tentropy = state.entropy()\n",
    "\t\t\tif entropy < min_entropy:\n",
    "\t\t\t\tmin_entropy = entropy\n",
    "\t\t\t\tself.state = state\n",
    "\t\t\t\t\n",
    "\t\tself.mdl = min_entropy\n",
    "\n",
    "\t\tL = len(self.state.levels)\n",
    "\t\tself.L = L\n",
    "\n",
    "\t\tself.groups = {}\n",
    "\t\tlogger.info(\"Saving data in %s\", self.saving_path)\n",
    "\t\tself.save_data(path_to_save=self.saving_path)\n",
    "\n",
    "\t\tlogger.info(\"Annotate object\")\n",
    "\t\tself.annotate_obj()\n",
    "\n",
    "\n",
    "\t# Helper functions\n",
    "\tdef save_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave the graph\n",
    "\n",
    "\t\t:param filename: name of the graph stored\n",
    "\t\t\"\"\"\n",
    "\t\tlogger.info(\"Saving graph to %s\", filename)\n",
    "\t\tself.g.save(filename)\n",
    "\t\n",
    "\t\n",
    "\tdef load_graph(self, filename: str = \"graph.xml.gz\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tLoad a saved graph from disk and rebuild documents, words, and keywords.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tfilename : str, optional\n",
    "\t\t\tPath to the saved graph file (default: \"graph.xml.gz\").\n",
    "\t\t\"\"\"\n",
    "\t\tlogger.info(\"Loading graph from %s\", filename)\n",
    "\n",
    "\t\tself.g = load_graph(filename)\n",
    "\t\tself.documents = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 0]\n",
    "\t\tself.words = [self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == 1]\n",
    "\t\tmetadata_indexes = np.unique(self.g.vp[\"kind\"].a)\n",
    "\t\tmetadata_indexes = metadata_indexes[metadata_indexes > 1] #no doc or words\n",
    "\t\tself.nbranches = len(metadata_indexes)\n",
    "\t\tfor i_keyword in metadata_indexes:\n",
    "\t\t\tself.keywords.append([self.g.vp['name'][v] for v in self.g.vertices() if self.g.vp['kind'][v] == i_keyword])\n",
    "\n",
    "\t\n",
    "\tdef _get_edge_cache(self):\n",
    "\t\t\"\"\"\n",
    "\t\tCache edge sources, targets, weights, and kinds once per graph.\n",
    "\t\t\"\"\"\n",
    "\t\tif hasattr(self, \"_edge_cache\"):\n",
    "\t\t\treturn self._edge_cache\n",
    "\t\n",
    "\t\tedge_array = self.g.get_edges()  # shape (m, 2)\n",
    "\t\tsources = edge_array[:, 0].astype(np.int64)\n",
    "\t\ttargets = edge_array[:, 1].astype(np.int64)\n",
    "\t\n",
    "\t\tweights = self.g.ep[\"count\"].a.astype(np.float64)\n",
    "\t\tkinds = self.g.vp[\"kind\"].a[targets].astype(np.int64)\n",
    "\t\n",
    "\t\tself._edge_cache = {\"sources\": sources, \"targets\": targets, \"weights\": weights, \"kinds\": kinds}\n",
    "\t\treturn self._edge_cache\n",
    "\t\n",
    "\t\n",
    "\tdef _get_state_l_edges_array(self, state_l):\n",
    "\t\t\"\"\"\n",
    "\t\tCache block assignments (z1, z2) once per hierarchy level.\n",
    "\t\t\"\"\"\n",
    "\t\tif hasattr(state_l, \"_edges_array_cache\"):\n",
    "\t\t\treturn state_l._edges_array_cache\n",
    "\t\n",
    "\t\tedges = list(self.g.edges())\n",
    "\t\tstate_l_edges = state_l.get_edge_blocks()\n",
    "\t\n",
    "\t\tarr = np.empty((len(edges), 2), dtype=np.int64)\n",
    "\t\tfor i, e in enumerate(edges):\n",
    "\t\t\tarr[i, 0] = state_l_edges[e][0]\n",
    "\t\t\tarr[i, 1] = state_l_edges[e][1]\n",
    "\t\n",
    "\t\tstate_l._edges_array_cache = arr\n",
    "\t\treturn arr\n",
    "\t\n",
    "\t\n",
    "\tdef get_groups(self, l=0):\n",
    "\t\t\"\"\"\n",
    "\t\tFully optimized get_groups with caching of edges and block assignments.\n",
    "\t\tNo per-edge Python loops in the hot path.\n",
    "\t\t\"\"\"\n",
    "\t\tif l in self.groups:\n",
    "\t\t\treturn self.groups[l]\n",
    "\t\n",
    "\t\t# --- Setup ---\n",
    "\t\tstate_l = self.state.project_level(l).copy(overlap=True)\n",
    "\t\tB = state_l.get_B()\n",
    "\t\tD, W, K = self.get_shape()\n",
    "\t\tnbranches = self.nbranches\n",
    "\t\tK_arr = np.array(K, dtype=np.int64)\n",
    "\t\n",
    "\t\t# --- Precompute branch offsets ---\n",
    "\t\tif nbranches > 0 and K_arr.size > 0:\n",
    "\t\t\tprefix_K = np.empty(nbranches, dtype=np.int64)\n",
    "\t\t\tprefix_K[0] = 0\n",
    "\t\t\tfor ii in range(1, nbranches):\n",
    "\t\t\t\tprefix_K[ii] = prefix_K[ii-1] + K_arr[ii-1]\n",
    "\t\t\toffsets = (D + W) + prefix_K\n",
    "\t\telse:\n",
    "\t\t\toffsets = np.empty(0, dtype=np.int64)\n",
    "\t\n",
    "\t\t# --- Get cached edge arrays ---\n",
    "\t\tedge_cache = _get_edge_cache(self)\n",
    "\t\tsources = edge_cache[\"sources\"]\n",
    "\t\ttargets = edge_cache[\"targets\"]\n",
    "\t\tweights = edge_cache[\"weights\"]\n",
    "\t\tkinds = edge_cache[\"kinds\"]\n",
    "\t\n",
    "\t\t# --- Get cached block assignments ---\n",
    "\t\tz_pairs = _get_state_l_edges_array(self, state_l)\n",
    "\t\tz1_arr = z_pairs[:, 0]\n",
    "\t\tz2_arr = z_pairs[:, 1]\n",
    "\t\n",
    "\t\t# --- Allocate accumulators ---\n",
    "\t\tn_wb = np.zeros((W, B), dtype=np.float64)\n",
    "\t\tn_db = np.zeros((D, B), dtype=np.float64)\n",
    "\t\tn_dbw = np.zeros((D, B), dtype=np.float64)\n",
    "\t\tn_w_key_b3 = np.zeros((nbranches, np.max(K_arr) if nbranches > 0 else 0, B), dtype=np.float64)\n",
    "\t\tn_dbw_key3 = np.zeros((nbranches, D, B), dtype=np.float64)\n",
    "\t\n",
    "\t\t# --- Process edges ---\n",
    "\t\tprocess_edges_numba_stack(sources, targets, z1_arr, z2_arr, kinds, weights, D, W, K_arr, offsets, nbranches, n_db, n_wb, n_dbw, n_w_key_b3, n_dbw_key3)\n",
    "\t\n",
    "\t\t# --- Trim and normalize (unchanged) ---\n",
    "\t\tind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n",
    "\t\tn_db = n_db[:, ind_d]\n",
    "\t\tBd = len(ind_d)\n",
    "\t\n",
    "\t\tind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n",
    "\t\tn_wb = n_wb[:, ind_w]\n",
    "\t\tBw = len(ind_w)\n",
    "\t\n",
    "\t\tind_w2 = np.where(np.sum(n_dbw, axis=0) > 0)[0]\n",
    "\t\tn_dbw = n_dbw[:, ind_w2]\n",
    "\t\n",
    "\t\tn_w_key_b_list, n_dbw_key_list, Bk = [], [], []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tKk = int(K_arr[ik]) if K_arr.size > 0 else 0\n",
    "\t\t\tif Kk > 0:\n",
    "\t\t\t\tcol_sums = np.sum(n_w_key_b3[ik, :Kk, :], axis=0)\n",
    "\t\t\t\tind_wk = np.where(col_sums > 0)[0]\n",
    "\t\t\t\tif ind_wk.size > 0:\n",
    "\t\t\t\t\tn_w_key_b_list.append(n_w_key_b3[ik, :Kk, :][:, ind_wk].copy())\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tn_w_key_b_list.append(np.zeros((Kk, 0), dtype=np.float64))\n",
    "\t\t\t\tBk.append(len(ind_wk))\n",
    "\t\t\telse:\n",
    "\t\t\t\tn_w_key_b_list.append(np.zeros((0, 0), dtype=np.float64))\n",
    "\t\t\t\tBk.append(0)\n",
    "\t\n",
    "\t\t\tcol_sums_dbw = np.sum(n_dbw_key3[ik], axis=0)\n",
    "\t\t\tind_w2k = np.where(col_sums_dbw > 0)[0]\n",
    "\t\t\tif ind_w2k.size > 0:\n",
    "\t\t\t\tn_dbw_key_list.append(n_dbw_key3[ik][:, ind_w2k].copy())\n",
    "\t\t\telse:\n",
    "\t\t\t\tn_dbw_key_list.append(np.zeros((D, 0), dtype=np.float64))\n",
    "\t\n",
    "\t\t# --- Distributions ---\n",
    "\t\tdenom = np.sum(n_wb, axis=1, keepdims=True)\n",
    "\t\tp_tw_w = (n_wb / denom).T\n",
    "\t\n",
    "\t\tp_tk_w_key = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_w_key_b_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=1, keepdims=True)\n",
    "\t\t\tp_tk_w_key.append((arr / denom).T)\n",
    "\t\n",
    "\t\tdenom = np.sum(n_wb, axis=0, keepdims=True)\n",
    "\t\tp_w_tw = n_wb / denom\n",
    "\t\n",
    "\t\tp_w_key_tk = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_w_key_b_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=0, keepdims=True)\n",
    "\t\t\tp_w_key_tk.append(arr / denom)\n",
    "\t\n",
    "\t\tdenom = np.sum(n_dbw, axis=1, keepdims=True)\n",
    "\t\tp_tw_d = (n_dbw / denom).T\n",
    "\t\n",
    "\t\tp_tk_d = []\n",
    "\t\tfor ik in range(nbranches):\n",
    "\t\t\tarr = n_dbw_key_list[ik]\n",
    "\t\t\tdenom = np.sum(arr, axis=1, keepdims=True)\n",
    "\t\t\tp_tk_d.append((arr / denom).T)\n",
    "\t\n",
    "\t\tdenom = np.sum(n_db, axis=1, keepdims=True)\n",
    "\t\tp_td_d = (n_db / denom).T\n",
    "\t\n",
    "\t\tresult = {'Bd': Bd, 'Bw': Bw, 'Bk': Bk,\n",
    "\t\t\t\t\t'p_tw_w': p_tw_w, 'p_tk_w_key': p_tk_w_key, 'p_td_d': p_td_d,\n",
    "\t\t\t\t\t'p_w_tw': p_w_tw, 'p_w_key_tk': p_w_key_tk, 'p_tw_d': p_tw_d, 'p_tk_d': p_tk_d}\n",
    "\t\n",
    "\t\tself.groups[l] = result\n",
    "\t\treturn result\n",
    "\n",
    "\n",
    "\n",
    "\tdef save_single_level(self, l: int, path_to_save: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave per-level probability matrices (topics, clusters, documents) for the given level.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tl : int\n",
    "\t\t\tThe level index to save. Must be within the range of available self levels.\n",
    "\t\tsavingpath_to_save_path : str\n",
    "\t\t\tBase path (folder + prefix) where files will be written.\n",
    "\t\t\tExample: \"results/myself\" → files like:\n",
    "\t\t\t\t- results/myself_level_0_mainfeature_topics.tsv.gz\n",
    "\t\t\t\t- results/myself_level_0_clusters.tsv.gz\n",
    "\t\t\t\t- results/myself_level_0_mainfeature_topics_documents.tsv.gz\n",
    "\t\t\t\t- results/myself_level_0_metafeature_topics.tsv.gz\n",
    "\t\t\t\t- results/myself_level_0_metafeature_topics_documents.tsv.gz\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- Files are written as tab-separated values (`.tsv.gz`) with gzip compression.\n",
    "\t\t- Raises RuntimeError if any file cannot be written.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# --- Validate inputs ---\n",
    "\t\tif not isinstance(l, int) or l < 0 or l >= len(self.state.levels) or l >= len(self.state.levels):\n",
    "\t\t\traise ValueError(f\"Invalid level index {l}. Must be between 0 and {len(self.state.levels) - 1}.\")\n",
    "\t\tif not isinstance(path_to_save, str) or not path_to_save.strip():\n",
    "\t\t\traise ValueError(\"`path_to_save` must be a non-empty string path prefix.\")\n",
    "\n",
    "\t\tmain_feature = self.modalities[0]\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tdata = self.get_groups(l)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise RuntimeError(f\"Failed to get group data for level {l}: {e}\") from e\n",
    "\n",
    "\t\t# Helper to safely save a DataFrame\n",
    "\t\tdef _safe_save(df, filepath):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tPath(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "\t\t\t\tdf.to_csv(filepath, compression=\"gzip\", sep=\"\\t\")\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\traise RuntimeError(f\"Failed to save {filepath}: {e}\") from e\n",
    "\n",
    "\t\t# --- P(document | cluster) ---\n",
    "\t\tclusters = pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)\n",
    "\t\t_safe_save(clusters, f\"{path_to_save}_level_{l}_clusters.tsv.gz\")\n",
    "\n",
    "\n",
    "\t\t# --- P(main_feature | main_topic) ---\n",
    "\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "\t\t_safe_save(p_w_tw, f\"{path_to_save}_level_{l}_{main_feature}_topics.tsv.gz\")\n",
    "\n",
    "\t\t# --- P(main_topic | documents) ---\n",
    "\t\tp_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])])\n",
    "\t\t_safe_save(p_tw_d, f\"{path_to_save}_level_{l}_{main_feature}_topics_documents.tsv.gz\")\n",
    "\n",
    "\t\t# --- P(meta_feature | meta_topic_feature), if any ---\n",
    "\t\tif len(self.modalities) > 1:\n",
    "\t\t\tfor k, meta_features in enumerate(self.modalities[1:]):\n",
    "\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "\t\t\t\t\tcolumns=[f\"{meta_features}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t_safe_save(p_w_tw, f\"{path_to_save}_level_{l}_{meta_features}_topics.tsv.gz\")\n",
    "\n",
    "\n",
    "\t\t\t# --- P(meta_topic | document) ---\n",
    "\t\t\tfor k, meta_features in enumerate(self.modalities[1:]):\n",
    "\t\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "\t\t\t\t\tcolumns=[f\"{meta_features}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t_safe_save(p_tw_d, f\"{path_to_save}_level_{l}_{meta_features}_topics_documents.tsv.gz\")\n",
    "\n",
    "\n",
    "\n",
    "\tdef save_data(self, path_to_save: str = \"results/myself\") -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tSave the global graph, self, state, and level-specific data for the current nSBM self.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tsavinpath_to_saveg_path : str, optional\n",
    "\t\t\tBase path (folder + prefix) where all outputs will be saved.\n",
    "\t\t\tExample: \"results/myself\" will produce:\n",
    "\t\t\t\t- results/myself_graph.xml.gz\n",
    "\t\t\t\t- results/myself_self.pkl\t\n",
    "\t\t\t\t- results/myself_entropy.txt\n",
    "\t\t\t\t- results/myself_state.pkl\n",
    "\t\t\t\t- results/myself_level_X_*.tsv.gz  (per level, up to 6 levels)\n",
    "\n",
    "\t\tNotes\n",
    "\t\t-----\n",
    "\t\t- The parent folder is created automatically if it does not exist.\n",
    "\t\t- Level saving is parallelized with threads for efficiency in I/O.\n",
    "\t\t- By default, at most self.max_depth levels are saved, or fewer if the self has <self.max_depth levels.\n",
    "\t\t\"\"\"\n",
    "\t\tlogger.info(\"Saving self data to %s\", path_to_save)\n",
    "\n",
    "\t\tL = min(len(self.state.levels), self.max_depth)\n",
    "\t\tself.L = L\n",
    "\t\tif L == 0:\n",
    "\t\t\tlogger.warning(\"Nothing to save (no levels found)\")\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tfolder = os.path.dirname(path_to_save)\n",
    "\t\tPath(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tself.save_graph(filename=f\"{path_to_save}_graph.xml.gz\")\n",
    "\t\t\tself.dump_self(filename=f\"{path_to_save}_self.pkl\")\n",
    "\n",
    "\t\t\twith open(f\"{path_to_save}_entropy.txt\", \"w\") as f:\n",
    "\t\t\t\tf.write(str(self.state.entropy()))\n",
    "\n",
    "\t\t\twith open(f\"{path_to_save}_state.pkl\", \"wb\") as f:\n",
    "\t\t\t\tpickle.dump(self.state, f)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tlogger.error(\"Failed to save global files: %s\", e)\n",
    "\t\t\traise RuntimeError(f\"Failed to save global files for self '{path_to_save}': {e}\") from e\n",
    "\n",
    "\n",
    "\t\terrors = []\n",
    "\t\twith ThreadPoolExecutor() as executor:\n",
    "\t\t\tfutures = {executor.submit(self.save_single_level, l, path_to_save): l for l in range(L)}\n",
    "\t\t\tfor future in as_completed(futures):\n",
    "\t\t\t\tl = futures[future]\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tfuture.result()\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\terrors.append((l, str(e)))\n",
    "\n",
    "\t\tif errors:\n",
    "\t\t\tmsg = \"; \".join([f\"Level {l}: {err}\" for l, err in errors])\n",
    "\t\t\tlogger.error(\"Errors occurred while saving levels: %s\", msg)\n",
    "\t\t\traise RuntimeError(f\"Errors occurred while saving levels: {msg}\")\n",
    "\n",
    "\n",
    "\tdef annotate_obj(self) -> None:\n",
    "\t\tL = min(len(self.state.levels), self.max_depth)\n",
    "\t\tfor l in range(0,L):\n",
    "\t\t\tmain_feature = self.modalities[0]\n",
    "\t\t\tdata = self.get_groups(l)\n",
    "\t\t\tself.obj.obs[f\"Level_{l}_cluster\"]=np.argmax(pd.DataFrame(data=data[\"p_td_d\"], columns=self.documents)[self.obj.obs.index], axis=0).astype(str)\n",
    "\t\t\t\n",
    "\t\n",
    "\t\t\tif isinstance(self.obj, MuData):\n",
    "\t\t\t\torder_var=self.obj[main_feature].var.index\n",
    "\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "\t\t\t\t\t\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "\t\t\t\tself.obj[main_feature].var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "\t\t\telif isinstance(self.obj, AnnData):\n",
    "\t\t\t\torder_var=self.obj.var.index\t\t\t \n",
    "\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_tw\"], index=self.words,\n",
    "\t\t\t\t\t\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[order_var]\n",
    "\t\t\t\tself.obj.var[f\"Level_{l}_{main_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tw_d\"].T,index=self.documents,\n",
    "\t\t\t\t\tcolumns=[f\"{main_feature}_topic_{i}\" for i in range(data[\"p_w_tw\"].shape[1])]).loc[self.obj.obs.index]\n",
    "\t\t\tp_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "\t\t\tself.obj.obs[f\"Level_{l}_{main_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\t\t\n",
    "\t\t\tif len(self.modalities) > 1:\n",
    "\t\t\t\tfor k, meta_feature in enumerate(self.modalities[1:]):\n",
    "\t\t\t\t\tp_w_tw = pd.DataFrame(data=data[\"p_w_key_tk\"][k], index=self.keywords[k],\n",
    "\t\t\t\t\t\tcolumns=[f\"{meta_feature}_topic_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t\tself.obj[meta_feature].var[f\"Level_{l}_{meta_feature}_topic\"]=np.argmax(p_w_tw, axis=1).astype(str)\n",
    "\t\t\t\n",
    "\t\t\t\t# --- P(meta_topic | document) ---\n",
    "\t\t\t\tfor k, meta_feature in enumerate(self.modalities[1:]):\n",
    "\t\t\t\t\tp_tw_d = pd.DataFrame(data=data[\"p_tk_d\"][k].T, index=self.documents,\n",
    "\t\t\t\t\t\tcolumns=[f\"{meta_feature}_topics_{i}\" for i in range(data[\"p_w_key_tk\"][k].shape[1])])\n",
    "\t\t\t\t\tp_tw_d=p_tw_d-p_tw_d.mean(axis=0)\n",
    "\t\t\t\t\tself.obj.obs[f\"Level_{l}_{meta_feature}\"]=np.argmax(p_tw_d, axis=1).astype(str)\n",
    "\n",
    "\tdef dump_model(self, filename=\"bionsbm.pkl\"):\n",
    "\t\t\"\"\"\n",
    "\t\tDump self using pickle\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tlogger.info(\"Dumping self to %s\", filename)\n",
    "\n",
    "\t\twith open(filename, 'wb') as f:\n",
    "\t\t\tpickle.dump(self, f)\n",
    "\n",
    "\tdef load_model(self, filename=\"bionsbm.pkl\"):\n",
    "\t\tlogger.info(\"Loading self from %s\", filename)\n",
    "\n",
    "\t\twith open(filename, \"rb\") as f:\n",
    "\t\t\tself = pickle.load(f)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef get_V(self):\n",
    "\t\t'''\n",
    "\t\treturn number of word-nodes == types\n",
    "\t\t'''\n",
    "\t\treturn int(np.sum(self.g.vp['kind'].a == 1))  # no. of types\n",
    "\n",
    "\tdef get_D(self):\n",
    "\t\t'''\n",
    "\t\treturn number of doc-nodes == number of documents\n",
    "\t\t'''\n",
    "\t\treturn int(np.sum(self.g.vp['kind'].a == 0))  # no. of types\n",
    "\n",
    "\tdef get_N(self):\n",
    "\t\t'''\n",
    "\t\treturn number of edges == tokens\n",
    "\t\t'''\n",
    "\t\treturn int(self.g.num_edges())  # no. of types\n",
    "\n",
    "\n",
    "\tdef get_mdl(self):\n",
    "\t\t\"\"\"\n",
    "\t\tGet minimum description length\n",
    "\n",
    "\t\tProxy to self.state.entropy()\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.mdl\n",
    "\t\t\t\n",
    "\tdef get_shape(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: list of tuples (number of documents, number of words, (number of keywords,...))\n",
    "\t\t\"\"\"\n",
    "\t\tD = int(np.sum(self.g.vp['kind'].a == 0)) #documents\n",
    "\t\tW = int(np.sum(self.g.vp['kind'].a == 1)) #words\n",
    "\t\tK = [int(np.sum(self.g.vp['kind'].a == (k+2))) for k in range(self.nbranches)] #keywords\n",
    "\t\treturn D, W, K\n",
    "\n",
    "##### Drawing\n",
    "\tdef draw(self, *args, **kwargs) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tDraw the network\n",
    "\n",
    "\t\t:param \\*args: positional arguments to pass to self.state.draw\n",
    "\t\t:param \\*\\*kwargs: keyword argument to pass to self.state.draw\n",
    "\t\t\"\"\"\n",
    "\t\tcolmap = self.g.vertex_properties[\"color\"] = self.g.new_vertex_property(\n",
    "\t\t\t\"vector<double>\")\n",
    "\t\t#https://medialab.github.io/iwanthue/\n",
    "\t\tcolors = [  [174,80,209],\n",
    "\t\t\t\t\t[108,192,70],\n",
    "\t\t\t\t\t[207, 170, 60],\n",
    "\t\t\t\t\t[131,120,197],\n",
    "\t\t\t\t\t[126,138,65],\n",
    "\t\t\t\t\t[201,90,138],\n",
    "\t\t\t\t\t[87,172,125],\n",
    "\t\t\t\t\t[213,73,57],\n",
    "\t\t\t\t\t[85,175,209],\n",
    "\t\t\t\t\t[193,120,81]]\n",
    "\t\tfor v in self.g.vertices():\n",
    "\t\t\tk = self.g.vertex_properties['kind'][v]\n",
    "\t\t\tif k < 10:\n",
    "\t\t\t\tcolor = np.array(colors[k])/255.\n",
    "\t\t\telse:\n",
    "\t\t\t\tcolor = np.array([187, 129, 164])/255.\n",
    "\t\t\tcolmap[v] = color\n",
    "\t\tself.state.draw(\n",
    "\t\t\tsubsample_edges = 5000, \n",
    "\t\t\tedge_pen_width = self.g.ep[\"count\"],\n",
    "\t\t\tvertex_color=colmap,\n",
    "\t\t\tvertex_fill_color=colmap, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e575da5f-1e6d-4477-bc07-43f154b129ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import muon as mu\n",
    "mdata=mu.read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fe8323-58e1-4e92-b74d-fbfe632ae148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 09:16:05,264 - INFO - Creating graph from multiple DataFrames\n",
      "2025-09-15 09:16:05,269 - INFO - Building graph with 1000 docs and 1000 words\n",
      "2025-09-15 09:16:05,369 - INFO - Saving graph to results/mymodel_graph.xml.gz\n"
     ]
    }
   ],
   "source": [
    "model = bionsbm(mdata[\"Peak\"], saving_path=\"results/mymodel\", modality=\"Peak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f229030-c41f-427a-84cc-b0af4c666faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 09:16:05,952 - INFO - Fit number: 0\n",
      "2025-09-15 09:16:19,174 - INFO - Saving data in results/mymodel\n",
      "2025-09-15 09:16:19,175 - INFO - Saving model data to results/mymodel\n",
      "2025-09-15 09:16:19,175 - INFO - Saving graph to results/mymodel_graph.xml.gz\n",
      "2025-09-15 09:16:19,219 - INFO - Dumping model to results/mymodel_model.pkl\n",
      "2025-09-15 09:16:21,507 - INFO - Annotate object\n"
     ]
    }
   ],
   "source": [
    "model.fit(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b48bcc-3762-4770-b5c0-72146e9135e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(model.g.edges())\n",
    "m = len(edges)\n",
    "sources = np.empty(m, dtype=np.int64)\n",
    "targets = np.empty(m, dtype=np.int64)\n",
    "z1_arr = np.empty(m, dtype=np.int64)\n",
    "z2_arr = np.empty(m, dtype=np.int64)\n",
    "weights = np.empty(m, dtype=np.float64)\n",
    "kinds = np.empty(m, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e5185a-4f99-4282-bff8-770b7f432826",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(edges):\n",
    "    sources[i] = int(e.source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9772a596-e312-4b15-95db-72ac5ff9998e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 999, 999, 999], shape=(19777,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6eb3d0-2817-4268-bbb6-3b7a9e354938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185,\n",
       " 2637,\n",
       " 5362,\n",
       " 5605,\n",
       " 8485,\n",
       " 9217,\n",
       " 13015,\n",
       " 13077,\n",
       " 13748,\n",
       " 13972,\n",
       " 14209,\n",
       " 14310,\n",
       " 14839,\n",
       " 15129,\n",
       " 16141,\n",
       " 17030,\n",
       " 17433,\n",
       " 18461,\n",
       " 18752,\n",
       " 19333,\n",
       " 872,\n",
       " 5725,\n",
       " 8446,\n",
       " 9625,\n",
       " 13334,\n",
       " 15718,\n",
       " 16005,\n",
       " 16640,\n",
       " 16870,\n",
       " 16915,\n",
       " 18409,\n",
       " 19632,\n",
       " 694,\n",
       " 3671,\n",
       " 3787,\n",
       " 6955,\n",
       " 12865,\n",
       " 13224,\n",
       " 16527,\n",
       " 19380,\n",
       " 564,\n",
       " 2358,\n",
       " 2847,\n",
       " 5726,\n",
       " 7916,\n",
       " 9919,\n",
       " 10402,\n",
       " 11292,\n",
       " 12879,\n",
       " 13656,\n",
       " 17152,\n",
       " 8,\n",
       " 3701,\n",
       " 4195,\n",
       " 4854,\n",
       " 5920,\n",
       " 6510,\n",
       " 6744,\n",
       " 8161,\n",
       " 8908,\n",
       " 9920,\n",
       " 10293,\n",
       " 10663,\n",
       " 11166,\n",
       " 11929,\n",
       " 12420,\n",
       " 13534,\n",
       " 13606,\n",
       " 14591,\n",
       " 15797,\n",
       " 16197,\n",
       " 16641,\n",
       " 18675,\n",
       " 19236,\n",
       " 263,\n",
       " 1054,\n",
       " 1929,\n",
       " 2153,\n",
       " 2848,\n",
       " 4897,\n",
       " 7612,\n",
       " 9336,\n",
       " 9921,\n",
       " 10664,\n",
       " 12251,\n",
       " 12659,\n",
       " 13225,\n",
       " 15106,\n",
       " 15871,\n",
       " 17273,\n",
       " 18967,\n",
       " 19482,\n",
       " 7749,\n",
       " 10145,\n",
       " 10475,\n",
       " 12343,\n",
       " 12571,\n",
       " 13290,\n",
       " 13586,\n",
       " 14495,\n",
       " 15027,\n",
       " 15958,\n",
       " 16484,\n",
       " 17031,\n",
       " 17884,\n",
       " 186,\n",
       " 466,\n",
       " 1708,\n",
       " 2276,\n",
       " 2810,\n",
       " 2957,\n",
       " 3245,\n",
       " 3907,\n",
       " 4355,\n",
       " 5449,\n",
       " 5606,\n",
       " 5830,\n",
       " 6225,\n",
       " 8240,\n",
       " 8486,\n",
       " 8772,\n",
       " 9416,\n",
       " 9730,\n",
       " 9755,\n",
       " 9879,\n",
       " 12660,\n",
       " 13125,\n",
       " 13291,\n",
       " 15222,\n",
       " 16515,\n",
       " 16589,\n",
       " 17032,\n",
       " 17409,\n",
       " 17803,\n",
       " 19702,\n",
       " 1509,\n",
       " 2359,\n",
       " 3267,\n",
       " 4422,\n",
       " 4575,\n",
       " 4944,\n",
       " 5422,\n",
       " 6656,\n",
       " 7613,\n",
       " 8162,\n",
       " 8725,\n",
       " 12252,\n",
       " 12421,\n",
       " 12788,\n",
       " 12880,\n",
       " 13104,\n",
       " 13420,\n",
       " 13458,\n",
       " 14592,\n",
       " 16235,\n",
       " 17989,\n",
       " 18196,\n",
       " 18676,\n",
       " 1843,\n",
       " 2488,\n",
       " 3379,\n",
       " 4898,\n",
       " 4945,\n",
       " 5228,\n",
       " 6657,\n",
       " 6956,\n",
       " 10003,\n",
       " 11771,\n",
       " 13657,\n",
       " 15719,\n",
       " 15872,\n",
       " 18197,\n",
       " 1055,\n",
       " 1930,\n",
       " 2510,\n",
       " 3770,\n",
       " 4047,\n",
       " 4830,\n",
       " 5786,\n",
       " 8058,\n",
       " 8391,\n",
       " 10231,\n",
       " 10340,\n",
       " 10718,\n",
       " 11692,\n",
       " 12253,\n",
       " 13016,\n",
       " 14469,\n",
       " 15577,\n",
       " 16282,\n",
       " 18299,\n",
       " 18667,\n",
       " 18785,\n",
       " 19246,\n",
       " 467,\n",
       " 1987,\n",
       " 3013,\n",
       " 5296,\n",
       " 5450,\n",
       " 7095,\n",
       " 11807,\n",
       " 13157,\n",
       " 15306,\n",
       " 15674,\n",
       " 16979,\n",
       " 17804,\n",
       " 19080,\n",
       " 19558,\n",
       " 264,\n",
       " 1641,\n",
       " 2277,\n",
       " 3073,\n",
       " 4254,\n",
       " 4479,\n",
       " 6052,\n",
       " 8059,\n",
       " 9144,\n",
       " 11293,\n",
       " 12254,\n",
       " 12661,\n",
       " 12866,\n",
       " 13459,\n",
       " 13607,\n",
       " 14593,\n",
       " 15433,\n",
       " 15981,\n",
       " 17190,\n",
       " 18718,\n",
       " 0,\n",
       " 695,\n",
       " 779,\n",
       " 905,\n",
       " 1931,\n",
       " 3177,\n",
       " 3372,\n",
       " 4028,\n",
       " 4612,\n",
       " 4638,\n",
       " 4831,\n",
       " 5451,\n",
       " 6339,\n",
       " 7595,\n",
       " 8060,\n",
       " 8126,\n",
       " 8584,\n",
       " 8940,\n",
       " 9286,\n",
       " 10072,\n",
       " 10113,\n",
       " 10403,\n",
       " 13226,\n",
       " 14377,\n",
       " 15515,\n",
       " 16213,\n",
       " 17365,\n",
       " 17401,\n",
       " 18410,\n",
       " 9,\n",
       " 2673,\n",
       " 4255,\n",
       " 4789,\n",
       " 5285,\n",
       " 5423,\n",
       " 6390,\n",
       " 7359,\n",
       " 10239,\n",
       " 10595,\n",
       " 10632,\n",
       " 11772,\n",
       " 12881,\n",
       " 13105,\n",
       " 13421,\n",
       " 13460,\n",
       " 14061,\n",
       " 14197,\n",
       " 14739,\n",
       " 16283,\n",
       " 18097,\n",
       " 1443,\n",
       " 2849,\n",
       " 3380,\n",
       " 3517,\n",
       " 4946,\n",
       " 5229,\n",
       " 6168,\n",
       " 9084,\n",
       " 10756,\n",
       " 13227,\n",
       " 13587,\n",
       " 14062,\n",
       " 16006,\n",
       " 16642,\n",
       " 16953,\n",
       " 17414,\n",
       " 19450,\n",
       " 10,\n",
       " 2360,\n",
       " 2677,\n",
       " 2850,\n",
       " 3381,\n",
       " 4947,\n",
       " 5089,\n",
       " 5727,\n",
       " 5862,\n",
       " 6857,\n",
       " 7536,\n",
       " 9358,\n",
       " 9507,\n",
       " 9922,\n",
       " 10268,\n",
       " 10633,\n",
       " 11294,\n",
       " 11727,\n",
       " 13335,\n",
       " 15720,\n",
       " 16284,\n",
       " 16584,\n",
       " 16883,\n",
       " 18198,\n",
       " 18789,\n",
       " 19408,\n",
       " 1056,\n",
       " 2308,\n",
       " 3382,\n",
       " 5079,\n",
       " 5728,\n",
       " 7084,\n",
       " 9359,\n",
       " 9923,\n",
       " 10665,\n",
       " 10757,\n",
       " 12662,\n",
       " 12882,\n",
       " 15798,\n",
       " 11,\n",
       " 122,\n",
       " 468,\n",
       " 906,\n",
       " 4639,\n",
       " 5685,\n",
       " 5831,\n",
       " 6153,\n",
       " 8669,\n",
       " 10028,\n",
       " 10269,\n",
       " 11159,\n",
       " 11448,\n",
       " 12663,\n",
       " 13032,\n",
       " 16485,\n",
       " 17434,\n",
       " 18462,\n",
       " 19483,\n",
       " 2361,\n",
       " 3364,\n",
       " 3383,\n",
       " 5286,\n",
       " 5921,\n",
       " 6116,\n",
       " 6511,\n",
       " 6713,\n",
       " 7614,\n",
       " 9360,\n",
       " 12771,\n",
       " 12776,\n",
       " 12883,\n",
       " 14210,\n",
       " 16643,\n",
       " 17600,\n",
       " 17742,\n",
       " 18068,\n",
       " 18199,\n",
       " 807,\n",
       " 1007,\n",
       " 1353,\n",
       " 1619,\n",
       " 1822,\n",
       " 2169,\n",
       " 3246,\n",
       " 3499,\n",
       " 3603,\n",
       " 3621,\n",
       " 3908,\n",
       " 4256,\n",
       " 5261,\n",
       " 5324,\n",
       " 5529,\n",
       " 5922,\n",
       " 6226,\n",
       " 8241,\n",
       " 8941,\n",
       " 9706,\n",
       " 10341,\n",
       " 11295,\n",
       " 11449,\n",
       " 13846,\n",
       " 14211,\n",
       " 14777,\n",
       " 15484,\n",
       " 16398,\n",
       " 17033,\n",
       " 17435,\n",
       " 18463,\n",
       " 18968,\n",
       " 19225,\n",
       " 19347,\n",
       " 19559,\n",
       " 317,\n",
       " 667,\n",
       " 1057,\n",
       " 1235,\n",
       " 2603,\n",
       " 2638,\n",
       " 2768,\n",
       " 3014,\n",
       " 3909,\n",
       " 3994,\n",
       " 4048,\n",
       " 4202,\n",
       " 5070,\n",
       " 5523,\n",
       " 5832,\n",
       " 6287,\n",
       " 6996,\n",
       " 7096,\n",
       " 7477,\n",
       " 7537,\n",
       " 7716,\n",
       " 8242,\n",
       " 8341,\n",
       " 8431,\n",
       " 8942,\n",
       " 9218,\n",
       " 9756,\n",
       " 10146,\n",
       " 10476,\n",
       " 11450,\n",
       " 12051,\n",
       " 12344,\n",
       " 13541,\n",
       " 14212,\n",
       " 14353,\n",
       " 14496,\n",
       " 14710,\n",
       " 15634,\n",
       " 17274,\n",
       " 17349,\n",
       " 17743,\n",
       " 18464,\n",
       " 18590,\n",
       " 19081,\n",
       " 19226,\n",
       " 2309,\n",
       " 4948,\n",
       " 8447,\n",
       " 8832,\n",
       " 10270,\n",
       " 13448,\n",
       " 16007,\n",
       " 16644,\n",
       " 18710,\n",
       " 780,\n",
       " 2201,\n",
       " 4603,\n",
       " 4613,\n",
       " 4640,\n",
       " 5248,\n",
       " 5863,\n",
       " 6227,\n",
       " 6460,\n",
       " 7596,\n",
       " 8216,\n",
       " 8355,\n",
       " 9757,\n",
       " 9880,\n",
       " 10004,\n",
       " 10719,\n",
       " 10820,\n",
       " 15516,\n",
       " 15578,\n",
       " 15996,\n",
       " 16789,\n",
       " 16884,\n",
       " 18423,\n",
       " 18668,\n",
       " 982,\n",
       " 3518,\n",
       " 4480,\n",
       " 8163,\n",
       " 9145,\n",
       " 12884,\n",
       " 15434,\n",
       " 15799,\n",
       " 17034,\n",
       " 18200,\n",
       " 873,\n",
       " 1410,\n",
       " 1709,\n",
       " 2170,\n",
       " 2473,\n",
       " 2540,\n",
       " 2705,\n",
       " 2958,\n",
       " 3702,\n",
       " 4144,\n",
       " 4203,\n",
       " 4733,\n",
       " 4790,\n",
       " 5062,\n",
       " 5410,\n",
       " 5452,\n",
       " 6817,\n",
       " 7717,\n",
       " 8061,\n",
       " 8243,\n",
       " 8448,\n",
       " 8943,\n",
       " 9021,\n",
       " 9085,\n",
       " 9118,\n",
       " 9417,\n",
       " 10147,\n",
       " 10937,\n",
       " 11062,\n",
       " 11271,\n",
       " 11561,\n",
       " 12572,\n",
       " 12755,\n",
       " 13033,\n",
       " 13158,\n",
       " 13369,\n",
       " 13443,\n",
       " 13557,\n",
       " 13866,\n",
       " 15362,\n",
       " 15579,\n",
       " 16142,\n",
       " 18120,\n",
       " 18176,\n",
       " 18411,\n",
       " 19082,\n",
       " 19484,\n",
       " 19703,\n",
       " 1,\n",
       " 469,\n",
       " 808,\n",
       " 2563,\n",
       " 2639,\n",
       " 3043,\n",
       " 4049,\n",
       " 4734,\n",
       " 4883,\n",
       " 4927,\n",
       " 6411,\n",
       " 7097,\n",
       " 7718,\n",
       " 7750,\n",
       " 7917,\n",
       " 8534,\n",
       " 9418,\n",
       " 9707,\n",
       " 11272,\n",
       " 11451,\n",
       " 12052,\n",
       " 12469,\n",
       " 13208,\n",
       " 13677,\n",
       " 14398,\n",
       " 14778,\n",
       " 16590,\n",
       " 17711,\n",
       " 18640,\n",
       " 758,\n",
       " 1531,\n",
       " 1932,\n",
       " 2230,\n",
       " 2640,\n",
       " 2811,\n",
       " 3771,\n",
       " 4387,\n",
       " 4832,\n",
       " 4873,\n",
       " 4899,\n",
       " 5565,\n",
       " 5642,\n",
       " 5787,\n",
       " 6228,\n",
       " 6340,\n",
       " 6512,\n",
       " 7098,\n",
       " 7597,\n",
       " 7988,\n",
       " 8217,\n",
       " 8833,\n",
       " 10294,\n",
       " 11167,\n",
       " 11693,\n",
       " 11773,\n",
       " 11930,\n",
       " 12422,\n",
       " 12867,\n",
       " 12885,\n",
       " 13780,\n",
       " 16285,\n",
       " 16645,\n",
       " 16904,\n",
       " 17024,\n",
       " 17436,\n",
       " 17861,\n",
       " 17885,\n",
       " 17911,\n",
       " 18069,\n",
       " 18098,\n",
       " 434,\n",
       " 2851,\n",
       " 2982,\n",
       " 3384,\n",
       " 3703,\n",
       " 3826,\n",
       " 4124,\n",
       " 4949,\n",
       " 5923,\n",
       " 6229,\n",
       " 6513,\n",
       " 6637,\n",
       " 6916,\n",
       " 7035,\n",
       " 7615,\n",
       " 8834,\n",
       " 9146,\n",
       " 9924,\n",
       " 10271,\n",
       " 10905,\n",
       " 12241,\n",
       " 12886,\n",
       " 16008,\n",
       " 16646,\n",
       " 16839,\n",
       " 18055,\n",
       " 18300,\n",
       " 668,\n",
       " 1008,\n",
       " 1058,\n",
       " 2959,\n",
       " 3268,\n",
       " 4050,\n",
       " 5249,\n",
       " 5924,\n",
       " 6123,\n",
       " 6154,\n",
       " 7823,\n",
       " 7918,\n",
       " 8042,\n",
       " 8244,\n",
       " 8487,\n",
       " 9119,\n",
       " 10295,\n",
       " 10553,\n",
       " 11627,\n",
       " 11877,\n",
       " 12053,\n",
       " 14399,\n",
       " 14905,\n",
       " 15418,\n",
       " 17123,\n",
       " 18442,\n",
       " 18465,\n",
       " 18753,\n",
       " 18790,\n",
       " 19638,\n",
       " 19704,\n",
       " 439,\n",
       " 457,\n",
       " 470,\n",
       " 1059,\n",
       " 1236,\n",
       " 1988,\n",
       " 2076,\n",
       " 2812,\n",
       " 3327,\n",
       " 3704,\n",
       " 3895,\n",
       " 4125,\n",
       " 4322,\n",
       " 5530,\n",
       " 6185,\n",
       " 6957,\n",
       " 7160,\n",
       " 7303,\n",
       " 7478,\n",
       " 7824,\n",
       " 8432,\n",
       " 8670,\n",
       " 9419,\n",
       " 9708,\n",
       " 11273,\n",
       " 11878,\n",
       " 12756,\n",
       " 13171,\n",
       " 14018,\n",
       " 14497,\n",
       " 14840,\n",
       " 15196,\n",
       " 16009,\n",
       " 16554,\n",
       " 17261,\n",
       " 17437,\n",
       " 17744,\n",
       " 18177,\n",
       " 18791,\n",
       " 19485,\n",
       " 19560,\n",
       " 12,\n",
       " 665,\n",
       " 1237,\n",
       " 3896,\n",
       " 5297,\n",
       " 5531,\n",
       " 5686,\n",
       " 6288,\n",
       " 6584,\n",
       " 6879,\n",
       " 7099,\n",
       " 7719,\n",
       " 7825,\n",
       " 9859,\n",
       " 10067,\n",
       " 10807,\n",
       " 11274,\n",
       " 11562,\n",
       " 13847,\n",
       " 13867,\n",
       " 16185,\n",
       " 16486,\n",
       " 17035,\n",
       " 17402,\n",
       " 19291,\n",
       " 19561,\n",
       " 983,\n",
       " 1416,\n",
       " 2723,\n",
       " 4257,\n",
       " 4481,\n",
       " 4791,\n",
       " 5702,\n",
       " 6053,\n",
       " 6706,\n",
       " 7360,\n",
       " 9086,\n",
       " 10089,\n",
       " 11168,\n",
       " 11296,\n",
       " 14740,\n",
       " 16647,\n",
       " 19155,\n",
       " 809,\n",
       " 907,\n",
       " 2813,\n",
       " 5453,\n",
       " 6477,\n",
       " 7479,\n",
       " 7751,\n",
       " 7826,\n",
       " 9758,\n",
       " 9860,\n",
       " 11988,\n",
       " 12470,\n",
       " 13292,\n",
       " 17895,\n",
       " 18792,\n",
       " 18969,\n",
       " 405,\n",
       " 1009,\n",
       " 1060,\n",
       " 1554,\n",
       " 3269,\n",
       " 4339,\n",
       " 4482,\n",
       " 5250,\n",
       " 6658,\n",
       " 7135,\n",
       " 7598,\n",
       " 7616,\n",
       " 9272,\n",
       " 9337,\n",
       " 9925,\n",
       " 10272,\n",
       " 10938,\n",
       " 11169,\n",
       " 14019,\n",
       " 14213,\n",
       " 14594,\n",
       " 14659,\n",
       " 15092,\n",
       " 15873,\n",
       " 16010,\n",
       " 17246,\n",
       " 19156,\n",
       " 1223,\n",
       " 2852,\n",
       " 5729,\n",
       " 7361,\n",
       " 7617,\n",
       " 11151,\n",
       " 13772,\n",
       " 14595,\n",
       " 16286,\n",
       " 658,\n",
       " 6230,\n",
       " 6659,\n",
       " 7919,\n",
       " 9147,\n",
       " 9532,\n",
       " 9626,\n",
       " 11170,\n",
       " 12664,\n",
       " 13106,\n",
       " 14498,\n",
       " 14660,\n",
       " 15800,\n",
       " 16287,\n",
       " 19072,\n",
       " 2706,\n",
       " 3679,\n",
       " 8835,\n",
       " 10666,\n",
       " 10829,\n",
       " 11931,\n",
       " 16916,\n",
       " 3270,\n",
       " 4950,\n",
       " 5353,\n",
       " 9926,\n",
       " 11774,\n",
       " 14937,\n",
       " 15510,\n",
       " 17990,\n",
       " 18201,\n",
       " 19112,\n",
       " 91,\n",
       " 265,\n",
       " 653,\n",
       " 3144,\n",
       " 3205,\n",
       " 4483,\n",
       " 5363,\n",
       " 6434,\n",
       " 7022,\n",
       " 9022,\n",
       " 9845,\n",
       " 10051,\n",
       " 11297,\n",
       " 11715,\n",
       " 12255,\n",
       " 12665,\n",
       " 15644,\n",
       " 18202,\n",
       " 19157,\n",
       " 565,\n",
       " 1868,\n",
       " 2362,\n",
       " 3074,\n",
       " 3385,\n",
       " 3519,\n",
       " 4204,\n",
       " 4423,\n",
       " 4951,\n",
       " 5925,\n",
       " 7521,\n",
       " 10596,\n",
       " 10906,\n",
       " 12423,\n",
       " 13505,\n",
       " 13797,\n",
       " 14674,\n",
       " 15721,\n",
       " 16363,\n",
       " 16917,\n",
       " 17275,\n",
       " 18627,\n",
       " 554,\n",
       " 2363,\n",
       " 3705,\n",
       " 5730,\n",
       " 8164,\n",
       " 10273,\n",
       " 10667,\n",
       " 12424,\n",
       " 12777,\n",
       " 12868,\n",
       " 13608,\n",
       " 14422,\n",
       " 18301,\n",
       " 19158,\n",
       " 1061,\n",
       " 2202,\n",
       " 3206,\n",
       " 3271,\n",
       " 4576,\n",
       " 4833,\n",
       " 6027,\n",
       " 6660,\n",
       " 6717,\n",
       " 8585,\n",
       " 9317,\n",
       " 11171,\n",
       " 14423,\n",
       " 15580,\n",
       " 16407,\n",
       " 18565,\n",
       " 18918,\n",
       " 18970,\n",
       " 4952,\n",
       " 5394,\n",
       " 6054,\n",
       " 7618,\n",
       " 10029,\n",
       " 10148,\n",
       " 13868,\n",
       " 14063,\n",
       " 14214,\n",
       " 14424,\n",
       " 14596,\n",
       " 16288,\n",
       " 18302,\n",
       " 18743,\n",
       " 18793,\n",
       " 2180,\n",
       " 2641,\n",
       " 3827,\n",
       " 4388,\n",
       " 4792,\n",
       " 4953,\n",
       " 5864,\n",
       " 6893,\n",
       " 7229,\n",
       " 7720,\n",
       " 7827,\n",
       " 7920,\n",
       " 8062,\n",
       " 8342,\n",
       " 8671,\n",
       " 10114,\n",
       " 11429,\n",
       " 11879,\n",
       " 12345,\n",
       " 13126,\n",
       " 13749,\n",
       " 14026,\n",
       " 14499,\n",
       " 15572,\n",
       " 15635,\n",
       " 16143,\n",
       " 17036,\n",
       " 18794,\n",
       " 19486,\n",
       " 19562,\n",
       " 471,\n",
       " 1062,\n",
       " 1238,\n",
       " 2231,\n",
       " 3706,\n",
       " 3910,\n",
       " 4051,\n",
       " 4458,\n",
       " 6186,\n",
       " 6818,\n",
       " 7828,\n",
       " 8602,\n",
       " 9731,\n",
       " 12054,\n",
       " 13678,\n",
       " 13869,\n",
       " 13973,\n",
       " 14215,\n",
       " 15028,\n",
       " 17037,\n",
       " 17805,\n",
       " 18795,\n",
       " 19487,\n",
       " 759,\n",
       " 1239,\n",
       " 1411,\n",
       " 1444,\n",
       " 1710,\n",
       " 2674,\n",
       " 3120,\n",
       " 4641,\n",
       " 4834,\n",
       " 5865,\n",
       " 6199,\n",
       " 6341,\n",
       " 6391,\n",
       " 6905,\n",
       " 7161,\n",
       " 7362,\n",
       " 7986,\n",
       " 7989,\n",
       " 8063,\n",
       " 8944,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.g.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56e12a-fe8c-4c17-9b40-1aa30ec4b822",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f27444-fda4-4323-8295-cb01d63bdf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bionsbm\n",
    "from muon import read_h5mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80ae9f3-21b3-405e-996c-fa13b14d066f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata=read_h5mu(\"../Test_data.h5mu\")\n",
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290fd507-b4a7-4261-9e29-95c85d035ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 15:25:14,005 - INFO - Creating graph from multiple DataFrames\n",
      "2025-09-12 15:25:14,024 - INFO - Building graph with 1000 docs and 2200 words\n",
      "2025-09-12 15:25:14,389 - INFO - Saving graph to results/mymodel_graph.xml.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bionsbm.model.bionsbm at 0x7eacd678d7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = bionsbm.model.bionsbm(obj=mdata, saving_path=\"results/mybionsbm\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9eefe6b-3e38-42a1-a262-7023508fadb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 18:15:30,827 - INFO - Fit number: 0\n",
      "2025-09-11 18:15:42,547 - INFO - Saving data in results/mymodel\n",
      "2025-09-11 18:15:42,547 - INFO - Saving model data to results/mymodel\n",
      "2025-09-11 18:15:42,548 - INFO - Saving graph to results/mymodel_graph.xml.gz\n",
      "2025-09-11 18:15:42,602 - INFO - Dumping model to results/mymodel_model.pkl\n",
      "2025-09-11 18:15:45,318 - INFO - Annotate object\n"
     ]
    }
   ],
   "source": [
    "model.fit(n_init=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64573bf5-4e18-48cb-b901-a94364ed229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  obs:\t&#x27;Level_0_cluster&#x27;, &#x27;Level_0_Peak&#x27;, &#x27;Level_0_mRNA&#x27;, &#x27;Level_0_lncRNA&#x27;, &#x27;Level_1_cluster&#x27;, &#x27;Level_1_Peak&#x27;, &#x27;Level_1_mRNA&#x27;, &#x27;Level_1_lncRNA&#x27;, &#x27;Level_2_cluster&#x27;, &#x27;Level_2_Peak&#x27;, &#x27;Level_2_mRNA&#x27;, &#x27;Level_2_lncRNA&#x27;, &#x27;Level_3_cluster&#x27;, &#x27;Level_3_Peak&#x27;, &#x27;Level_3_mRNA&#x27;, &#x27;Level_3_lncRNA&#x27;, &#x27;Level_4_cluster&#x27;, &#x27;Level_4_Peak&#x27;, &#x27;Level_4_mRNA&#x27;, &#x27;Level_4_lncRNA&#x27;, &#x27;Level_5_cluster&#x27;, &#x27;Level_5_Peak&#x27;, &#x27;Level_5_mRNA&#x27;, &#x27;Level_5_lncRNA&#x27;\n",
       "  var:\t&#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t&#x27;RNA_celltype&#x27;, &#x27;Raw_CellType&#x27;, &#x27;CellType&#x27;, &#x27;nucleosome_signal&#x27;, &#x27;tss_enrichment_score&#x27;, &#x27;n_features&#x27;, &#x27;log_n_features&#x27;, &#x27;n_counts&#x27;, &#x27;log_n_counts&#x27;, &#x27;tss&#x27;, &#x27;passes_filter&#x27;\n",
       "      var:\t&#x27;chr&#x27;, &#x27;start&#x27;, &#x27;stop&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;, &#x27;Level_0_Peak_topic&#x27;, &#x27;Level_1_Peak_topic&#x27;, &#x27;Level_2_Peak_topic&#x27;, &#x27;Level_3_Peak_topic&#x27;, &#x27;Level_4_Peak_topic&#x27;, &#x27;Level_5_Peak_topic&#x27;\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;, &#x27;Level_0_mRNA_topic&#x27;, &#x27;Level_1_mRNA_topic&#x27;, &#x27;Level_2_mRNA_topic&#x27;, &#x27;Level_3_mRNA_topic&#x27;, &#x27;Level_4_mRNA_topic&#x27;, &#x27;Level_5_mRNA_topic&#x27;\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t&#x27;gene_ids&#x27;, &#x27;feature_types&#x27;, &#x27;n_cells&#x27;, &#x27;log_n_cells&#x27;, &#x27;passes_filter&#x27;, &#x27;highly_variable&#x27;, &#x27;highly_variable_rank&#x27;, &#x27;means&#x27;, &#x27;variances&#x27;, &#x27;variances_norm&#x27;, &#x27;Level_0_lncRNA_topic&#x27;, &#x27;Level_1_lncRNA_topic&#x27;, &#x27;Level_2_lncRNA_topic&#x27;, &#x27;Level_3_lncRNA_topic&#x27;, &#x27;Level_4_lncRNA_topic&#x27;, &#x27;Level_5_lncRNA_topic&#x27;</pre>"
      ],
      "text/plain": [
       "MuData object with n_obs × n_vars = 1000 × 2200\n",
       "  obs:\t'Level_0_cluster', 'Level_0_Peak', 'Level_0_mRNA', 'Level_0_lncRNA', 'Level_1_cluster', 'Level_1_Peak', 'Level_1_mRNA', 'Level_1_lncRNA', 'Level_2_cluster', 'Level_2_Peak', 'Level_2_mRNA', 'Level_2_lncRNA', 'Level_3_cluster', 'Level_3_Peak', 'Level_3_mRNA', 'Level_3_lncRNA', 'Level_4_cluster', 'Level_4_Peak', 'Level_4_mRNA', 'Level_4_lncRNA', 'Level_5_cluster', 'Level_5_Peak', 'Level_5_mRNA', 'Level_5_lncRNA'\n",
       "  var:\t'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm'\n",
       "  3 modalities\n",
       "    Peak:\t1000 x 1000\n",
       "      obs:\t'RNA_celltype', 'Raw_CellType', 'CellType', 'nucleosome_signal', 'tss_enrichment_score', 'n_features', 'log_n_features', 'n_counts', 'log_n_counts', 'tss', 'passes_filter'\n",
       "      var:\t'chr', 'start', 'stop', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'Level_0_Peak_topic', 'Level_1_Peak_topic', 'Level_2_Peak_topic', 'Level_3_Peak_topic', 'Level_4_Peak_topic', 'Level_5_Peak_topic'\n",
       "    mRNA:\t1000 x 700\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'Level_0_mRNA_topic', 'Level_1_mRNA_topic', 'Level_2_mRNA_topic', 'Level_3_mRNA_topic', 'Level_4_mRNA_topic', 'Level_5_mRNA_topic'\n",
       "    lncRNA:\t1000 x 500\n",
       "      var:\t'gene_ids', 'feature_types', 'n_cells', 'log_n_cells', 'passes_filter', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'Level_0_lncRNA_topic', 'Level_1_lncRNA_topic', 'Level_2_lncRNA_topic', 'Level_3_lncRNA_topic', 'Level_4_lncRNA_topic', 'Level_5_lncRNA_topic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6c5a7-abb0-4c25-8d10-968f77646141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ef878-72c9-4a76-83c2-201b9d5bd4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff410f1-6692-4088-9c18-50bf889a05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16363eda-692e-4871-b2fc-d4c7d621a8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220f25e-344d-4a56-bcb4-8dd1fa9f4311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c615bc6-f24c-41dc-ae74-6b9946646701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740b4ae-1fb4-473f-ab7e-ad047fd76a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c33bb5-f6f0-48a7-911c-076596c5a5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489baee-5516-4a7a-b861-16e875b158f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
